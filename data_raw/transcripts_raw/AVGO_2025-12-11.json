{
  "event_id": "AVGO_2025-12-11",
  "ticker": "AVGO",
  "company": "Broadcom Inc.",
  "quarter": 4,
  "fiscal_year": 2025,
  "call_date": "2025-12-11",
  "call_start_ts": "2025-12-11 22:00:00+00:00",
  "raw_text": "\n|[pic]                     |\n\nBroadcom Inc. NasdaqGS:AVGO\nFQ4 2025 Earnings Call Transcripts\nThursday, December 11, 2025 10:00 PM GMT\nS&P Global Market Intelligence Estimates\n|      |-FQ4 2025-           |-FQ1 2026-   |-FY 2025-            |-FY   |\n|      |                     |             |                     |2026- |\n|                              |CONSENSUS      |ACTUAL         |SURPRISE       |\n|                   |CONSENSUS          |ACTUAL             |SURPRISE           |\n|FQ1 2025           |1.51               |1.60               |[pic]5.96 %        |\n|FQ2 2025           |1.57               |1.58               |[pic]0.64 %        |\n|FQ3 2025           |1.66               |1.69               |[pic]1.81 %        |\n|FQ4 2025           |1.87               |1.95               |[pic]4.28 %        |\n\n|Table of Contents                                     |   |\n|Call Participants          |..............................................|3      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|Presentation               |..............................................|4      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|Question and Answer        |..............................................|8      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|                                                                                  |\n|Call Participants                                                                 |\n|                           |                           |                           |\n|EXECUTIVES                 |                           |                           |\n|                           |Karl  Ackerman             |                           |\n|                           |BNP Paribas, Research      |                           |\n|Hock E. Tan                |Division                   |                           |\n|President, CEO & Executive |                           |                           |\n|Director                   |                           |                           |\n|                           |Ross Clark Seymore         |                           |\n|                           |Deutsche Bank AG, Research |                           |\n|Ji  Yoo                    |Division                   |                           |\n|Director of Investor       |                           |                           |\n|Relations                  |                           |                           |\n|                           |Stacy Aaron Rasgon         |                           |\n|                           |Sanford C. Bernstein & Co.,|                           |\n|Kirsten M. Spears          |LLC., Research Division    |                           |\n|CFO & Chief Accounting     |                           |                           |\n|Officer                    |                           |                           |\n|                           |Vivek  Arya                |                           |\n|                           |BofA Securities, Research  |                           |\n|ANALYSTS                   |Division                   |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Benjamin Alexander Reitzes |                           |                           |\n|                           |                           |                           |\n|Melius Research LLC        |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Blayne Peter Curtis        |                           |                           |\n|Jefferies LLC, Research    |                           |                           |\n|Division                   |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Christopher Adam Jackson   |                           |                           |\n|Rolland                    |                           |                           |\n|Susquehanna Financial      |                           |                           |\n|Group, LLLP, Research      |                           |                           |\n|Division                   |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Christopher James Muse     |                           |                           |\n|Cantor Fitzgerald & Co.,   |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Harlan L.  Sur             |                           |                           |\n|JPMorgan Chase & Co,       |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Harsh V. Kumar             |                           |                           |\n|Piper Sandler & Co.,       |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|James Edward Schneider     |                           |                           |\n|Goldman Sachs Group, Inc., |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Joseph Lawrence Moore      |                           |                           |\n|Morgan Stanley, Research   |                           |                           |\n|Division                   |                           |                           |\n|                                                                                  |\n\n\nPresentation\n\n\nOperator\n\nWelcome to Broadcom Inc. Fourth Quarter and Fiscal Year 2025 Financial\nResults Conference Call.\n\n\nAt this time, for opening remarks and introductions, I would like to turn\nthe call over to Ji Yoo, Head of Investor Relations of Broadcom Inc.\n\n\nJi  Yoo\nDirector of Investor Relations\n\nThank you, Sheri, and good afternoon, everyone. Joining me on today's call\nare Hock Tan, President and CEO; Kirsten Spears, Chief Financial Officer;\nand Charlie Kawwas, President, Semiconductor Solutions Group.\n\n\nBroadcom distributed a press release and financial tables after the market\nclose, describing our financial performance for the fourth quarter and\nfiscal year 2025. If you did not receive a copy, you may obtain the\ninformation from the Investors section of Broadcom's website at\nbroadcom.com.\n\n\nThis conference call is being webcast live, and an audio replay of the call\ncan be accessed for 1 year through the Investors section of Broadcom's\nwebsite.\n\n\nDuring the prepared remarks, Hock and Kirsten will be providing details of\nour fourth quarter and fiscal year 2025 results, guidance for our first\nquarter of fiscal year 2026 as well as commentary regarding the business\nenvironment. We'll take questions after the end of our prepared comments.\n\n\nPlease refer to our press release today and our recent filings with the SEC\nfor information on the specific risk factors that could cause our actual\nresults to differ materially from the forward-looking statements made on\nthis call.\n\n\nIn addition to U.S. GAAP reporting, Broadcom reports certain financial\nmeasures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP\nmeasures is included in the tables attached to today's press release.\nComments made during today's call will primarily refer to our non-GAAP\nfinancial results.\n\n\nI'll now turn the call over to Hock.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nThank you, Ji, and thank you, everyone, for joining us today. Well, we just\nended our Q4 fiscal '25. And before I get into details of that quarter, let\nme recap the year.\n\n\nIn our fiscal 2025, consolidated revenue grew 24% year-over-year to a\nrecord $64 billion, and it's driven by AI semiconductors and VMware. AI\nrevenue grew 65% year-over-year to $20 billion, driving the semiconductor\nrevenue for this company to a record $37 billion for the year. In our\ninfrastructure software business, strong adoption of VMware Cloud\nFoundation or VCF, as we call it, drove revenue growth of 26% year-on-year\nto $27 billion.\n\n\nIn summary, 2025 was another strong year for Broadcom. And we see the\nspending momentum by our customers for -- in AI, continuing to accelerate\nin 2026. Now let's move on to the results of our fourth quarter 2025.\n\n\nTotal revenue was a record $18 billion, up 28% year-on-year, and above our\nguidance on better-than-expected growth in AI semiconductors as well as\ninfrastructure software. Q4 consolidated adjusted EBITDA was a record\n$12.12 billion (sic) [ $12.218 billion ], up 34% year-on-year. So let me\ngive you more color on our two segments.\n\n\nIn semiconductors, revenue was $11.1 billion as year-on-year growth\naccelerated to 35%. And this robust growth was driven by the AI\nsemiconductor revenue of $6.5 billion, which was up 74% year-on-year. And\nthis represents a growth trajectory exceeding 10x over the 11 quarters we\nhave reported this line of business. Our custom accelerated business more\nthan doubled year-over-year, as we see our customers increase adoption of\nXPUs, as we call those custom accelerators, in training their LLMs and\nmonetizing their platforms through inferencing APIs and applications.\n\n\nThese XPUs, I may add, are not only being used to train and inference\ninternal workloads by our customers, the same XPUs, in some situations,\nhave been extended externally to other LLM peers, best exemplified at\nGoogle, where the TPUs used in creating Gemini have also been used for AI\ncloud computing by Apple, Cohere and SSI as an example. And the scale at\nwhich we see this happening could be significant.\n\n\nAnd as you are aware, last quarter, Q3 '25, we received a $10 billion order\nto sell the latest TPU Ironwood racks to Anthropic. And this was our fourth\ncustomer that we mentioned. And in this quarter Q4, we received an\nadditional $11 billion order from the same customer for delivery in late\n2026.\n\n\nBut that does not mean our other two customers are using TPUs. In fact,\nthey prefer to control their own destiny by continuing to drive their\nmultiyear journey to create their own custom AI accelerators or XPU racks,\nas we call them. And I'm pleased today to report that during this quarter,\nwe acquired a fifth XPU customer through a $1 billion order placed for\ndelivery in late 2026.\n\n\nNow moving on to AI networking. Demand here has even been stronger as we\nsee customers build out their data center infrastructure ahead of deploying\nAI accelerators. Our current order backlog for AI switches exceeds $10\nbillion as our latest 102-terabit per second Tomahawk 6 switch, the first\nand only one of its capability out there, continues to book at record\nrates. And this is just a subset of what we have.\n\n\nWe have also secured record orders on DSPs, optical components like lasers\nand PCI Express switches to be deployed -- all to be deployed in AI data\ncenters. And all these components, combined with our XPUs, bring our total\norder on hand in excess of $73 billion today, which is almost half\nBroadcom's consolidated backlog of $162 billion. We expect this $73 billion\nin AI backlog to be delivered over the next 18 months. And in Q1 fiscal\n'26, we expect our AI revenue to double year-on-year to $8.2 billion.\n\n\nTurning to non-AI semiconductors. Q4 revenue of $4.6 billion was up 2% year-\non-year and up 16% sequentially based on favorable wireless seasonality.\nYear-on-year, broadband showed solid recovery, wireless was flat and all\nthe other end markets were down as enterprise spending continued to show\nlimited signs of recovery. And accordingly, in Q1, we forecast non-AI-\nsemiconductor revenue to be approximately $4.1 billion, flat from a year\nago, down sequentially due to wireless seasonality.\n\n\nLet me now talk about our infrastructure software segment. Q4\ninfrastructure software revenue of $6.9 billion was up 19% year-on-year and\nabove our outlook of $6.7 billion. Bookings continue to be strong as total\ncontract value booked in Q4 exceeded $10.4 billion versus $8.2 billion a\nyear ago. We ended the year with $73 billion of infrastructure software\nbacklog, up from $49 billion a year ago. We expect renewals to be seasonal\nin Q1 and forecast infrastructure software revenue to be approximately $6.8\nbillion. We still expect, however, that for fiscal '26, infrastructure\nsoftware revenue to grow low double-digit percentage.\n\n\nSo here's what we see in 2026. Directionally, we expect AI revenue to\ncontinue to accelerate and drive most of our growth, and non-AI\nsemiconductor revenue to be stable. Infrastructure software revenue will\ncontinue to be driven by VMware growth at low double digits. And for Q1\n'26, we expect consolidated revenue of approximately $19.1 billion, up 28%\nyear-on-year. And we expect adjusted EBITDA to be approximately 67% of\nrevenue.\n\n\nAnd with that, let me turn the call over to Kirsten.\n\n\nKirsten M. Spears\nCFO & Chief Accounting Officer\n\nThank you, Hock. Let me now provide additional detail on our Q4 financial\nperformance. Consolidated revenue was a record $18 billion for the quarter,\nup 28% from a year ago. Gross margin was 77.9% of revenue in the quarter,\nbetter than we originally guided on higher software revenues and product\nmix within semiconductors.\n\n\nConsolidated operating expenses were $2.1 billion, of which $1.5 billion\nwas research and development. Q4 operating income was a record $11.9\nbillion, up 35% from a year ago. Now on a sequential basis, even as gross\nmargin was down 50 basis points on semiconductor product mix, operating\nmargin increased 70 basis points sequentially to 66.2% on favorable\noperating leverage. Adjusted EBITDA of $12.2 billion, or 68% of revenue,\nwas above our guidance of 67%. This figure excludes $148 million of\ndepreciation.\n\n\nNow a review of the P&L for our 2 segments, starting with semiconductors.\nRevenue for our semiconductor solutions segment was a record $11.1 billion\nwith growth accelerating to 35% year-on-year, driven by AI. Semiconductor\nrevenue represented 61% of total revenue in the quarter. Gross margin for\nour semiconductor solutions segment was approximately 68%. Operating\nexpenses increased 16% year-on-year to $1.1 billion on increased investment\nin R&D for leading-edge AI semiconductors. Semiconductor operating margin\nof 59% was up 250 basis points year-on-year.\n\n\nNow moving to infrastructure software. Revenue for infrastructure software\nof $6.9 billion was up 19% year-on-year and represented 39% of total\nrevenue. Gross margin for infrastructure software was 93% in the quarter\ncompared to 91% a year ago. Operating expenses were $1.1 billion in the\nquarter, resulting in infrastructure software operating margin of 78%. This\ncompares to operating margin of 72% a year ago, reflecting the completion\nof the integration of VMware.\n\n\nMoving on to cash flow. Free cash flow in the quarter was $7.5 billion and\nrepresented 41% of revenue. We spent $237 million on capital expenditures.\nDays sales outstanding were 36 days in the fourth quarter, compared to 29\ndays a year ago. We ended the fourth quarter with inventory of $2.3\nbillion, up 4% sequentially. Our days of inventory on hand were 58 days in\nQ4, compared to 66 days in Q3, as we continue to remain disciplined on how\nwe manage inventory across the ecosystem.\n\n\nWe ended the fourth quarter with $16.2 billion of cash, up $5.5 billion\nsequentially on strong cash flow generation. The weighted average coupon\nrate in years to maturity of our gross principal fixed rate debt of $67.1\nbillion is 4% and 7.2 years, respectively.\n\n\nTurning to capital allocation. In Q4, we paid stockholders $2.8 billion of\ncash dividends based on a quarterly common stock cash dividend to $0.59 per\nshare. In Q1, we expect the non-GAAP diluted share count to be\napproximately 4.97 billion shares, excluding the potential impact of any\nshare repurchases.\n\n\nNow let me recap our financial performance for fiscal year 2025. Our\nrevenue hit a record $63.9 billion with organic growth accelerating to 24%\nyear-on-year. Semiconductor revenue was $36.9 billion, up 22% year-over-\nyear. Infrastructure software revenue was $27 billion, up 26% year-on-year.\nFiscal 2025 adjusted EBITDA was $43 billion and represented 67% of revenue.\nFree cash flow grew 39% year-on-year to $26.9 billion.\n\n\nFor fiscal 2025, we returned $17.5 billion of cash to shareholders in the\nform of $11.1 billion of dividends and $6.4 billion in share repurchases\nand elimination.\n\n\nAligned with our ability to generate increased cash flows in the preceding\nyear, we are announcing an increase in our quarterly common stock cash\ndividend in Q1 fiscal 2026 to $0.65 per share, an increase of 10% from the\nprior quarter. We intend to maintain this target quarterly dividend\nthroughout fiscal '26, subject to quarterly Board approval. This implies\nour fiscal 2026 annual common stock dividend to be a record $2.60 per\nshare, an increase of 10% year-on-year. I would like to highlight that this\nrepresents the 15th consecutive increase in annual dividends since we\ninitiated dividends in fiscal 2011. The Board also approved an extension of\nour share repurchase program, of which $7.5 billion remains, through the\nend of calendar year 2026.\n\n\nNow moving to guidance. Our guidance for Q1 is for consolidated revenue of\n$19.1 billion, up 28% year-on-year. We forecast semiconductor revenue of\napproximately $12.3 billion, up 50% year-on-year. Within this, we expect Q1\nAI semiconductor revenue of $8.2 billion, up approximately 100% year-on-\nyear. We expect infrastructure software revenue of approximately $6.8\nbillion, up 2% year-on-year.\n\n\nFor your modeling purposes, we expect Q1 consolidated gross margin to be\ndown approximately 100 basis points sequentially, primarily reflecting a\nhigher mix of AI revenue. As a reminder, consolidated gross margins through\nthe year will be impacted by the revenue mix of infrastructure software and\nsemiconductors and also product mix within semiconductors.\n\n\nWe expect Q1 adjusted EBITDA to be approximately 67%. We expect the non-\nGAAP tax rate for Q1 and fiscal year 2026 to increase from 14% to\napproximately 16.5% due to the impact of the global minimum tax and shift\nin geographic mix of income compared to that of fiscal year 2025.\nThat concludes my prepared remarks. Operator, please open up the call for\nquestions.\n\n\nQuestion and Answer\n\n\nOperator\n\n[Operator Instructions] Our first question will come from the line of Vivek\nArya with Bank of America.\n\n\nVivek  Arya\nBofA Securities, Research Division\n\nJust wanted to clarify, Hock, you said $73 billion over 18 months for AI,\nthat's roughly $50-ish billion plus for fiscal '26 for AI. I just wanted to\nget -- make sure I got that right.\n\n\nAnd then the main question, Hock, is that there is sort of this emerging\ndebate about customer-owned tooling, your ASIC customers potentially\nwanting to do more things on their own. How do you see your XPU content and\nshare at your largest customer evolve over the next 1 or 2 years?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, to answer your first question, what we said is correct that as of\nnow, we have $73 billion of backlog in place secured of XPUs, switches,\nDSPs, lasers for AI data centers that we anticipate shipping over the next\n18 months. And obviously, this is as of now. I mean we fully expect more\nbookings to come in over that period of time. And so don't take that $73\nbillion as that's the revenue that we ship over the next 18 months. We're\njust saying we have that now and that bookings has been accelerating.\n\n\nAnd frankly, we see that bookings not just in XPUs, but in switches, DSPs,\nall the other components that go into AI data centers. We have never seen\nbookings of the nature that what we have seen over the past 3 months,\nparticularly with respect to Tomahawk 6 switches. This is one of the\nfastest-growing products in terms of deployment that we've ever seen of any\nswitch products that we've put out there. It is pretty interesting and\npartly because it's the only one of its kind out there at this point at 102\nterabits per second. And that's the exact product needed to expand the\nclusters of the latest GPU and XPUs out there.\n\n\nSo that's great. But as far as what is the future XPU is your broader\nquestion, my answer to you is don't follow what you hear out there as\ngospel. It's a trajectory. It's a multiyear journey. And many of the\nplayers, and not too many players, doing LLMs want to do their own custom\nAI accelerator for very good reasons. You can put in hardware, if you use a\ngeneral purpose GPU, you can only do in software and kernels and software.\n\n\nYou can achieve, performance-wise, so much better in the custom purpose-\ndesigned, hardware-driven XPU. And we see that in the TPU and we see that\nin all the accelerators we are doing for our other customers. Much, much\nbetter in areas of sparse core, training, inference, reasoning, all that\nstuff.\n\n\nNow, will that mean that over time, they all want to go do it themselves,\nnot necessarily. And in fact, because the technology in silicon keeps\nupdating, keeps evolving. And if you are an LLM player, where do you put\nyour resources in order to compete in this space, especially when you have\nto compete at the end of the day against merchant GPU who are not slowing\ndown in the rate of evolution. So I see that as this concept of customer\ntooling is an overblown hypothesis, which frankly, I don't think will\nhappen.\n\n\nOperator\n\nA moment for our next question, and that will come from the line of Ross\nSeymore with Deutsche Bank.\n\n\nRoss Clark Seymore\nDeutsche Bank AG, Research Division\n\nHock, I want to go to something you touched on earlier about the TPUs going\na little bit more to like a merchant go-to market to other customers. Do\nyou believe that's a substitution effect for customers who otherwise would\nhave done ASICs with you? Or do you think it's actually broadening the\nmarket? And so what are kind of the financial implications of that from\nyour perspective?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nSo that's a very good question, Ross. And what we see right now is the most\nobvious move it does is it goes -- the people who use TPUs, the alternative\nis GPUs, merchant basis, as the most common thing that happens. Because to\ndo that substitution for another custom, it's different. To make an\ninvestment in custom accelerator is a multiyear journey. It's a strategic\ndirectional thing. It's not necessarily a very transactional or short-term\nmove.\n\n\nMoving from GPU to TPU is a transactional move. Going into AI accelerator\nof your own is a long-term strategic move and nothing would deter you from\nthere to continue to make that investment towards that end goal of\nsuccessfully creating and deploying your own custom AI accelerator. So\nthat's the motion we see.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of\nHarlan Sur with JPMorgan.\n\n\nHarlan L.  Sur\nJPMorgan Chase & Co, Research Division\n\nCongratulations on the strong results, guidance and execution. Hock, again,\nI just want to reiterate -- I just want to sort of verify this, right? So\nyou talked about total AI backlog of $73 billion over the next 6 quarters,\nright? This is just a snapshot of your order book like right now. But given\nyour lead times, I think customers can and still will place orders for AI\nin quarters 4, 5 and 6. So as time moves forward, that backlog number for\nmore shipments in the second half of '26 will probably still go up, right?\nIs that the correct interpretation?\n\n\nAnd then given the strong and growing backlog, right, the question is, does\nthe team have 3-nanometer, 2-nanometer wafer supply, colos, substrate, HBM\nsupply commitments to support all of the demand in your order book? And I\nknow one of the areas where you are trying to mitigate this is in advanced\npackaging, right? You're bringing up your Singapore facility. Can you guys\njust remind us what part of the advanced packaging process the team is\nfocusing on with the Singapore facility?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, to answer your first simpler question, Harlan, you're right. You can\nsay that $73 billion is the backlog we have today to ship over the next 6\nquarters. You might also say that given our lead time, we expect more\norders to be able to be absorbed into our backlog for shipments over the\nnext 6 quarters.\n\n\nSo taking that, we expect revenue -- a minimum revenue, one way to look at\nit, of $73 billion over the next 6 quarters, but we do expect much more as\nmore orders come in for shipments within the next 6 quarters. Our lead\ntime, depending on the particular product it is, can be anywhere from 6\nmonths to a year.\n\n\nWith respect to supply chain is what you're asking, critical supply chain\non silicon and packaging, yes, that's an interesting challenge that we have\nbeen addressing constantly and continue to. And with the strength of the\ndemand and the need for more innovative packaging, advanced packaging,\nbecause you are talking about multi-chips in creating every custom\naccelerator now, the packaging becomes a very interesting and technical\nchallenge. And building our Singapore fab is to really talk about partially\nin-sourcing those advanced packaging.\n\n\nWe believe that we have enough demand, we can literally in-source not --\nfrom the viewpoint of not just costs, but in a viewpoint of supply chain\nsecurity and delivery. We're building up a fairly substantial facility for\npackaging -- advanced packaging in Singapore, as indicated, purely for the\npurpose to address the advanced packaging side.\n\n\nSilicon-wise, now we go back to the same precious source in Taiwan, TSMC.\nAnd so we keep going for more and more capacity in 2-nanometers, 3-\nnanometers. And so far, we do not have that constraint. But again, time\nwill tell as we progress and as our backlog builds up.\n\n\nOperator\n\nThe next question will come from the line of Blayne Curtis with Jefferies.\n\n\nBlayne Peter Curtis\nJefferies LLC, Research Division\n\nI wanted to ask, with the original $10 billion deal, you talked about a\nrack sale, I just wanted to -- with the follow-on order as well as the\nfifth customer, can you just maybe describe how you're going to deliver\nthose? Is it an XPU or is it a rack?\n\n\nAnd then maybe you can kind of just walk us through the math and kind of\nwhat the deliverable is? Obviously, Google uses its own networking. So I'm\nkind of curious, too, would it be a copy exact of what Google does, now\nthat you could talk [ to it by ] name? Or would you have your own\nnetworking in there as well?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nThat's a very complicated question, Blayne. Let me tell you what it is,\nit's a system sale. How about that? It's a real system sale. We have so\nmany components beyond XPUs, custom accelerators in any system -- in AI\nsystem, any AI system used by hyperscalers that, yes, we believe it begins\nto make sense to do it as a system sale and be responsible, but be fully\nresponsible for the entire system or rack, as you call it. I think people\nare understanding as a system sale better. And so on this customer number\n4, we are selling it as a system with our key components in it. And that's\nno different than selling a chip. We certify a final ability to run as part\nof the whole selling process.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of Stacy\nRasgon with Bernstein.\n\n\nStacy Aaron Rasgon\nSanford C. Bernstein & Co., LLC., Research Division\n\nI wanted to touch on gross margins and maybe it feeds into a little bit the\nprior question. So I understand why the AI business is somewhat dilutive to\ngross margins. We have the HBM pass-through. And then presumably with the\nsystem sales, that will be more dilutive. And you hinted at this in the\npast, but I was wondering if it could be a little more explicit. As this AI\nrevenue starts to ramp, as we start to get system sales, how should we be\nthinking about that gross margin number? Say, if we're looking out 4\nquarters or 6 quarters, is it low 70s? I mean could it start with the 6 at\nthe corporate level?\n\n\nAnd, I guess, I'm also wondering -- I understand how that comes down, but\nwhat about the operating margins? Do you think you get enough operating\nleverage on the OpEx side to keep operating margins flat? Or do they need\nto come down as well?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nI'll let Kirsten give you the details, but enough for me to broadly, high\nlevel, explain to you, Stacy. Good question. Phenomenal. You don't see that\nimpacting us right now, and we have already started that process of some\nsystems sales. You don't see that in our numbers, but [ it will ]. And we\nhave said that openly. The AI revenue has a lower gross margin than our --\nobviously, the rest of our business including software, of course.\n\n\nBut we expect the rate of growth of -- as we do more and more AI revenue to\nbe so much that we get the operating leverage on our operating spending\nthat operating margin will deliver dollars that are still high level of\ngrowth from what it has been. So we expect operating leverage to benefit us\nat the operating margin level, even as gross margin will start to\ndeteriorate, high level.\n\n\nKirsten M. Spears\nCFO & Chief Accounting Officer\n\nNow, I think Hock said that fairly. And the second half of the year when we\ndo start shipping more systems, the situation is straightforward. We'll be\npassing through more components that are not ours. So think of it similar\nto the XPUs where we have memory on those XPUs and we're passing through\nthose costs. We'll be passing through more cost within the rack. And so\nthose gross margins will be lower. However, overall, the way Hock said it,\ngross margin dollars will go up, margins will go down, operating margins,\nbecause we have leverage, operating margin dollars will go up, but the\nmargin itself as a percentage of revenues will come down a bit. But we're\nnot -- I mean we'll guide closer to the end of the year for that.\n\n\nOperator\n\nOne moment for our next question, that will come from the line of Jim\nSchneider with Goldman Sachs.\n\n\nJames Edward Schneider\nGoldman Sachs Group, Inc., Research Division\n\nHock, I was wondering if you might care to calibrate your expectations for\nAI revenue in fiscal '26 a little bit more closely. I believe you talked\nabout acceleration in fiscal '26 off of the 65% growth rate you did in\nfiscal '25. And then you're guiding to 100% growth for Q1. So I'm just\nwondering if the Q1 is a good jumping off point for the growth rate you\nexpect for the full year or something maybe a little bit less than that.\n\n\nAnd then maybe if you could separately clarify whether your $1 billion of\norders for the fifth customer is indeed OpenAI, which you made a separate\nannouncement about.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWow, there's a lot of questions here. But let me start off with '26. Our\nbacklog is very dynamic these days, as I said, and it is continuing to ramp\nup. And you're right, we originally, 6 months ago, said, maybe year-on-year\nAI revenues would grow in '26, 60%, 70%. Q1, we doubled. And Q1 '26 today,\nwe're saying it's doubled. And we're looking at it because all the fresh\norders keeps coming in, and we give you a milestone of where we are today,\nwhich is $73 billion of backlog to be shipped over the next 18 months. And\nwe do fully expect, as I answered the earlier question, for that $73\nbillion over the 18 months to keep growing.\n\n\nNow it's a moving number as we move in time, but it will grow. And it's\nhard for me to pinpoint what '26 is going to look like precisely. So I'd\nrather not give you guys any guidance, and that's why we don't give you\nguidance, but we do give it for Q1. Give it time, we'll give it for Q2. And\nyou're right to ask is it an accelerating trend? And my answer is it's\nlikely to be an accelerating trend as we progress through '26. I hope that\nanswers your question.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of Ben\nReitzes with Melius Research.\n\n\nBenjamin Alexander Reitzes\nMelius Research LLC\n\nHock, I wanted to ask, I'm not sure if the last caller said something on\nit, but I didn't hear it in the answer was -- I wanted to ask about the\nOpenAI contract. It's supposed to start in the second half of the year and\ngo through 2029 for 10 gigawatts. I'm going to assume that, that's the\nfifth customer order there. And I was just wondering if you're still\nconfident in that being a driver. Are there any obstacles to making that a\nmajor driver? And when you expect that to contribute and your confidence in\nit?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nYou didn't hear that answer from my last caller, Jim's question, because I\ndidn't answer it. I did not answer it and I'm not answering it either. It's\nthe fifth customer, and it's a real customer and it will grow. They are on\ntheir multiyear journey to their own XPUs. And let's leave it at that.\n\n\nAs far as the OpenAI view that you have, we appreciate the fact that it is\na multiyear journey that will run through '29 as our press release with\nOpenAI showed, 10 gigawatts between '26 -- more like '27, '28, '29, Ben,\nnot '26. It's more like '27, '28, '29, 10 gigawatts. That was the OpenAI\ndiscussion. And that's, I call it, an agreement and alignment of where\nwe're headed with respect to various respected and valued customer, OpenAI.\nBut we do not expect much in '26.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of C.J.\nMuse with Cantor Fitzgerald.\n\n\nChristopher James Muse\nCantor Fitzgerald & Co., Research Division\n\nI guess, Hock, I wanted to talk about custom silicon and maybe speak to how\nyou expect content to grow for Broadcom generation to generation. And as\npart of that, your competitor announced CPX offering, essentially\naccelerator for -- an accelerator for massive context windows. I'm curious\nif you see a broadening opportunity for your existing 5 customers to have\nmultiple XPU offerings.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nThank you. No, yes, you hit it right on. I mean the nice thing about a\ncustom accelerator is you try not to do one-size-fits-all and\ngenerationally. Each of these 5 customers now can create their version of\nan XPU custom accelerator for training and inference. And basically, it's\nalmost 2 parallel threads going on almost simultaneously for each of them.\nSo I don't have plenty of versions to deal with. I don't need to create any\nmore version. We've got plenty of different content out there just on the\nbasis of creating these custom accelerators.\n\n\nAnd by the way, when you do custom accelerators, you tend to put more\nhardware in that unique differentiated versus trying to make it work on\nsoftware and creating kernels into software. I know that's very tricky too.\nBut thinking about the difference where you can create in hardware, those\nsparse core data routers versus the dense matrix multipliers, all in one\nsame chip.\n\n\nAnd that's many -- just one example of what creating custom accelerators is\nletting us do, or for that matter, a variation in how much memory capacity\nor memory bandwidth from -- for the same customer from chip to chip, just\nbecause even in inference, you want to do more reasoning versus decoding\nversus something else like prefill.\n\n\nSo you literally start to create different hardware for different aspects\nof how you want to train or inference and run your workloads. It's a very\nfascinating area, and we are seeing a lot of variations and multiple chips\nfor each of our customers.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of Harsh\nKumar with Piper Sandler.\n\n\nHarsh V. Kumar\nPiper Sandler & Co., Research Division\n\nYes, Hock and team, first of all, congratulations on some pretty stunning\nnumbers. I've got an easy one and a more strategic one. The easy one is,\nyour guide in AI, Hock and Kirsten, is calling for almost $1.7 billion of\nsequential growth. I was curious maybe you can talk about the diversity of\nthe growth between the 3 existing customers. Is it pretty well spread out?\nAll of them growing? Or is one sort of driving much of the growth?\n\n\nAnd then, Hock, strategically, one of your competitors bought a photonic\nfabric company recently. I was curious about your take on that technology\nand if you think it's disruptive or you think it's just gimmickry at this\npoint in time.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nI like the way you address this question because the way that you address\nthe tech question to me. It's almost hesitant. Thank you. I appreciate\nthat.\n\n\nBut on your first part, yes, we are driving growth and it began to feel\nlike this thing never ends, and it's a real mixed bag of existing customers\nand on existing XPUs. And a big part of it is XPUs that we're seeing. And\nthat's not to slow down the fact that, as I indicated in my remarks and\ncommented on the demand for switches, not just Tomahawk 6, Tomahawk 5\nswitches, the demand for our latest 1.6  terabit per second DSPs that\nenables optical interconnects for scale out particularly. It's just very,\nvery strong. And by extension, demand for the optical components like\nlasers, PIN diodes just going nuts. All that come together.\n\n\nNow all that is small, relatively lesser dollars when it comes to XPUs, as\nyou probably guess. I mean, to give you a sense, maybe let me look at it on\na backlog side. Of the $73 billion of AI revenue backlog over the next 18\nmonths, I talked about, maybe $20 billion of it is everything else. The\nrest is XPUs. Hope that gives you a sense of what the mix is. But\n[indiscernible] the rest is still $20 billion, that's not small by any\nmeans. So we value that.\n\n\nSo when you talk about your next question of silicon photonics as a means\nto create basically much better, more efficient, lower power interconnects\nin not just scale-out, but hopefully, scale-up, yes, I could see a point in\ntime in the future when silicon photonics method is the only way to do it.\nWe're not quite there yet. But we have the technology and we continue to\ndevelop the technology, even at each time we develop it first for 400\ngigabit bandwidth, then going on to 800 gigabit bandwidth, not ready for it\nyet.\n\n\nAnd even with the product -- and we're now doing it for 1.6 terabit\nbandwidth to create silicon photonics switches, silicon photonics\ninterconnects, not even sure it will get fully deployed because engineers --\n our engineers, our peers -- and the peers we have out there will somehow\ntry to find a way to still do -- try to do scale-up within a rack in copper\nas long as possible and in scale-out in non-pluggable optics.\n\n\nThe final, final straw is when you can do it well in pluggable optics. And\nof course, when you can do it even in copper, then you're right, you go to\nsilicon photonics and it will happen, and we're ready for it, just saying\nnot anytime soon.\n\n\nOperator\n\nOne moment for our next question, that will come from the line of Karl\nAckerman with BNP Paribas.\n\n\nKarl  Ackerman\nBNP Paribas, Research Division\n\nHock, could you speak to the supply chain resiliency and visibility you\nhave with your key material suppliers, particularly CoWoS as you not only\nsupport your existing customer programs but the two new custom compute\nprocessors that you announced intra-quarter. I guess, what I can get at is\nyou also happen to address the very large subset of networking and compute\nAI supply chains. You talked about record backlog. If you were to pinpoint\nsome of the bottlenecks that you have, the areas that you're aiming to\naddress and mitigate from supply chain bottlenecks, what would they be? And\nhow do you see that ameliorating into '26?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nIt's across the board, typically. I mean it's -- we are very fortunate in\nsome ways that we have the product, technology and the operating business\nlines to create multiple key leading-edge components that enables today's\nstate-of-the-art AI data centers. I mean our DSP, as I said earlier, is now\nat 1.6 terabit per second. That's the leading edge connectivity for\nbandwidth for this -- for the top of the [indiscernible] XPU and even GPU.\nAnd we intend to be that way. And we have the lasers, EMLs, VCSELs, CW\nlasers that goes with it.\n\n\nSo it's fortunate that we have all this and the key active components that\ngo with it. And we see it very early, and we expand the capacity as we do\nthe design to match it. And long -- this is a long answer to what I'm\ntrying to get at, which is, I think we are -- of any of these data center\nsuppliers of the system racks, not counting the power shell and all that.\nNow that starts to get beyond us on the power shell and the transformers\nand the gas turbines.\n\n\nIf you just look at the rack, the systems on AI, we probably have a good\nhandle on where the bottlenecks are because sometimes we are part of the\nbottlenecks, which we then want to get resolved. So we feel pretty good\nabout that through 2026.\n\n\nOperator\n\nOne moment for our next question, that will come from the line of\nChristopher Rolland with Susquehanna.\n\n\nChristopher Adam Jackson Rolland\nSusquehanna Financial Group, LLLP, Research Division\n\nJust first, a clarification and then my question. And sorry to come back to\nthis issue. But if I understand you correctly, Hock, I think you were\nsaying that OpenAI would be a general agreement, so it's not binding, maybe\nsimilar to the agreements with both NVIDIA and AMD.\n\n\nAnd then secondly, you talked about flat non-AI semiconductor revenue,\nmaybe what's going on there? Is there still an inventory overhang? And what\ncould -- what do we need to get that going again? Do you see growth\neventually in that business?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, on the non-AI semiconductor, we see broadband literally recovering\nvery well. And we don't see the others -- no, we see stability. We don't\nsee a sharp recovery that is sustainable yet. So I guess, given a couple\nmore quarters, but we don't see any further deterioration in demand. And\nit's more, I think, maybe AI is sucking the oxygen a lot out of enterprise\nspending elsewhere and hyperscaler spending elsewhere. We don't see getting\nany worse. We don't see it recovering very quickly with the exception of\nbroadband. That's a simple summary of non-AI.\n\n\nWith respect to OpenAI, without diving in too deep, I'm just telling you\nwhat that 10 gigawatt announcement is all about. Separately, the journey\nwith them on the custom accelerator progresses at a very advanced stage and\nwill happen very, very quickly. And it's -- and we will have a committed\nelement to this whole thing, and [ it will ]. But what I was articulating\nearlier was the 10 gigawatt announcement. And that 10 gigawatt announcement\nis an agreement to be aligned on developing 10 gigawatts for OpenAI over\n'27 to '29 time frame. That's different from the XPU program we're\ndeveloping with them.\n\n\nOperator\n\nAnd we do have time for one final question, and that will come from the\nline of Joe Moore with Morgan Stanley.\n\n\nJoseph Lawrence Moore\nMorgan Stanley, Research Division\n\nGreat. So if you have $21 billion of rack revenue in the second half of\n'26, I guess, do we stay at that run rate beyond that? Are you going to\ncontinue to sell racks? Or does that sort of -- that type of business mix\nshift over time? And I'm really just trying to figure out the percentage of\nyour 18-month backlog that's actually full systems at this point.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, it's an interesting question. And that question basically comes to\nhow much compute capacity is needed by our customers over the next, as I\nsay, over the period beyond 18 months. And your guess is probably as good\nas mine based on what we all know out there, which is really what they\nrelate to. But if they need more, then you see that continuing even larger.\nIf they don't need it, then probably it won't. But as of -- what we are\ntrying to indicate is that's the demand we are seeing over that period of\ntime right now.\n\n\nOperator\n\nI would now like to turn the call back over to Ji Yoo for any closing\nremarks.\n\n\nJi  Yoo\nDirector of Investor Relations\n\nThank you, operator. This quarter, Broadcom will be presenting at the New\nStreet Research Virtual AI Big Idea Conference on Monday, December 15,\n2025. Broadcom currently plans to report its earnings for the first quarter\nof fiscal year 2026 after close of market on Wednesday, March 4, 2026. A\npublic webcast of Broadcom's earnings conference call will follow at 2:00\np.m. Pacific.\n\n\nThat will conclude our earnings call today. Thank you all for joining.\nOperator, you may end the call.\n\n\nOperatorThis concludes today's program. Thank you all for participating.\nYou may now disconnect.\nCopyright Â© 2025 by S&P Global Market Intelligence, a division of S&P\nGlobal Inc. All rights reserved.\n\n\nThese materials have been prepared solely for information purposes based\nupon information generally available to the public and from sources\nbelieved to be reliable. No content (including index data, ratings, credit-\nrelated analyses and data, research, model, software or other application\nor output therefrom) or any part thereof (Content) may be modified, reverse\nengineered, reproduced or distributed in any form by any means, or stored\nin a database or retrieval system, without the prior written permission of\nS&P Global Market Intelligence or its affiliates (collectively, S&P\nGlobal). The Content shall not be used for any unlawful or unauthorized\npurposes. S&P Global and any third-party providers, (collectively S&P\nGlobal Parties) do not guarantee the accuracy, completeness, timeliness or\navailability of the Content. S&P Global Parties are not responsible for any\nerrors or omissions, regardless of the cause, for the results obtained from\nthe use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P\nGLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS\nFOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR\nDEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE\nCONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no\nevent shall S&P Global Parties be liable to any party for any direct,\nindirect, incidental, exemplary, compensatory, punitive, special or\nconsequential damages, costs, expenses, legal fees, or losses (including,\nwithout limitation, lost income or lost profits and opportunity costs or\nlosses caused by negligence) in connection with any use of the Content even\nif advised of the possibility of such damages. S&P Global Market\nIntelligence's opinions, quotes and credit-related and other analyses are\nstatements of opinion as of the date they are expressed and not statements\nof fact or recommendations to purchase, hold, or sell any securities or to\nmake any investment decisions, and do not address the suitability of any\nsecurity. S&P Global Market Intelligence may provide index data. Direct\ninvestment in an index is not possible. Exposure to an asset class\nrepresented by an index is available through investable instruments based\non that index. S&P Global Market Intelligence assumes no obligation to\nupdate the Content following publication in any form or format. The Content\nshould not be relied on and is not a substitute for the skill, judgment and\nexperience of the user, its management, employees, advisors and/or clients\nwhen making investment and other business decisions. S&P Global Market\nIntelligence does not act as a fiduciary or an investment advisor except\nwhere registered as such. S&P Global keeps certain activities of its\ndivisions separate from each other in order to preserve the independence\nand objectivity of their respective activities. As a result, certain\ndivisions of S&P Global may have information that is not available to other\nS&P Global divisions. S&P Global has established policies and procedures to\nmaintain the confidentiality of certain nonpublic information received in\nconnection with each analytical process.\n\n\nS&P Global may receive compensation for its ratings and certain analyses,\nnormally from issuers or underwriters of securities or from obligors. S&P\nGlobal reserves the right to disseminate its opinions and analyses. S&P\nGlobal's public ratings and analyses are made available on its Web sites,\nwww.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and\nwww.globalcreditportal.com (subscription), and may be distributed through\nother means, including via S&P Global publications and third-party\nredistributors. Additional information about our ratings fees is available\nat www.standardandpoors.com/usratingsfees.\nÂ© 2025 S&P Global Market Intelligence.\n\n",
  "presentation_text": "Operator\n\nWelcome to Broadcom Inc. Fourth Quarter and Fiscal Year 2025 Financial\nResults Conference Call.\n\n\nAt this time, for opening remarks and introductions, I would like to turn\nthe call over to Ji Yoo, Head of Investor Relations of Broadcom Inc.\n\n\nJi  Yoo\nDirector of Investor Relations\n\nThank you, Sheri, and good afternoon, everyone. Joining me on today's call\nare Hock Tan, President and CEO; Kirsten Spears, Chief Financial Officer;\nand Charlie Kawwas, President, Semiconductor Solutions Group.\n\n\nBroadcom distributed a press release and financial tables after the market\nclose, describing our financial performance for the fourth quarter and\nfiscal year 2025. If you did not receive a copy, you may obtain the\ninformation from the Investors section of Broadcom's website at\nbroadcom.com.\n\n\nThis conference call is being webcast live, and an audio replay of the call\ncan be accessed for 1 year through the Investors section of Broadcom's\nwebsite.\n\n\nDuring the prepared remarks, Hock and Kirsten will be providing details of\nour fourth quarter and fiscal year 2025 results, guidance for our first\nquarter of fiscal year 2026 as well as commentary regarding the business\nenvironment. We'll take questions after the end of our prepared comments.\n\n\nPlease refer to our press release today and our recent filings with the SEC\nfor information on the specific risk factors that could cause our actual\nresults to differ materially from the forward-looking statements made on\nthis call.\n\n\nIn addition to U.S. GAAP reporting, Broadcom reports certain financial\nmeasures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP\nmeasures is included in the tables attached to today's press release.\nComments made during today's call will primarily refer to our non-GAAP\nfinancial results.\n\n\nI'll now turn the call over to Hock.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nThank you, Ji, and thank you, everyone, for joining us today. Well, we just\nended our Q4 fiscal '25. And before I get into details of that quarter, let\nme recap the year.\n\n\nIn our fiscal 2025, consolidated revenue grew 24% year-over-year to a\nrecord $64 billion, and it's driven by AI semiconductors and VMware. AI\nrevenue grew 65% year-over-year to $20 billion, driving the semiconductor\nrevenue for this company to a record $37 billion for the year. In our\ninfrastructure software business, strong adoption of VMware Cloud\nFoundation or VCF, as we call it, drove revenue growth of 26% year-on-year\nto $27 billion.\n\n\nIn summary, 2025 was another strong year for Broadcom. And we see the\nspending momentum by our customers for -- in AI, continuing to accelerate\nin 2026. Now let's move on to the results of our fourth quarter 2025.\n\n\nTotal revenue was a record $18 billion, up 28% year-on-year, and above our\nguidance on better-than-expected growth in AI semiconductors as well as\ninfrastructure software. Q4 consolidated adjusted EBITDA was a record\n$12.12 billion (sic) [ $12.218 billion ], up 34% year-on-year. So let me\ngive you more color on our two segments.\n\n\nIn semiconductors, revenue was $11.1 billion as year-on-year growth\naccelerated to 35%. And this robust growth was driven by the AI\nsemiconductor revenue of $6.5 billion, which was up 74% year-on-year. And\nthis represents a growth trajectory exceeding 10x over the 11 quarters we\nhave reported this line of business. Our custom accelerated business more\nthan doubled year-over-year, as we see our customers increase adoption of\nXPUs, as we call those custom accelerators, in training their LLMs and\nmonetizing their platforms through inferencing APIs and applications.\n\n\nThese XPUs, I may add, are not only being used to train and inference\ninternal workloads by our customers, the same XPUs, in some situations,\nhave been extended externally to other LLM peers, best exemplified at\nGoogle, where the TPUs used in creating Gemini have also been used for AI\ncloud computing by Apple, Cohere and SSI as an example. And the scale at\nwhich we see this happening could be significant.\n\n\nAnd as you are aware, last quarter, Q3 '25, we received a $10 billion order\nto sell the latest TPU Ironwood racks to Anthropic. And this was our fourth\ncustomer that we mentioned. And in this quarter Q4, we received an\nadditional $11 billion order from the same customer for delivery in late\n2026.\n\n\nBut that does not mean our other two customers are using TPUs. In fact,\nthey prefer to control their own destiny by continuing to drive their\nmultiyear journey to create their own custom AI accelerators or XPU racks,\nas we call them. And I'm pleased today to report that during this quarter,\nwe acquired a fifth XPU customer through a $1 billion order placed for\ndelivery in late 2026.\n\n\nNow moving on to AI networking. Demand here has even been stronger as we\nsee customers build out their data center infrastructure ahead of deploying\nAI accelerators. Our current order backlog for AI switches exceeds $10\nbillion as our latest 102-terabit per second Tomahawk 6 switch, the first\nand only one of its capability out there, continues to book at record\nrates. And this is just a subset of what we have.\n\n\nWe have also secured record orders on DSPs, optical components like lasers\nand PCI Express switches to be deployed -- all to be deployed in AI data\ncenters. And all these components, combined with our XPUs, bring our total\norder on hand in excess of $73 billion today, which is almost half\nBroadcom's consolidated backlog of $162 billion. We expect this $73 billion\nin AI backlog to be delivered over the next 18 months. And in Q1 fiscal\n'26, we expect our AI revenue to double year-on-year to $8.2 billion.\n\n\nTurning to non-AI semiconductors. Q4 revenue of $4.6 billion was up 2% year-\non-year and up 16% sequentially based on favorable wireless seasonality.\nYear-on-year, broadband showed solid recovery, wireless was flat and all\nthe other end markets were down as enterprise spending continued to show\nlimited signs of recovery. And accordingly, in Q1, we forecast non-AI-\nsemiconductor revenue to be approximately $4.1 billion, flat from a year\nago, down sequentially due to wireless seasonality.\n\n\nLet me now talk about our infrastructure software segment. Q4\ninfrastructure software revenue of $6.9 billion was up 19% year-on-year and\nabove our outlook of $6.7 billion. Bookings continue to be strong as total\ncontract value booked in Q4 exceeded $10.4 billion versus $8.2 billion a\nyear ago. We ended the year with $73 billion of infrastructure software\nbacklog, up from $49 billion a year ago. We expect renewals to be seasonal\nin Q1 and forecast infrastructure software revenue to be approximately $6.8\nbillion. We still expect, however, that for fiscal '26, infrastructure\nsoftware revenue to grow low double-digit percentage.\n\n\nSo here's what we see in 2026. Directionally, we expect AI revenue to\ncontinue to accelerate and drive most of our growth, and non-AI\nsemiconductor revenue to be stable. Infrastructure software revenue will\ncontinue to be driven by VMware growth at low double digits. And for Q1\n'26, we expect consolidated revenue of approximately $19.1 billion, up 28%\nyear-on-year. And we expect adjusted EBITDA to be approximately 67% of\nrevenue.\n\n\nAnd with that, let me turn the call over to Kirsten.\n\n\nKirsten M. Spears\nCFO & Chief Accounting Officer\n\nThank you, Hock. Let me now provide additional detail on our Q4 financial\nperformance. Consolidated revenue was a record $18 billion for the quarter,\nup 28% from a year ago. Gross margin was 77.9% of revenue in the quarter,\nbetter than we originally guided on higher software revenues and product\nmix within semiconductors.\n\n\nConsolidated operating expenses were $2.1 billion, of which $1.5 billion\nwas research and development. Q4 operating income was a record $11.9\nbillion, up 35% from a year ago. Now on a sequential basis, even as gross\nmargin was down 50 basis points on semiconductor product mix, operating\nmargin increased 70 basis points sequentially to 66.2% on favorable\noperating leverage. Adjusted EBITDA of $12.2 billion, or 68% of revenue,\nwas above our guidance of 67%. This figure excludes $148 million of\ndepreciation.\n\n\nNow a review of the P&L for our 2 segments, starting with semiconductors.\nRevenue for our semiconductor solutions segment was a record $11.1 billion\nwith growth accelerating to 35% year-on-year, driven by AI. Semiconductor\nrevenue represented 61% of total revenue in the quarter. Gross margin for\nour semiconductor solutions segment was approximately 68%. Operating\nexpenses increased 16% year-on-year to $1.1 billion on increased investment\nin R&D for leading-edge AI semiconductors. Semiconductor operating margin\nof 59% was up 250 basis points year-on-year.\n\n\nNow moving to infrastructure software. Revenue for infrastructure software\nof $6.9 billion was up 19% year-on-year and represented 39% of total\nrevenue. Gross margin for infrastructure software was 93% in the quarter\ncompared to 91% a year ago. Operating expenses were $1.1 billion in the\nquarter, resulting in infrastructure software operating margin of 78%. This\ncompares to operating margin of 72% a year ago, reflecting the completion\nof the integration of VMware.\n\n\nMoving on to cash flow. Free cash flow in the quarter was $7.5 billion and\nrepresented 41% of revenue. We spent $237 million on capital expenditures.\nDays sales outstanding were 36 days in the fourth quarter, compared to 29\ndays a year ago. We ended the fourth quarter with inventory of $2.3\nbillion, up 4% sequentially. Our days of inventory on hand were 58 days in\nQ4, compared to 66 days in Q3, as we continue to remain disciplined on how\nwe manage inventory across the ecosystem.\n\n\nWe ended the fourth quarter with $16.2 billion of cash, up $5.5 billion\nsequentially on strong cash flow generation. The weighted average coupon\nrate in years to maturity of our gross principal fixed rate debt of $67.1\nbillion is 4% and 7.2 years, respectively.\n\n\nTurning to capital allocation. In Q4, we paid stockholders $2.8 billion of\ncash dividends based on a quarterly common stock cash dividend to $0.59 per\nshare. In Q1, we expect the non-GAAP diluted share count to be\napproximately 4.97 billion shares, excluding the potential impact of any\nshare repurchases.\n\n\nNow let me recap our financial performance for fiscal year 2025. Our\nrevenue hit a record $63.9 billion with organic growth accelerating to 24%\nyear-on-year. Semiconductor revenue was $36.9 billion, up 22% year-over-\nyear. Infrastructure software revenue was $27 billion, up 26% year-on-year.\nFiscal 2025 adjusted EBITDA was $43 billion and represented 67% of revenue.\nFree cash flow grew 39% year-on-year to $26.9 billion.\n\n\nFor fiscal 2025, we returned $17.5 billion of cash to shareholders in the\nform of $11.1 billion of dividends and $6.4 billion in share repurchases\nand elimination.\n\n\nAligned with our ability to generate increased cash flows in the preceding\nyear, we are announcing an increase in our quarterly common stock cash\ndividend in Q1 fiscal 2026 to $0.65 per share, an increase of 10% from the\nprior quarter. We intend to maintain this target quarterly dividend\nthroughout fiscal '26, subject to quarterly Board approval. This implies\nour fiscal 2026 annual common stock dividend to be a record $2.60 per\nshare, an increase of 10% year-on-year. I would like to highlight that this\nrepresents the 15th consecutive increase in annual dividends since we\ninitiated dividends in fiscal 2011. The Board also approved an extension of\nour share repurchase program, of which $7.5 billion remains, through the\nend of calendar year 2026.\n\n\nNow moving to guidance. Our guidance for Q1 is for consolidated revenue of\n$19.1 billion, up 28% year-on-year. We forecast semiconductor revenue of\napproximately $12.3 billion, up 50% year-on-year. Within this, we expect Q1\nAI semiconductor revenue of $8.2 billion, up approximately 100% year-on-\nyear. We expect infrastructure software revenue of approximately $6.8\nbillion, up 2% year-on-year.\n\n\nFor your modeling purposes, we expect Q1 consolidated gross margin to be\ndown approximately 100 basis points sequentially, primarily reflecting a\nhigher mix of AI revenue. As a reminder, consolidated gross margins through\nthe year will be impacted by the revenue mix of infrastructure software and\nsemiconductors and also product mix within semiconductors.\n\n\nWe expect Q1 adjusted EBITDA to be approximately 67%. We expect the non-\nGAAP tax rate for Q1 and fiscal year 2026 to increase from 14% to\napproximately 16.5% due to the impact of the global minimum tax and shift\nin geographic mix of income compared to that of fiscal year 2025.\nThat concludes my prepared remarks. Operator, please open up the call for\nquestions.",
  "qa_text": "Operator\n\n[Operator Instructions] Our first question will come from the line of Vivek\nArya with Bank of America.\n\n\nVivek  Arya\nBofA Securities, Research Division\n\nJust wanted to clarify, Hock, you said $73 billion over 18 months for AI,\nthat's roughly $50-ish billion plus for fiscal '26 for AI. I just wanted to\nget -- make sure I got that right.\n\n\nAnd then the main question, Hock, is that there is sort of this emerging\ndebate about customer-owned tooling, your ASIC customers potentially\nwanting to do more things on their own. How do you see your XPU content and\nshare at your largest customer evolve over the next 1 or 2 years?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, to answer your first question, what we said is correct that as of\nnow, we have $73 billion of backlog in place secured of XPUs, switches,\nDSPs, lasers for AI data centers that we anticipate shipping over the next\n18 months. And obviously, this is as of now. I mean we fully expect more\nbookings to come in over that period of time. And so don't take that $73\nbillion as that's the revenue that we ship over the next 18 months. We're\njust saying we have that now and that bookings has been accelerating.\n\n\nAnd frankly, we see that bookings not just in XPUs, but in switches, DSPs,\nall the other components that go into AI data centers. We have never seen\nbookings of the nature that what we have seen over the past 3 months,\nparticularly with respect to Tomahawk 6 switches. This is one of the\nfastest-growing products in terms of deployment that we've ever seen of any\nswitch products that we've put out there. It is pretty interesting and\npartly because it's the only one of its kind out there at this point at 102\nterabits per second. And that's the exact product needed to expand the\nclusters of the latest GPU and XPUs out there.\n\n\nSo that's great. But as far as what is the future XPU is your broader\nquestion, my answer to you is don't follow what you hear out there as\ngospel. It's a trajectory. It's a multiyear journey. And many of the\nplayers, and not too many players, doing LLMs want to do their own custom\nAI accelerator for very good reasons. You can put in hardware, if you use a\ngeneral purpose GPU, you can only do in software and kernels and software.\n\n\nYou can achieve, performance-wise, so much better in the custom purpose-\ndesigned, hardware-driven XPU. And we see that in the TPU and we see that\nin all the accelerators we are doing for our other customers. Much, much\nbetter in areas of sparse core, training, inference, reasoning, all that\nstuff.\n\n\nNow, will that mean that over time, they all want to go do it themselves,\nnot necessarily. And in fact, because the technology in silicon keeps\nupdating, keeps evolving. And if you are an LLM player, where do you put\nyour resources in order to compete in this space, especially when you have\nto compete at the end of the day against merchant GPU who are not slowing\ndown in the rate of evolution. So I see that as this concept of customer\ntooling is an overblown hypothesis, which frankly, I don't think will\nhappen.\n\n\nOperator\n\nA moment for our next question, and that will come from the line of Ross\nSeymore with Deutsche Bank.\n\n\nRoss Clark Seymore\nDeutsche Bank AG, Research Division\n\nHock, I want to go to something you touched on earlier about the TPUs going\na little bit more to like a merchant go-to market to other customers. Do\nyou believe that's a substitution effect for customers who otherwise would\nhave done ASICs with you? Or do you think it's actually broadening the\nmarket? And so what are kind of the financial implications of that from\nyour perspective?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nSo that's a very good question, Ross. And what we see right now is the most\nobvious move it does is it goes -- the people who use TPUs, the alternative\nis GPUs, merchant basis, as the most common thing that happens. Because to\ndo that substitution for another custom, it's different. To make an\ninvestment in custom accelerator is a multiyear journey. It's a strategic\ndirectional thing. It's not necessarily a very transactional or short-term\nmove.\n\n\nMoving from GPU to TPU is a transactional move. Going into AI accelerator\nof your own is a long-term strategic move and nothing would deter you from\nthere to continue to make that investment towards that end goal of\nsuccessfully creating and deploying your own custom AI accelerator. So\nthat's the motion we see.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of\nHarlan Sur with JPMorgan.\n\n\nHarlan L.  Sur\nJPMorgan Chase & Co, Research Division\n\nCongratulations on the strong results, guidance and execution. Hock, again,\nI just want to reiterate -- I just want to sort of verify this, right? So\nyou talked about total AI backlog of $73 billion over the next 6 quarters,\nright? This is just a snapshot of your order book like right now. But given\nyour lead times, I think customers can and still will place orders for AI\nin quarters 4, 5 and 6. So as time moves forward, that backlog number for\nmore shipments in the second half of '26 will probably still go up, right?\nIs that the correct interpretation?\n\n\nAnd then given the strong and growing backlog, right, the question is, does\nthe team have 3-nanometer, 2-nanometer wafer supply, colos, substrate, HBM\nsupply commitments to support all of the demand in your order book? And I\nknow one of the areas where you are trying to mitigate this is in advanced\npackaging, right? You're bringing up your Singapore facility. Can you guys\njust remind us what part of the advanced packaging process the team is\nfocusing on with the Singapore facility?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, to answer your first simpler question, Harlan, you're right. You can\nsay that $73 billion is the backlog we have today to ship over the next 6\nquarters. You might also say that given our lead time, we expect more\norders to be able to be absorbed into our backlog for shipments over the\nnext 6 quarters.\n\n\nSo taking that, we expect revenue -- a minimum revenue, one way to look at\nit, of $73 billion over the next 6 quarters, but we do expect much more as\nmore orders come in for shipments within the next 6 quarters. Our lead\ntime, depending on the particular product it is, can be anywhere from 6\nmonths to a year.\n\n\nWith respect to supply chain is what you're asking, critical supply chain\non silicon and packaging, yes, that's an interesting challenge that we have\nbeen addressing constantly and continue to. And with the strength of the\ndemand and the need for more innovative packaging, advanced packaging,\nbecause you are talking about multi-chips in creating every custom\naccelerator now, the packaging becomes a very interesting and technical\nchallenge. And building our Singapore fab is to really talk about partially\nin-sourcing those advanced packaging.\n\n\nWe believe that we have enough demand, we can literally in-source not --\nfrom the viewpoint of not just costs, but in a viewpoint of supply chain\nsecurity and delivery. We're building up a fairly substantial facility for\npackaging -- advanced packaging in Singapore, as indicated, purely for the\npurpose to address the advanced packaging side.\n\n\nSilicon-wise, now we go back to the same precious source in Taiwan, TSMC.\nAnd so we keep going for more and more capacity in 2-nanometers, 3-\nnanometers. And so far, we do not have that constraint. But again, time\nwill tell as we progress and as our backlog builds up.\n\n\nOperator\n\nThe next question will come from the line of Blayne Curtis with Jefferies.\n\n\nBlayne Peter Curtis\nJefferies LLC, Research Division\n\nI wanted to ask, with the original $10 billion deal, you talked about a\nrack sale, I just wanted to -- with the follow-on order as well as the\nfifth customer, can you just maybe describe how you're going to deliver\nthose? Is it an XPU or is it a rack?\n\n\nAnd then maybe you can kind of just walk us through the math and kind of\nwhat the deliverable is? Obviously, Google uses its own networking. So I'm\nkind of curious, too, would it be a copy exact of what Google does, now\nthat you could talk [ to it by ] name? Or would you have your own\nnetworking in there as well?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nThat's a very complicated question, Blayne. Let me tell you what it is,\nit's a system sale. How about that? It's a real system sale. We have so\nmany components beyond XPUs, custom accelerators in any system -- in AI\nsystem, any AI system used by hyperscalers that, yes, we believe it begins\nto make sense to do it as a system sale and be responsible, but be fully\nresponsible for the entire system or rack, as you call it. I think people\nare understanding as a system sale better. And so on this customer number\n4, we are selling it as a system with our key components in it. And that's\nno different than selling a chip. We certify a final ability to run as part\nof the whole selling process.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of Stacy\nRasgon with Bernstein.\n\n\nStacy Aaron Rasgon\nSanford C. Bernstein & Co., LLC., Research Division\n\nI wanted to touch on gross margins and maybe it feeds into a little bit the\nprior question. So I understand why the AI business is somewhat dilutive to\ngross margins. We have the HBM pass-through. And then presumably with the\nsystem sales, that will be more dilutive. And you hinted at this in the\npast, but I was wondering if it could be a little more explicit. As this AI\nrevenue starts to ramp, as we start to get system sales, how should we be\nthinking about that gross margin number? Say, if we're looking out 4\nquarters or 6 quarters, is it low 70s? I mean could it start with the 6 at\nthe corporate level?\n\n\nAnd, I guess, I'm also wondering -- I understand how that comes down, but\nwhat about the operating margins? Do you think you get enough operating\nleverage on the OpEx side to keep operating margins flat? Or do they need\nto come down as well?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nI'll let Kirsten give you the details, but enough for me to broadly, high\nlevel, explain to you, Stacy. Good question. Phenomenal. You don't see that\nimpacting us right now, and we have already started that process of some\nsystems sales. You don't see that in our numbers, but [ it will ]. And we\nhave said that openly. The AI revenue has a lower gross margin than our --\nobviously, the rest of our business including software, of course.\n\n\nBut we expect the rate of growth of -- as we do more and more AI revenue to\nbe so much that we get the operating leverage on our operating spending\nthat operating margin will deliver dollars that are still high level of\ngrowth from what it has been. So we expect operating leverage to benefit us\nat the operating margin level, even as gross margin will start to\ndeteriorate, high level.\n\n\nKirsten M. Spears\nCFO & Chief Accounting Officer\n\nNow, I think Hock said that fairly. And the second half of the year when we\ndo start shipping more systems, the situation is straightforward. We'll be\npassing through more components that are not ours. So think of it similar\nto the XPUs where we have memory on those XPUs and we're passing through\nthose costs. We'll be passing through more cost within the rack. And so\nthose gross margins will be lower. However, overall, the way Hock said it,\ngross margin dollars will go up, margins will go down, operating margins,\nbecause we have leverage, operating margin dollars will go up, but the\nmargin itself as a percentage of revenues will come down a bit. But we're\nnot -- I mean we'll guide closer to the end of the year for that.\n\n\nOperator\n\nOne moment for our next question, that will come from the line of Jim\nSchneider with Goldman Sachs.\n\n\nJames Edward Schneider\nGoldman Sachs Group, Inc., Research Division\n\nHock, I was wondering if you might care to calibrate your expectations for\nAI revenue in fiscal '26 a little bit more closely. I believe you talked\nabout acceleration in fiscal '26 off of the 65% growth rate you did in\nfiscal '25. And then you're guiding to 100% growth for Q1. So I'm just\nwondering if the Q1 is a good jumping off point for the growth rate you\nexpect for the full year or something maybe a little bit less than that.\n\n\nAnd then maybe if you could separately clarify whether your $1 billion of\norders for the fifth customer is indeed OpenAI, which you made a separate\nannouncement about.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWow, there's a lot of questions here. But let me start off with '26. Our\nbacklog is very dynamic these days, as I said, and it is continuing to ramp\nup. And you're right, we originally, 6 months ago, said, maybe year-on-year\nAI revenues would grow in '26, 60%, 70%. Q1, we doubled. And Q1 '26 today,\nwe're saying it's doubled. And we're looking at it because all the fresh\norders keeps coming in, and we give you a milestone of where we are today,\nwhich is $73 billion of backlog to be shipped over the next 18 months. And\nwe do fully expect, as I answered the earlier question, for that $73\nbillion over the 18 months to keep growing.\n\n\nNow it's a moving number as we move in time, but it will grow. And it's\nhard for me to pinpoint what '26 is going to look like precisely. So I'd\nrather not give you guys any guidance, and that's why we don't give you\nguidance, but we do give it for Q1. Give it time, we'll give it for Q2. And\nyou're right to ask is it an accelerating trend? And my answer is it's\nlikely to be an accelerating trend as we progress through '26. I hope that\nanswers your question.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of Ben\nReitzes with Melius Research.\n\n\nBenjamin Alexander Reitzes\nMelius Research LLC\n\nHock, I wanted to ask, I'm not sure if the last caller said something on\nit, but I didn't hear it in the answer was -- I wanted to ask about the\nOpenAI contract. It's supposed to start in the second half of the year and\ngo through 2029 for 10 gigawatts. I'm going to assume that, that's the\nfifth customer order there. And I was just wondering if you're still\nconfident in that being a driver. Are there any obstacles to making that a\nmajor driver? And when you expect that to contribute and your confidence in\nit?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nYou didn't hear that answer from my last caller, Jim's question, because I\ndidn't answer it. I did not answer it and I'm not answering it either. It's\nthe fifth customer, and it's a real customer and it will grow. They are on\ntheir multiyear journey to their own XPUs. And let's leave it at that.\n\n\nAs far as the OpenAI view that you have, we appreciate the fact that it is\na multiyear journey that will run through '29 as our press release with\nOpenAI showed, 10 gigawatts between '26 -- more like '27, '28, '29, Ben,\nnot '26. It's more like '27, '28, '29, 10 gigawatts. That was the OpenAI\ndiscussion. And that's, I call it, an agreement and alignment of where\nwe're headed with respect to various respected and valued customer, OpenAI.\nBut we do not expect much in '26.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of C.J.\nMuse with Cantor Fitzgerald.\n\n\nChristopher James Muse\nCantor Fitzgerald & Co., Research Division\n\nI guess, Hock, I wanted to talk about custom silicon and maybe speak to how\nyou expect content to grow for Broadcom generation to generation. And as\npart of that, your competitor announced CPX offering, essentially\naccelerator for -- an accelerator for massive context windows. I'm curious\nif you see a broadening opportunity for your existing 5 customers to have\nmultiple XPU offerings.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nThank you. No, yes, you hit it right on. I mean the nice thing about a\ncustom accelerator is you try not to do one-size-fits-all and\ngenerationally. Each of these 5 customers now can create their version of\nan XPU custom accelerator for training and inference. And basically, it's\nalmost 2 parallel threads going on almost simultaneously for each of them.\nSo I don't have plenty of versions to deal with. I don't need to create any\nmore version. We've got plenty of different content out there just on the\nbasis of creating these custom accelerators.\n\n\nAnd by the way, when you do custom accelerators, you tend to put more\nhardware in that unique differentiated versus trying to make it work on\nsoftware and creating kernels into software. I know that's very tricky too.\nBut thinking about the difference where you can create in hardware, those\nsparse core data routers versus the dense matrix multipliers, all in one\nsame chip.\n\n\nAnd that's many -- just one example of what creating custom accelerators is\nletting us do, or for that matter, a variation in how much memory capacity\nor memory bandwidth from -- for the same customer from chip to chip, just\nbecause even in inference, you want to do more reasoning versus decoding\nversus something else like prefill.\n\n\nSo you literally start to create different hardware for different aspects\nof how you want to train or inference and run your workloads. It's a very\nfascinating area, and we are seeing a lot of variations and multiple chips\nfor each of our customers.\n\n\nOperator\n\nOne moment for our next question, and that will come from the line of Harsh\nKumar with Piper Sandler.\n\n\nHarsh V. Kumar\nPiper Sandler & Co., Research Division\n\nYes, Hock and team, first of all, congratulations on some pretty stunning\nnumbers. I've got an easy one and a more strategic one. The easy one is,\nyour guide in AI, Hock and Kirsten, is calling for almost $1.7 billion of\nsequential growth. I was curious maybe you can talk about the diversity of\nthe growth between the 3 existing customers. Is it pretty well spread out?\nAll of them growing? Or is one sort of driving much of the growth?\n\n\nAnd then, Hock, strategically, one of your competitors bought a photonic\nfabric company recently. I was curious about your take on that technology\nand if you think it's disruptive or you think it's just gimmickry at this\npoint in time.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nI like the way you address this question because the way that you address\nthe tech question to me. It's almost hesitant. Thank you. I appreciate\nthat.\n\n\nBut on your first part, yes, we are driving growth and it began to feel\nlike this thing never ends, and it's a real mixed bag of existing customers\nand on existing XPUs. And a big part of it is XPUs that we're seeing. And\nthat's not to slow down the fact that, as I indicated in my remarks and\ncommented on the demand for switches, not just Tomahawk 6, Tomahawk 5\nswitches, the demand for our latest 1.6  terabit per second DSPs that\nenables optical interconnects for scale out particularly. It's just very,\nvery strong. And by extension, demand for the optical components like\nlasers, PIN diodes just going nuts. All that come together.\n\n\nNow all that is small, relatively lesser dollars when it comes to XPUs, as\nyou probably guess. I mean, to give you a sense, maybe let me look at it on\na backlog side. Of the $73 billion of AI revenue backlog over the next 18\nmonths, I talked about, maybe $20 billion of it is everything else. The\nrest is XPUs. Hope that gives you a sense of what the mix is. But\n[indiscernible] the rest is still $20 billion, that's not small by any\nmeans. So we value that.\n\n\nSo when you talk about your next question of silicon photonics as a means\nto create basically much better, more efficient, lower power interconnects\nin not just scale-out, but hopefully, scale-up, yes, I could see a point in\ntime in the future when silicon photonics method is the only way to do it.\nWe're not quite there yet. But we have the technology and we continue to\ndevelop the technology, even at each time we develop it first for 400\ngigabit bandwidth, then going on to 800 gigabit bandwidth, not ready for it\nyet.\n\n\nAnd even with the product -- and we're now doing it for 1.6 terabit\nbandwidth to create silicon photonics switches, silicon photonics\ninterconnects, not even sure it will get fully deployed because engineers --\n our engineers, our peers -- and the peers we have out there will somehow\ntry to find a way to still do -- try to do scale-up within a rack in copper\nas long as possible and in scale-out in non-pluggable optics.\n\n\nThe final, final straw is when you can do it well in pluggable optics. And\nof course, when you can do it even in copper, then you're right, you go to\nsilicon photonics and it will happen, and we're ready for it, just saying\nnot anytime soon.\n\n\nOperator\n\nOne moment for our next question, that will come from the line of Karl\nAckerman with BNP Paribas.\n\n\nKarl  Ackerman\nBNP Paribas, Research Division\n\nHock, could you speak to the supply chain resiliency and visibility you\nhave with your key material suppliers, particularly CoWoS as you not only\nsupport your existing customer programs but the two new custom compute\nprocessors that you announced intra-quarter. I guess, what I can get at is\nyou also happen to address the very large subset of networking and compute\nAI supply chains. You talked about record backlog. If you were to pinpoint\nsome of the bottlenecks that you have, the areas that you're aiming to\naddress and mitigate from supply chain bottlenecks, what would they be? And\nhow do you see that ameliorating into '26?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nIt's across the board, typically. I mean it's -- we are very fortunate in\nsome ways that we have the product, technology and the operating business\nlines to create multiple key leading-edge components that enables today's\nstate-of-the-art AI data centers. I mean our DSP, as I said earlier, is now\nat 1.6 terabit per second. That's the leading edge connectivity for\nbandwidth for this -- for the top of the [indiscernible] XPU and even GPU.\nAnd we intend to be that way. And we have the lasers, EMLs, VCSELs, CW\nlasers that goes with it.\n\n\nSo it's fortunate that we have all this and the key active components that\ngo with it. And we see it very early, and we expand the capacity as we do\nthe design to match it. And long -- this is a long answer to what I'm\ntrying to get at, which is, I think we are -- of any of these data center\nsuppliers of the system racks, not counting the power shell and all that.\nNow that starts to get beyond us on the power shell and the transformers\nand the gas turbines.\n\n\nIf you just look at the rack, the systems on AI, we probably have a good\nhandle on where the bottlenecks are because sometimes we are part of the\nbottlenecks, which we then want to get resolved. So we feel pretty good\nabout that through 2026.\n\n\nOperator\n\nOne moment for our next question, that will come from the line of\nChristopher Rolland with Susquehanna.\n\n\nChristopher Adam Jackson Rolland\nSusquehanna Financial Group, LLLP, Research Division\n\nJust first, a clarification and then my question. And sorry to come back to\nthis issue. But if I understand you correctly, Hock, I think you were\nsaying that OpenAI would be a general agreement, so it's not binding, maybe\nsimilar to the agreements with both NVIDIA and AMD.\n\n\nAnd then secondly, you talked about flat non-AI semiconductor revenue,\nmaybe what's going on there? Is there still an inventory overhang? And what\ncould -- what do we need to get that going again? Do you see growth\neventually in that business?\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, on the non-AI semiconductor, we see broadband literally recovering\nvery well. And we don't see the others -- no, we see stability. We don't\nsee a sharp recovery that is sustainable yet. So I guess, given a couple\nmore quarters, but we don't see any further deterioration in demand. And\nit's more, I think, maybe AI is sucking the oxygen a lot out of enterprise\nspending elsewhere and hyperscaler spending elsewhere. We don't see getting\nany worse. We don't see it recovering very quickly with the exception of\nbroadband. That's a simple summary of non-AI.\n\n\nWith respect to OpenAI, without diving in too deep, I'm just telling you\nwhat that 10 gigawatt announcement is all about. Separately, the journey\nwith them on the custom accelerator progresses at a very advanced stage and\nwill happen very, very quickly. And it's -- and we will have a committed\nelement to this whole thing, and [ it will ]. But what I was articulating\nearlier was the 10 gigawatt announcement. And that 10 gigawatt announcement\nis an agreement to be aligned on developing 10 gigawatts for OpenAI over\n'27 to '29 time frame. That's different from the XPU program we're\ndeveloping with them.\n\n\nOperator\n\nAnd we do have time for one final question, and that will come from the\nline of Joe Moore with Morgan Stanley.\n\n\nJoseph Lawrence Moore\nMorgan Stanley, Research Division\n\nGreat. So if you have $21 billion of rack revenue in the second half of\n'26, I guess, do we stay at that run rate beyond that? Are you going to\ncontinue to sell racks? Or does that sort of -- that type of business mix\nshift over time? And I'm really just trying to figure out the percentage of\nyour 18-month backlog that's actually full systems at this point.\n\n\nHock E. Tan\nPresident, CEO & Executive Director\n\nWell, it's an interesting question. And that question basically comes to\nhow much compute capacity is needed by our customers over the next, as I\nsay, over the period beyond 18 months. And your guess is probably as good\nas mine based on what we all know out there, which is really what they\nrelate to. But if they need more, then you see that continuing even larger.\nIf they don't need it, then probably it won't. But as of -- what we are\ntrying to indicate is that's the demand we are seeing over that period of\ntime right now.\n\n\nOperator\n\nI would now like to turn the call back over to Ji Yoo for any closing\nremarks.\n\n\nJi  Yoo\nDirector of Investor Relations\n\nThank you, operator. This quarter, Broadcom will be presenting at the New\nStreet Research Virtual AI Big Idea Conference on Monday, December 15,\n2025. Broadcom currently plans to report its earnings for the first quarter\nof fiscal year 2026 after close of market on Wednesday, March 4, 2026. A\npublic webcast of Broadcom's earnings conference call will follow at 2:00\np.m. Pacific.\n\n\nThat will conclude our earnings call today. Thank you all for joining.\nOperator, you may end the call.\n\n\nOperatorThis concludes today's program. Thank you all for participating.\nYou may now disconnect.\nCopyright Â© 2025 by S&P Global Market Intelligence, a division of S&P\nGlobal Inc. All rights reserved.\n\n\nThese materials have been prepared solely for information purposes based\nupon information generally available to the public and from sources\nbelieved to be reliable. No content (including index data, ratings, credit-\nrelated analyses and data, research, model, software or other application\nor output therefrom) or any part thereof (Content) may be modified, reverse\nengineered, reproduced or distributed in any form by any means, or stored\nin a database or retrieval system, without the prior written permission of\nS&P Global Market Intelligence or its affiliates (collectively, S&P\nGlobal). The Content shall not be used for any unlawful or unauthorized\npurposes. S&P Global and any third-party providers, (collectively S&P\nGlobal Parties) do not guarantee the accuracy, completeness, timeliness or\navailability of the Content. S&P Global Parties are not responsible for any\nerrors or omissions, regardless of the cause, for the results obtained from\nthe use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P\nGLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS\nFOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR\nDEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE\nCONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no\nevent shall S&P Global Parties be liable to any party for any direct,\nindirect, incidental, exemplary, compensatory, punitive, special or\nconsequential damages, costs, expenses, legal fees, or losses (including,\nwithout limitation, lost income or lost profits and opportunity costs or\nlosses caused by negligence) in connection with any use of the Content even\nif advised of the possibility of such damages. S&P Global Market\nIntelligence's opinions, quotes and credit-related and other analyses are\nstatements of opinion as of the date they are expressed and not statements\nof fact or recommendations to purchase, hold, or sell any securities or to\nmake any investment decisions, and do not address the suitability of any\nsecurity. S&P Global Market Intelligence may provide index data. Direct\ninvestment in an index is not possible. Exposure to an asset class\nrepresented by an index is available through investable instruments based\non that index. S&P Global Market Intelligence assumes no obligation to\nupdate the Content following publication in any form or format. The Content\nshould not be relied on and is not a substitute for the skill, judgment and\nexperience of the user, its management, employees, advisors and/or clients\nwhen making investment and other business decisions. S&P Global Market\nIntelligence does not act as a fiduciary or an investment advisor except\nwhere registered as such. S&P Global keeps certain activities of its\ndivisions separate from each other in order to preserve the independence\nand objectivity of their respective activities. As a result, certain\ndivisions of S&P Global may have information that is not available to other\nS&P Global divisions. S&P Global has established policies and procedures to\nmaintain the confidentiality of certain nonpublic information received in\nconnection with each analytical process.\n\n\nS&P Global may receive compensation for its ratings and certain analyses,\nnormally from issuers or underwriters of securities or from obligors. S&P\nGlobal reserves the right to disseminate its opinions and analyses. S&P\nGlobal's public ratings and analyses are made available on its Web sites,\nwww.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and\nwww.globalcreditportal.com (subscription), and may be distributed through\nother means, including via S&P Global publications and third-party\nredistributors. Additional information about our ratings fees is available\nat www.standardandpoors.com/usratingsfees.\nÂ© 2025 S&P Global Market Intelligence.",
  "has_qa": 1,
  "speaker_turns": [
    {
      "speaker": "Unknown",
      "role": "",
      "text": "Broadcom Inc. NasdaqGS:AVGO FQ4 2025 Earnings Call Transcripts Thursday, December 11, 2025 10:00 PM GMT S&P Global Market Intelligence Estimates Presentation"
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Welcome to Broadcom Inc. Fourth Quarter and Fiscal Year 2025 Financial Results Conference Call. At this time, for opening remarks and introductions, I would like to turn the call over to Ji Yoo, Head of Investor Relations of Broadcom Inc."
    },
    {
      "speaker": "Ji  Yoo",
      "role": "Director of Investor Relations",
      "text": "Director of Investor Relations Thank you, Sheri, and good afternoon, everyone. Joining me on today's call are Hock Tan, President and CEO; Kirsten Spears, Chief Financial Officer; and Charlie Kawwas, President, Semiconductor Solutions Group. Broadcom distributed a press release and financial tables after the market close, describing our financial performance for the fourth quarter and fiscal year 2025. If you did not receive a copy, you may obtain the information from the Investors section of Broadcom's website at broadcom.com. This conference call is being webcast live, and an audio replay of the call can be accessed for 1 year through the Investors section of Broadcom's website. During the prepared remarks, Hock and Kirsten will be providing details of our fourth quarter and fiscal year 2025 results, guidance for our first quarter of fiscal year 2026 as well as commentary regarding the business environment. We'll take questions after the end of our prepared comments. Please refer to our press release today and our recent filings with the SEC for information on the specific risk factors that could cause our actual results to differ materially from the forward-looking statements made on this call. In addition to U.S. GAAP reporting, Broadcom reports certain financial measures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP measures is included in the tables attached to today's press release. Comments made during today's call will primarily refer to our non-GAAP financial results. I'll now turn the call over to Hock."
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director Thank you, Ji, and thank you, everyone, for joining us today. Well, we just ended our Q4 fiscal '25. And before I get into details of that quarter, let me recap the year. In our fiscal 2025, consolidated revenue grew 24% year-over-year to a record $64 billion, and it's driven by AI semiconductors and VMware. AI revenue grew 65% year-over-year to $20 billion, driving the semiconductor revenue for this company to a record $37 billion for the year. In our infrastructure software business, strong adoption of VMware Cloud Foundation or VCF, as we call it, drove revenue growth of 26% year-on-year to $27 billion. In summary, 2025 was another strong year for Broadcom. And we see the spending momentum by our customers for -- in AI, continuing to accelerate in 2026. Now let's move on to the results of our fourth quarter 2025. Total revenue was a record $18 billion, up 28% year-on-year, and above our guidance on better-than-expected growth in AI semiconductors as well as infrastructure software. Q4 consolidated adjusted EBITDA was a record $12.12 billion (sic) [ $12.218 billion ], up 34% year-on-year. So let me give you more color on our two segments. In semiconductors, revenue was $11.1 billion as year-on-year growth accelerated to 35%. And this robust growth was driven by the AI semiconductor revenue of $6.5 billion, which was up 74% year-on-year. And this represents a growth trajectory exceeding 10x over the 11 quarters we have reported this line of business. Our custom accelerated business more than doubled year-over-year, as we see our customers increase adoption of XPUs, as we call those custom accelerators, in training their LLMs and monetizing their platforms through inferencing APIs and applications. These XPUs, I may add, are not only being used to train and inference internal workloads by our customers, the same XPUs, in some situations, have been extended externally to other LLM peers, best exemplified at Google, where the TPUs used in creating Gemini have also been used for AI cloud computing by Apple, Cohere and SSI as an example. And the scale at which we see this happening could be significant. And as you are aware, last quarter, Q3 '25, we received a $10 billion order to sell the latest TPU Ironwood racks to Anthropic. And this was our fourth customer that we mentioned. And in this quarter Q4, we received an additional $11 billion order from the same customer for delivery in late 2026. But that does not mean our other two customers are using TPUs. In fact, they prefer to control their own destiny by continuing to drive their multiyear journey to create their own custom AI accelerators or XPU racks, as we call them. And I'm pleased today to report that during this quarter, we acquired a fifth XPU customer through a $1 billion order placed for delivery in late 2026. Now moving on to AI networking. Demand here has even been stronger as we see customers build out their data center infrastructure ahead of deploying AI accelerators. Our current order backlog for AI switches exceeds $10 billion as our latest 102-terabit per second Tomahawk 6 switch, the first and only one of its capability out there, continues to book at record rates. And this is just a subset of what we have. We have also secured record orders on DSPs, optical components like lasers and PCI Express switches to be deployed -- all to be deployed in AI data centers. And all these components, combined with our XPUs, bring our total order on hand in excess of $73 billion today, which is almost half Broadcom's consolidated backlog of $162 billion. We expect this $73 billion in AI backlog to be delivered over the next 18 months. And in Q1 fiscal '26, we expect our AI revenue to double year-on-year to $8.2 billion. Turning to non-AI semiconductors. Q4 revenue of $4.6 billion was up 2% year- on-year and up 16% sequentially based on favorable wireless seasonality. Year-on-year, broadband showed solid recovery, wireless was flat and all the other end markets were down as enterprise spending continued to show limited signs of recovery. And accordingly, in Q1, we forecast non-AI- semiconductor revenue to be approximately $4.1 billion, flat from a year ago, down sequentially due to wireless seasonality. Let me now talk about our infrastructure software segment. Q4 infrastructure software revenue of $6.9 billion was up 19% year-on-year and above our outlook of $6.7 billion. Bookings continue to be strong as total contract value booked in Q4 exceeded $10.4 billion versus $8.2 billion a year ago. We ended the year with $73 billion of infrastructure software backlog, up from $49 billion a year ago. We expect renewals to be seasonal in Q1 and forecast infrastructure software revenue to be approximately $6.8 billion. We still expect, however, that for fiscal '26, infrastructure software revenue to grow low double-digit percentage. So here's what we see in 2026. Directionally, we expect AI revenue to continue to accelerate and drive most of our growth, and non-AI semiconductor revenue to be stable. Infrastructure software revenue will continue to be driven by VMware growth at low double digits. And for Q1 '26, we expect consolidated revenue of approximately $19.1 billion, up 28% year-on-year. And we expect adjusted EBITDA to be approximately 67% of revenue. And with that, let me turn the call over to Kirsten."
    },
    {
      "speaker": "Kirsten M. Spears",
      "role": "CFO & Chief Accounting Officer",
      "text": "CFO & Chief Accounting Officer Thank you, Hock. Let me now provide additional detail on our Q4 financial performance. Consolidated revenue was a record $18 billion for the quarter, up 28% from a year ago. Gross margin was 77.9% of revenue in the quarter, better than we originally guided on higher software revenues and product mix within semiconductors. Consolidated operating expenses were $2.1 billion, of which $1.5 billion was research and development. Q4 operating income was a record $11.9 billion, up 35% from a year ago. Now on a sequential basis, even as gross margin was down 50 basis points on semiconductor product mix, operating margin increased 70 basis points sequentially to 66.2% on favorable operating leverage. Adjusted EBITDA of $12.2 billion, or 68% of revenue, was above our guidance of 67%. This figure excludes $148 million of depreciation. Now a review of the P&L for our 2 segments, starting with semiconductors. Revenue for our semiconductor solutions segment was a record $11.1 billion with growth accelerating to 35% year-on-year, driven by AI. Semiconductor revenue represented 61% of total revenue in the quarter. Gross margin for our semiconductor solutions segment was approximately 68%. Operating expenses increased 16% year-on-year to $1.1 billion on increased investment in R&D for leading-edge AI semiconductors. Semiconductor operating margin of 59% was up 250 basis points year-on-year. Now moving to infrastructure software. Revenue for infrastructure software of $6.9 billion was up 19% year-on-year and represented 39% of total revenue. Gross margin for infrastructure software was 93% in the quarter compared to 91% a year ago. Operating expenses were $1.1 billion in the quarter, resulting in infrastructure software operating margin of 78%. This compares to operating margin of 72% a year ago, reflecting the completion of the integration of VMware. Moving on to cash flow. Free cash flow in the quarter was $7.5 billion and represented 41% of revenue. We spent $237 million on capital expenditures. Days sales outstanding were 36 days in the fourth quarter, compared to 29 days a year ago. We ended the fourth quarter with inventory of $2.3 billion, up 4% sequentially. Our days of inventory on hand were 58 days in Q4, compared to 66 days in Q3, as we continue to remain disciplined on how we manage inventory across the ecosystem. We ended the fourth quarter with $16.2 billion of cash, up $5.5 billion sequentially on strong cash flow generation. The weighted average coupon rate in years to maturity of our gross principal fixed rate debt of $67.1 billion is 4% and 7.2 years, respectively. Turning to capital allocation. In Q4, we paid stockholders $2.8 billion of cash dividends based on a quarterly common stock cash dividend to $0.59 per share. In Q1, we expect the non-GAAP diluted share count to be approximately 4.97 billion shares, excluding the potential impact of any share repurchases. Now let me recap our financial performance for fiscal year 2025. Our revenue hit a record $63.9 billion with organic growth accelerating to 24% year-on-year. Semiconductor revenue was $36.9 billion, up 22% year-over- year. Infrastructure software revenue was $27 billion, up 26% year-on-year. Fiscal 2025 adjusted EBITDA was $43 billion and represented 67% of revenue. Free cash flow grew 39% year-on-year to $26.9 billion. For fiscal 2025, we returned $17.5 billion of cash to shareholders in the form of $11.1 billion of dividends and $6.4 billion in share repurchases and elimination. Aligned with our ability to generate increased cash flows in the preceding year, we are announcing an increase in our quarterly common stock cash dividend in Q1 fiscal 2026 to $0.65 per share, an increase of 10% from the prior quarter. We intend to maintain this target quarterly dividend throughout fiscal '26, subject to quarterly Board approval. This implies our fiscal 2026 annual common stock dividend to be a record $2.60 per share, an increase of 10% year-on-year. I would like to highlight that this represents the 15th consecutive increase in annual dividends since we initiated dividends in fiscal 2011. The Board also approved an extension of our share repurchase program, of which $7.5 billion remains, through the end of calendar year 2026. Now moving to guidance. Our guidance for Q1 is for consolidated revenue of $19.1 billion, up 28% year-on-year. We forecast semiconductor revenue of approximately $12.3 billion, up 50% year-on-year. Within this, we expect Q1 AI semiconductor revenue of $8.2 billion, up approximately 100% year-on- year. We expect infrastructure software revenue of approximately $6.8 billion, up 2% year-on-year. For your modeling purposes, we expect Q1 consolidated gross margin to be down approximately 100 basis points sequentially, primarily reflecting a higher mix of AI revenue. As a reminder, consolidated gross margins through the year will be impacted by the revenue mix of infrastructure software and semiconductors and also product mix within semiconductors. We expect Q1 adjusted EBITDA to be approximately 67%. We expect the non- GAAP tax rate for Q1 and fiscal year 2026 to increase from 14% to approximately 16.5% due to the impact of the global minimum tax and shift in geographic mix of income compared to that of fiscal year 2025. That concludes my prepared remarks. Operator, please open up the call for questions. Question and Answer"
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "[Operator Instructions] Our first question will come from the line of Vivek Arya with Bank of America."
    },
    {
      "speaker": "Vivek  Arya",
      "role": "BofA Securities, Research Division",
      "text": "BofA Securities, Research Division Just wanted to clarify, Hock, you said $73 billion over 18 months for AI, that's roughly $50-ish billion plus for fiscal '26 for AI. I just wanted to get -- make sure I got that right. And then the main question, Hock, is that there is sort of this emerging debate about customer-owned tooling, your ASIC customers potentially wanting to do more things on their own. How do you see your XPU content and share at your largest customer evolve over the next 1 or 2 years?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director Well, to answer your first question, what we said is correct that as of now, we have $73 billion of backlog in place secured of XPUs, switches, DSPs, lasers for AI data centers that we anticipate shipping over the next 18 months. And obviously, this is as of now. I mean we fully expect more bookings to come in over that period of time. And so don't take that $73 billion as that's the revenue that we ship over the next 18 months. We're just saying we have that now and that bookings has been accelerating. And frankly, we see that bookings not just in XPUs, but in switches, DSPs, all the other components that go into AI data centers. We have never seen bookings of the nature that what we have seen over the past 3 months, particularly with respect to Tomahawk 6 switches. This is one of the fastest-growing products in terms of deployment that we've ever seen of any switch products that we've put out there. It is pretty interesting and partly because it's the only one of its kind out there at this point at 102 terabits per second. And that's the exact product needed to expand the clusters of the latest GPU and XPUs out there. So that's great. But as far as what is the future XPU is your broader question, my answer to you is don't follow what you hear out there as gospel. It's a trajectory. It's a multiyear journey. And many of the players, and not too many players, doing LLMs want to do their own custom AI accelerator for very good reasons. You can put in hardware, if you use a general purpose GPU, you can only do in software and kernels and software. You can achieve, performance-wise, so much better in the custom purpose- designed, hardware-driven XPU. And we see that in the TPU and we see that in all the accelerators we are doing for our other customers. Much, much better in areas of sparse core, training, inference, reasoning, all that stuff. Now, will that mean that over time, they all want to go do it themselves, not necessarily. And in fact, because the technology in silicon keeps updating, keeps evolving. And if you are an LLM player, where do you put your resources in order to compete in this space, especially when you have to compete at the end of the day against merchant GPU who are not slowing down in the rate of evolution. So I see that as this concept of customer tooling is an overblown hypothesis, which frankly, I don't think will happen."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "A moment for our next question, and that will come from the line of Ross Seymore with Deutsche Bank."
    },
    {
      "speaker": "Ross Clark Seymore",
      "role": "Deutsche Bank AG, Research Division",
      "text": "Deutsche Bank AG, Research Division Hock, I want to go to something you touched on earlier about the TPUs going a little bit more to like a merchant go-to market to other customers. Do you believe that's a substitution effect for customers who otherwise would have done ASICs with you? Or do you think it's actually broadening the market? And so what are kind of the financial implications of that from your perspective?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director So that's a very good question, Ross. And what we see right now is the most obvious move it does is it goes -- the people who use TPUs, the alternative is GPUs, merchant basis, as the most common thing that happens. Because to do that substitution for another custom, it's different. To make an investment in custom accelerator is a multiyear journey. It's a strategic directional thing. It's not necessarily a very transactional or short-term move. Moving from GPU to TPU is a transactional move. Going into AI accelerator of your own is a long-term strategic move and nothing would deter you from there to continue to make that investment towards that end goal of successfully creating and deploying your own custom AI accelerator. So that's the motion we see."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, and that will come from the line of Harlan Sur with JPMorgan."
    },
    {
      "speaker": "Harlan L.  Sur",
      "role": "JPMorgan Chase & Co, Research Division",
      "text": "JPMorgan Chase & Co, Research Division Congratulations on the strong results, guidance and execution. Hock, again, I just want to reiterate -- I just want to sort of verify this, right? So you talked about total AI backlog of $73 billion over the next 6 quarters, right? This is just a snapshot of your order book like right now. But given your lead times, I think customers can and still will place orders for AI in quarters 4, 5 and 6. So as time moves forward, that backlog number for more shipments in the second half of '26 will probably still go up, right? Is that the correct interpretation? And then given the strong and growing backlog, right, the question is, does the team have 3-nanometer, 2-nanometer wafer supply, colos, substrate, HBM supply commitments to support all of the demand in your order book? And I know one of the areas where you are trying to mitigate this is in advanced packaging, right? You're bringing up your Singapore facility. Can you guys just remind us what part of the advanced packaging process the team is focusing on with the Singapore facility?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director Well, to answer your first simpler question, Harlan, you're right. You can say that $73 billion is the backlog we have today to ship over the next 6 quarters. You might also say that given our lead time, we expect more orders to be able to be absorbed into our backlog for shipments over the next 6 quarters. So taking that, we expect revenue -- a minimum revenue, one way to look at it, of $73 billion over the next 6 quarters, but we do expect much more as more orders come in for shipments within the next 6 quarters. Our lead time, depending on the particular product it is, can be anywhere from 6 months to a year. With respect to supply chain is what you're asking, critical supply chain on silicon and packaging, yes, that's an interesting challenge that we have been addressing constantly and continue to. And with the strength of the demand and the need for more innovative packaging, advanced packaging, because you are talking about multi-chips in creating every custom accelerator now, the packaging becomes a very interesting and technical challenge. And building our Singapore fab is to really talk about partially in-sourcing those advanced packaging. We believe that we have enough demand, we can literally in-source not -- from the viewpoint of not just costs, but in a viewpoint of supply chain security and delivery. We're building up a fairly substantial facility for packaging -- advanced packaging in Singapore, as indicated, purely for the purpose to address the advanced packaging side. Silicon-wise, now we go back to the same precious source in Taiwan, TSMC. And so we keep going for more and more capacity in 2-nanometers, 3- nanometers. And so far, we do not have that constraint. But again, time will tell as we progress and as our backlog builds up."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "The next question will come from the line of Blayne Curtis with Jefferies."
    },
    {
      "speaker": "Blayne Peter Curtis",
      "role": "Jefferies LLC, Research Division",
      "text": "Jefferies LLC, Research Division I wanted to ask, with the original $10 billion deal, you talked about a rack sale, I just wanted to -- with the follow-on order as well as the fifth customer, can you just maybe describe how you're going to deliver those? Is it an XPU or is it a rack? And then maybe you can kind of just walk us through the math and kind of what the deliverable is? Obviously, Google uses its own networking. So I'm kind of curious, too, would it be a copy exact of what Google does, now that you could talk [ to it by ] name? Or would you have your own networking in there as well?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director That's a very complicated question, Blayne. Let me tell you what it is, it's a system sale. How about that? It's a real system sale. We have so many components beyond XPUs, custom accelerators in any system -- in AI system, any AI system used by hyperscalers that, yes, we believe it begins to make sense to do it as a system sale and be responsible, but be fully responsible for the entire system or rack, as you call it. I think people are understanding as a system sale better. And so on this customer number 4, we are selling it as a system with our key components in it. And that's no different than selling a chip. We certify a final ability to run as part of the whole selling process."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, and that will come from the line of Stacy Rasgon with Bernstein."
    },
    {
      "speaker": "Stacy Aaron Rasgon",
      "role": "Sanford C. Bernstein & Co., LLC., Research Division",
      "text": "Sanford C. Bernstein & Co., LLC., Research Division I wanted to touch on gross margins and maybe it feeds into a little bit the prior question. So I understand why the AI business is somewhat dilutive to gross margins. We have the HBM pass-through. And then presumably with the system sales, that will be more dilutive. And you hinted at this in the past, but I was wondering if it could be a little more explicit. As this AI revenue starts to ramp, as we start to get system sales, how should we be thinking about that gross margin number? Say, if we're looking out 4 quarters or 6 quarters, is it low 70s? I mean could it start with the 6 at the corporate level? And, I guess, I'm also wondering -- I understand how that comes down, but what about the operating margins? Do you think you get enough operating leverage on the OpEx side to keep operating margins flat? Or do they need to come down as well?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director I'll let Kirsten give you the details, but enough for me to broadly, high level, explain to you, Stacy. Good question. Phenomenal. You don't see that impacting us right now, and we have already started that process of some systems sales. You don't see that in our numbers, but [ it will ]. And we have said that openly. The AI revenue has a lower gross margin than our -- obviously, the rest of our business including software, of course. But we expect the rate of growth of -- as we do more and more AI revenue to be so much that we get the operating leverage on our operating spending that operating margin will deliver dollars that are still high level of growth from what it has been. So we expect operating leverage to benefit us at the operating margin level, even as gross margin will start to deteriorate, high level."
    },
    {
      "speaker": "Kirsten M. Spears",
      "role": "CFO & Chief Accounting Officer",
      "text": "CFO & Chief Accounting Officer Now, I think Hock said that fairly. And the second half of the year when we do start shipping more systems, the situation is straightforward. We'll be passing through more components that are not ours. So think of it similar to the XPUs where we have memory on those XPUs and we're passing through those costs. We'll be passing through more cost within the rack. And so those gross margins will be lower. However, overall, the way Hock said it, gross margin dollars will go up, margins will go down, operating margins, because we have leverage, operating margin dollars will go up, but the margin itself as a percentage of revenues will come down a bit. But we're not -- I mean we'll guide closer to the end of the year for that."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, that will come from the line of Jim Schneider with Goldman Sachs."
    },
    {
      "speaker": "James Edward Schneider",
      "role": "Goldman Sachs Group, Inc., Research Division",
      "text": "Goldman Sachs Group, Inc., Research Division Hock, I was wondering if you might care to calibrate your expectations for AI revenue in fiscal '26 a little bit more closely. I believe you talked about acceleration in fiscal '26 off of the 65% growth rate you did in fiscal '25. And then you're guiding to 100% growth for Q1. So I'm just wondering if the Q1 is a good jumping off point for the growth rate you expect for the full year or something maybe a little bit less than that. And then maybe if you could separately clarify whether your $1 billion of orders for the fifth customer is indeed OpenAI, which you made a separate announcement about."
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director Wow, there's a lot of questions here. But let me start off with '26. Our backlog is very dynamic these days, as I said, and it is continuing to ramp up. And you're right, we originally, 6 months ago, said, maybe year-on-year AI revenues would grow in '26, 60%, 70%. Q1, we doubled. And Q1 '26 today, we're saying it's doubled. And we're looking at it because all the fresh orders keeps coming in, and we give you a milestone of where we are today, which is $73 billion of backlog to be shipped over the next 18 months. And we do fully expect, as I answered the earlier question, for that $73 billion over the 18 months to keep growing. Now it's a moving number as we move in time, but it will grow. And it's hard for me to pinpoint what '26 is going to look like precisely. So I'd rather not give you guys any guidance, and that's why we don't give you guidance, but we do give it for Q1. Give it time, we'll give it for Q2. And you're right to ask is it an accelerating trend? And my answer is it's likely to be an accelerating trend as we progress through '26. I hope that answers your question."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, and that will come from the line of Ben Reitzes with Melius Research."
    },
    {
      "speaker": "Benjamin Alexander Reitzes",
      "role": "Melius Research LLC",
      "text": "Melius Research LLC Hock, I wanted to ask, I'm not sure if the last caller said something on it, but I didn't hear it in the answer was -- I wanted to ask about the OpenAI contract. It's supposed to start in the second half of the year and go through 2029 for 10 gigawatts. I'm going to assume that, that's the fifth customer order there. And I was just wondering if you're still confident in that being a driver. Are there any obstacles to making that a major driver? And when you expect that to contribute and your confidence in it?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director You didn't hear that answer from my last caller, Jim's question, because I didn't answer it. I did not answer it and I'm not answering it either. It's the fifth customer, and it's a real customer and it will grow. They are on their multiyear journey to their own XPUs. And let's leave it at that. As far as the OpenAI view that you have, we appreciate the fact that it is a multiyear journey that will run through '29 as our press release with OpenAI showed, 10 gigawatts between '26 -- more like '27, '28, '29, Ben, not '26. It's more like '27, '28, '29, 10 gigawatts. That was the OpenAI discussion. And that's, I call it, an agreement and alignment of where we're headed with respect to various respected and valued customer, OpenAI. But we do not expect much in '26."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, and that will come from the line of C.J. Muse with Cantor Fitzgerald."
    },
    {
      "speaker": "Christopher James Muse",
      "role": "Cantor Fitzgerald & Co., Research Division",
      "text": "Cantor Fitzgerald & Co., Research Division I guess, Hock, I wanted to talk about custom silicon and maybe speak to how you expect content to grow for Broadcom generation to generation. And as part of that, your competitor announced CPX offering, essentially accelerator for -- an accelerator for massive context windows. I'm curious if you see a broadening opportunity for your existing 5 customers to have multiple XPU offerings."
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director Thank you. No, yes, you hit it right on. I mean the nice thing about a custom accelerator is you try not to do one-size-fits-all and generationally. Each of these 5 customers now can create their version of an XPU custom accelerator for training and inference. And basically, it's almost 2 parallel threads going on almost simultaneously for each of them. So I don't have plenty of versions to deal with. I don't need to create any more version. We've got plenty of different content out there just on the basis of creating these custom accelerators. And by the way, when you do custom accelerators, you tend to put more hardware in that unique differentiated versus trying to make it work on software and creating kernels into software. I know that's very tricky too. But thinking about the difference where you can create in hardware, those sparse core data routers versus the dense matrix multipliers, all in one same chip. And that's many -- just one example of what creating custom accelerators is letting us do, or for that matter, a variation in how much memory capacity or memory bandwidth from -- for the same customer from chip to chip, just because even in inference, you want to do more reasoning versus decoding versus something else like prefill. So you literally start to create different hardware for different aspects of how you want to train or inference and run your workloads. It's a very fascinating area, and we are seeing a lot of variations and multiple chips for each of our customers."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, and that will come from the line of Harsh Kumar with Piper Sandler."
    },
    {
      "speaker": "Harsh V. Kumar",
      "role": "Piper Sandler & Co., Research Division",
      "text": "Piper Sandler & Co., Research Division Yes, Hock and team, first of all, congratulations on some pretty stunning numbers. I've got an easy one and a more strategic one. The easy one is, your guide in AI, Hock and Kirsten, is calling for almost $1.7 billion of sequential growth. I was curious maybe you can talk about the diversity of the growth between the 3 existing customers. Is it pretty well spread out? All of them growing? Or is one sort of driving much of the growth? And then, Hock, strategically, one of your competitors bought a photonic fabric company recently. I was curious about your take on that technology and if you think it's disruptive or you think it's just gimmickry at this point in time."
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director I like the way you address this question because the way that you address the tech question to me. It's almost hesitant. Thank you. I appreciate that. But on your first part, yes, we are driving growth and it began to feel like this thing never ends, and it's a real mixed bag of existing customers and on existing XPUs. And a big part of it is XPUs that we're seeing. And that's not to slow down the fact that, as I indicated in my remarks and commented on the demand for switches, not just Tomahawk 6, Tomahawk 5 switches, the demand for our latest 1.6  terabit per second DSPs that enables optical interconnects for scale out particularly. It's just very, very strong. And by extension, demand for the optical components like lasers, PIN diodes just going nuts. All that come together. Now all that is small, relatively lesser dollars when it comes to XPUs, as you probably guess. I mean, to give you a sense, maybe let me look at it on a backlog side. Of the $73 billion of AI revenue backlog over the next 18 months, I talked about, maybe $20 billion of it is everything else. The rest is XPUs. Hope that gives you a sense of what the mix is. But [indiscernible] the rest is still $20 billion, that's not small by any means. So we value that. So when you talk about your next question of silicon photonics as a means to create basically much better, more efficient, lower power interconnects in not just scale-out, but hopefully, scale-up, yes, I could see a point in time in the future when silicon photonics method is the only way to do it. We're not quite there yet. But we have the technology and we continue to develop the technology, even at each time we develop it first for 400 gigabit bandwidth, then going on to 800 gigabit bandwidth, not ready for it yet. And even with the product -- and we're now doing it for 1.6 terabit bandwidth to create silicon photonics switches, silicon photonics interconnects, not even sure it will get fully deployed because engineers -- our engineers, our peers -- and the peers we have out there will somehow try to find a way to still do -- try to do scale-up within a rack in copper as long as possible and in scale-out in non-pluggable optics. The final, final straw is when you can do it well in pluggable optics. And of course, when you can do it even in copper, then you're right, you go to silicon photonics and it will happen, and we're ready for it, just saying not anytime soon."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, that will come from the line of Karl Ackerman with BNP Paribas."
    },
    {
      "speaker": "Karl  Ackerman",
      "role": "BNP Paribas, Research Division",
      "text": "BNP Paribas, Research Division Hock, could you speak to the supply chain resiliency and visibility you have with your key material suppliers, particularly CoWoS as you not only support your existing customer programs but the two new custom compute processors that you announced intra-quarter. I guess, what I can get at is you also happen to address the very large subset of networking and compute AI supply chains. You talked about record backlog. If you were to pinpoint some of the bottlenecks that you have, the areas that you're aiming to address and mitigate from supply chain bottlenecks, what would they be? And how do you see that ameliorating into '26?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director It's across the board, typically. I mean it's -- we are very fortunate in some ways that we have the product, technology and the operating business lines to create multiple key leading-edge components that enables today's state-of-the-art AI data centers. I mean our DSP, as I said earlier, is now at 1.6 terabit per second. That's the leading edge connectivity for bandwidth for this -- for the top of the [indiscernible] XPU and even GPU. And we intend to be that way. And we have the lasers, EMLs, VCSELs, CW lasers that goes with it. So it's fortunate that we have all this and the key active components that go with it. And we see it very early, and we expand the capacity as we do the design to match it. And long -- this is a long answer to what I'm trying to get at, which is, I think we are -- of any of these data center suppliers of the system racks, not counting the power shell and all that. Now that starts to get beyond us on the power shell and the transformers and the gas turbines. If you just look at the rack, the systems on AI, we probably have a good handle on where the bottlenecks are because sometimes we are part of the bottlenecks, which we then want to get resolved. So we feel pretty good about that through 2026."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "One moment for our next question, that will come from the line of Christopher Rolland with Susquehanna."
    },
    {
      "speaker": "Christopher Adam Jackson Rolland",
      "role": "Susquehanna Financial Group, LLLP, Research Division",
      "text": "Susquehanna Financial Group, LLLP, Research Division Just first, a clarification and then my question. And sorry to come back to this issue. But if I understand you correctly, Hock, I think you were saying that OpenAI would be a general agreement, so it's not binding, maybe similar to the agreements with both NVIDIA and AMD. And then secondly, you talked about flat non-AI semiconductor revenue, maybe what's going on there? Is there still an inventory overhang? And what could -- what do we need to get that going again? Do you see growth eventually in that business?"
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director Well, on the non-AI semiconductor, we see broadband literally recovering very well. And we don't see the others -- no, we see stability. We don't see a sharp recovery that is sustainable yet. So I guess, given a couple more quarters, but we don't see any further deterioration in demand. And it's more, I think, maybe AI is sucking the oxygen a lot out of enterprise spending elsewhere and hyperscaler spending elsewhere. We don't see getting any worse. We don't see it recovering very quickly with the exception of broadband. That's a simple summary of non-AI. With respect to OpenAI, without diving in too deep, I'm just telling you what that 10 gigawatt announcement is all about. Separately, the journey with them on the custom accelerator progresses at a very advanced stage and will happen very, very quickly. And it's -- and we will have a committed element to this whole thing, and [ it will ]. But what I was articulating earlier was the 10 gigawatt announcement. And that 10 gigawatt announcement is an agreement to be aligned on developing 10 gigawatts for OpenAI over '27 to '29 time frame. That's different from the XPU program we're developing with them."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "And we do have time for one final question, and that will come from the line of Joe Moore with Morgan Stanley."
    },
    {
      "speaker": "Joseph Lawrence Moore",
      "role": "Morgan Stanley, Research Division",
      "text": "Morgan Stanley, Research Division Great. So if you have $21 billion of rack revenue in the second half of '26, I guess, do we stay at that run rate beyond that? Are you going to continue to sell racks? Or does that sort of -- that type of business mix shift over time? And I'm really just trying to figure out the percentage of your 18-month backlog that's actually full systems at this point."
    },
    {
      "speaker": "Hock E. Tan",
      "role": "President, CEO & Executive Director",
      "text": "President, CEO & Executive Director Well, it's an interesting question. And that question basically comes to how much compute capacity is needed by our customers over the next, as I say, over the period beyond 18 months. And your guess is probably as good as mine based on what we all know out there, which is really what they relate to. But if they need more, then you see that continuing even larger. If they don't need it, then probably it won't. But as of -- what we are trying to indicate is that's the demand we are seeing over that period of time right now."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "I would now like to turn the call back over to Ji Yoo for any closing remarks."
    },
    {
      "speaker": "Ji  Yoo",
      "role": "Director of Investor Relations",
      "text": "Director of Investor Relations Thank you, operator. This quarter, Broadcom will be presenting at the New Street Research Virtual AI Big Idea Conference on Monday, December 15, 2025. Broadcom currently plans to report its earnings for the first quarter of fiscal year 2026 after close of market on Wednesday, March 4, 2026. A public webcast of Broadcom's earnings conference call will follow at 2:00 p.m. Pacific. That will conclude our earnings call today. Thank you all for joining. Operator, you may end the call. OperatorThis concludes today's program. Thank you all for participating. You may now disconnect. Copyright Â© 2025 by S&P Global Market Intelligence, a division of S&P Global Inc. All rights reserved. These materials have been prepared solely for information purposes based upon information generally available to the public and from sources believed to be reliable. No content (including index data, ratings, credit- related analyses and data, research, model, software or other application or output therefrom) or any part thereof (Content) may be modified, reverse engineered, reproduced or distributed in any form by any means, or stored in a database or retrieval system, without the prior written permission of S&P Global Market Intelligence or its affiliates (collectively, S&P Global). The Content shall not be used for any unlawful or unauthorized purposes. S&P Global and any third-party providers, (collectively S&P Global Parties) do not guarantee the accuracy, completeness, timeliness or availability of the Content. S&P Global Parties are not responsible for any errors or omissions, regardless of the cause, for the results obtained from the use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P GLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR DEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE CONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no event shall S&P Global Parties be liable to any party for any direct, indirect, incidental, exemplary, compensatory, punitive, special or consequential damages, costs, expenses, legal fees, or losses (including, without limitation, lost income or lost profits and opportunity costs or losses caused by negligence) in connection with any use of the Content even if advised of the possibility of such damages. S&P Global Market Intelligence's opinions, quotes and credit-related and other analyses are statements of opinion as of the date they are expressed and not statements of fact or recommendations to purchase, hold, or sell any securities or to make any investment decisions, and do not address the suitability of any security. S&P Global Market Intelligence may provide index data. Direct investment in an index is not possible. Exposure to an asset class represented by an index is available through investable instruments based on that index. S&P Global Market Intelligence assumes no obligation to update the Content following publication in any form or format. The Content should not be relied on and is not a substitute for the skill, judgment and experience of the user, its management, employees, advisors and/or clients when making investment and other business decisions. S&P Global Market Intelligence does not act as a fiduciary or an investment advisor except where registered as such. S&P Global keeps certain activities of its divisions separate from each other in order to preserve the independence and objectivity of their respective activities. As a result, certain divisions of S&P Global may have information that is not available to other S&P Global divisions. S&P Global has established policies and procedures to maintain the confidentiality of certain nonpublic information received in connection with each analytical process. S&P Global may receive compensation for its ratings and certain analyses, normally from issuers or underwriters of securities or from obligors. S&P Global reserves the right to disseminate its opinions and analyses. S&P Global's public ratings and analyses are made available on its Web sites, www.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and www.globalcreditportal.com (subscription), and may be distributed through other means, including via S&P Global publications and third-party redistributors. Additional information about our ratings fees is available at www.standardandpoors.com/usratingsfees. Â© 2025 S&P Global Market Intelligence."
    }
  ],
  "source_file": "Broadcom Inc., Q4 2025 Earnings Call, Dec 11, 2025.rtf"
}