{
  "event_id": "NVDA_2025-02-26",
  "ticker": "NVDA",
  "company": "NVIDIA Corporation",
  "quarter": 4,
  "fiscal_year": 2025,
  "call_date": "2025-02-26",
  "call_start_ts": "2025-02-26 22:00:00+00:00",
  "raw_text": "\n|[pic]                     |\n\nNVIDIA Corporation NasdaqGS:NVDA\nFQ4 2025 Earnings Call Transcripts\nWednesday, February 26, 2025 10:00 PM GMT\nS&P Global Market Intelligence Estimates\n|      |-FQ4 2025-           |-FQ1  |-FY 2025-            |-FY   |\n|      |                     |2026- |                     |2026- |\n|                              |CONSENSUS      |ACTUAL         |SURPRISE       |\n|                   |CONSENSUS          |ACTUAL             |SURPRISE           |\n|FQ1 2025           |0.56               |0.61               |[pic]8.93 %        |\n|FQ2 2025           |0.64               |0.68               |[pic]6.25 %        |\n|FQ3 2025           |0.75               |0.81               |[pic]8.00 %        |\n|FQ4 2025           |0.85               |0.89               |[pic]4.71 %        |\n\n|Table of Contents                                     |   |\n|Call Participants          |..............................................|3      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|Presentation               |..............................................|4      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|Question and Answer        |..............................................|9      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|                                                                                  |\n|Call Participants                                                                 |\n|                           |                           |                           |\n|EXECUTIVES                 |                           |                           |\n|                           |Vivek  Arya                |                           |\n|                           |BofA Securities, Research  |                           |\n|Colette M. Kress           |Division                   |                           |\n|Executive VP & CFO         |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Jen-Hsun  Huang            |                           |                           |\n|Co-Founder, CEO, President |                           |                           |\n|& Director                 |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Stewart  Stecker           |                           |                           |\n|Director of Investor       |                           |                           |\n|Relations                  |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|ANALYSTS                   |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Aaron Christopher Rakers   |                           |                           |\n|Wells Fargo Securities,    |                           |                           |\n|LLC, Research Division     |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Atif  Malik                |                           |                           |\n|Citigroup Inc., Research   |                           |                           |\n|Division                   |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Benjamin Alexander Reitzes |                           |                           |\n|                           |                           |                           |\n|Melius Research LLC        |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Christopher James Muse     |                           |                           |\n|Cantor Fitzgerald & Co.,   |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Harlan L.  Sur             |                           |                           |\n|JPMorgan Chase & Co,       |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Joseph Lawrence Moore      |                           |                           |\n|Morgan Stanley, Research   |                           |                           |\n|Division                   |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Mark John Lipacis          |                           |                           |\n|Evercore ISI Institutional |                           |                           |\n|Equities, Research Division|                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Timothy Michael Arcuri     |                           |                           |\n|UBS Investment Bank,       |                           |                           |\n|Research Division          |                           |                           |\n|                                                                                  |\n\n\nPresentation\n\n\nOperator\n\nGood afternoon. My name is Krista, and I will be your conference operator\ntoday. At this time, I would like to welcome everyone to NVIDIA's fourth\nquarter earnings call. [Operator Instructions]\n\n\nStewart Stecker, you may begin your conference.\n\n\nStewart  Stecker\nDirector of Investor Relations\n\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference\ncall for the fourth quarter of fiscal 2025. With me today from NVIDIA are\nJensen Huang, President and Chief Executive Officer; and Colette Kress,\nExecutive Vice President and Chief Financial Officer.\n\n\nI'd like to remind you that our call is being webcast live on NVIDIA's\nInvestor Relations website. The webcast will be available for replay until\nthe conference call to discuss our financial results for the first quarter\nof fiscal 2026. The content of today's call is NVIDIA's property. It can't\nbe reproduced or transcribed without prior written consent.\n\n\nDuring this call, we may make forward-looking statements based on current\nexpectations. These are subject to a number of significant risks and\nuncertainties, and our actual results may differ materially. For a\ndiscussion of factors that could affect our future financial results and\nbusiness, please refer to the disclosure in today's earnings release, our\nmost recent Forms 10-K and 10-Q and the reports that we may file on Form 8-\nK with the Securities and Exchange Commission. All our statements are made\nas of today, February 26, 2025, based on information currently available to\nus. Except as required by law, we assume no obligation to update any such\nstatements.\n\n\nDuring this call, we will discuss non-GAAP financial measures. You can find\na reconciliation of these non-GAAP financial measures to GAAP financial\nmeasures in our CFO commentary, which is posted on our website.\n\n\nWith that, let me turn the call over to Colette.\n\n\nColette M. Kress\nExecutive VP & CFO\n\nThanks, Stewart. Q4 was another record quarter. Revenue of $39.3 billion\nwas up 12% sequentially and up 78% year-on-year and above our outlook of\n$37.5 billion. For fiscal 2025 revenue was $130.5 billion, up 114% from the\nprior year.\n\n\nLet's start with Data Center. Data center revenue for fiscal 2025 was\n$115.2 billion, more than doubling from the prior year. In the fourth\nquarter, Data Center revenue of $35.6 billion was a record, up 16%\nsequentially and 93% year-on-year, as the Blackwell ramp commenced and\nHopper 200 continued sequential growth.\n\n\nIn Q4, Blackwell sales exceeded our expectations. We delivered $11 billion\nof Blackwell revenue to meet strong demand. This is the fastest product\nramp in our company's history, unprecedented in its speed and scale.\nBlackwell production is in full gear across multiple configurations, and we\nare increasing supply quickly expanding customer adoption.\n\n\nOur Q4 Data Center compute revenue jumped 18% sequentially and over 2x year-\non-year. Customers are racing to scale infrastructure to train the next\ngeneration of cutting-edge models and unlock the next level of AI\ncapabilities. With Blackwell, it will be common for these clusters to start\nwith 100,000 GPUs or more. Shipments have already started for multiple\ninfrastructures of this size.\n\n\nPost-training and model customization are fueling demand for NVIDIA\ninfrastructure and software as developers and enterprises leverage\ntechniques such as fine-tuning, reinforcement learning and distillation to\ntailor models for domain-specific use cases. Hugging Face alone hosts over\n90,000 derivatives created from the Llama foundation model.\n\n\nThe scale of post-training and model customization is massive and can\ncollectively demand orders of magnitude, more compute than pretraining. Our\ninference demand is accelerating, driven by test-time scaling and new\nreasoning models like OpenAI o3, DeepSeek-R1 and Grok 3. Long thinking\nreasoning AI can require 100x more compute per task compared to one-shot\ninferences. Blackwell was architected for reasoning AI inference. Blackwell\nsupercharges reasoning AI models with up to 25x higher token throughput and\n20x lower cost versus Hopper 100. It is revolutionary. Transformer engine\nis built for LLM and mixture of experts inference.\n\n\nAnd its NVLink domain delivers 14x the throughput of PCIe Gen 5, ensuring\nthe response time, throughput and cost efficiency needed to tackle the\ngrowing complexity of inference at scale. Companies across industries are\ntapping into NVIDIA's full stack inference platform to boost performance\nand slash costs. Now tripled inference throughput and cut costs by 66%\nusing NVIDIA TensorRT for its screenshot feature. Perplexity sees 435\nmillion monthly queries and reduced its inference costs 3x with NVIDIA\nTriton Inference Server and TensorRT-LLM.\n\n\nMicrosoft Bing achieved a 5x speed up at major TCO savings for Visual\nSearch across billions of images with NVIDIA TensorRT and acceleration\nlibraries. Blackwell has great demand for inference. Many of the early\nGB200 deployments are earmarked for inference, a first for a new\narchitecture. Blackwell addresses the entire AI market from pretraining,\npost-training to inference across cloud, to on-premise, to enterprise.\nCUDA's programmable architecture accelerates every AI model and over 4,400\napplications, ensuring large infrastructure investments against\nobsolescence in rapidly evolving market.\n\n\nOur performance and pace of innovation is unmatched. We're driven to a 200x\nreduction in inference costs in just the last 2 years. We delivered the\nlowest TCO and the highest ROI. And full stack optimizations for NVIDIA and\nour large ecosystem, including 5.9 million developers continuously improve\nour customers' economics.\n\n\nIn Q4, large CSPs represented about half of our data center revenue, and\nthese sales increased nearly 2x year-on-year. Large CSPs were some of the\nfirst to stand up Blackwell with Azure, GCP, AWS and OCI bringing GB200\nsystems to cloud regions around the world to meet surging customer demand\nfor AI. Regional cloud hosting NVIDIA GPUs increased as a percentage of\ndata center revenue, reflecting continued AI factory build-outs globally\nand rapidly rising demand for AI reasoning models and agents where we've\nlaunched a 100,000 GB200 cluster-based incidents with NVLink Switch and\nQuantum-2 InfiniBand.\n\n\nConsumer Internet revenue grew 3x year-on-year, driven by an expanding set\nof generative AI and deep learning use cases. These include recommender\nsystems, vision-language understanding, synthetic data generation, search\nand agentic AI. For example, xAI is adopting the GB200 to train and\ninference its next generation of Grok AI models. Meta's cutting-edge\nAndromeda advertising engine runs on NVIDIA's Grace Hopper Superchip\nserving vast quantities of ads across Instagram, Facebook applications.\nAndromeda harnesses Grace Hopper's fast interconnect and large memory to\nboost inference throughput by 3x, enhanced ad personalization and deliver\nmeaningful jumps in monetization and ROI.\n\n\nEnterprise revenue increased nearly 2x year on accelerating demand for\nmodel fine-tuning, RAG and agentic AI workflows and GPU accelerated data\nprocessing. We introduced NVIDIA Llama Nemotron model family NIMs to help\ndevelopers create and deploy AI agents across a range of applications,\nincluding customer support, fraud detection and product supply chain and\ninventory management.\n\n\nLeading AI agent platform providers, including SAP and ServiceNow are among\nthe first to use new models. Health care leaders, IQVIA, Illumina and Mayo\nClinic as well as ARC Institute are using NVIDIA AI to speed drug\ndiscovery, enhance genomic research and pioneer advanced health care\nservices with generative and agentic AI.\n\n\nAs AI expands beyond the digital world, NVIDIA infrastructure and software\nplatforms are increasingly being adopted to power robotics and physical AI\ndevelopment. One of the early and largest robotics applications in\nautonomous vehicles where virtually every AV company is developing on\nNVIDIA in the data center, the car or both.\n\n\nNVIDIA's automotive vertical revenue is expected to grow to approximately\n$5 billion this fiscal year. At CES, Hyundai Motor Group announced it is\nadopting NVIDIA technologies to accelerate AV and robotics development and\nsmart factory initiatives. Vision transformers, self-supervised learning,\nmultimodal sensor fusion and high fidelity simulation are driving\nbreakthroughs in AV development and will require 10x more compute. At CES,\nwe announced the NVIDIA Cosmos World Foundation Model Platform. Just as\nlanguage, foundation models have revolutionized language AI, Cosmos is a\nphysical AI to revolutionize robotics. Leading robotics and automotive\ncompanies, including ridesharing giant Uber, are among the first to adopt\nthe platform.\n\n\nFrom a geographic perspective, sequential growth in our Data Center revenue\nwas strongest in the U.S., driven by the initial ramp up Blackwell.\nCountries across the globe are building their AI ecosystems and demand for\ncompute infrastructure is surging. France's EUR 200 billion AI investment\nand the EU's EUR 200 billion InvestAI initiatives offer a glimpse into the\nbuild-out to set redefined global AI infrastructure in the coming years.\n\n\nNow as a percentage of total data center revenue, data center sales in\nChina remained well below levels seen on the onset of export controls.\nAbsent any change in regulations, we believe that China shipments will\nremain roughly at the current percentage. The market in China for data\ncenter solutions remains very competitive. We will continue to comply with\nexport controls while serving our customers.\n\n\nNetworking revenue declined 3% sequentially. Our networking attached to GPU\ncompute systems is robust at over 75%. We are transitioning from small\nNVLink 8 with InfiniBand to large NVLink 72 with Spectrum-X. Spectrum-X and\nNVLink Switch revenue increased and represents a major new growth vector.\nWe expect networking to return to growth in Q1.\n\n\nAI requires a new class of networking. NVIDIA offers NVLink Switch systems\nfor scale-up compute. For scale out, we offer Quantum InfiniBand for HPC\nsupercomputers and Spectrum-X for Ethernet environments. Spectrum-X\nenhances the Ethernet for AI computing and has been a huge success.\nMicrosoft Azure, OCI, CoreWeave and others are building large AI factories\nwith Spectrum-X.\n\n\nThe first Stargate data centers will use Spectrum-X. Yesterday, Cisco\nannounced integrating Spectrum-X into their networking portfolio to help\nenterprises build AI infrastructure. With its large enterprise footprint\nand global reach, Cisco will bring NVIDIA Ethernet to every industry.\n\n\nNow moving to Gaming and AI PCs. Gaming revenue of $2.5 billion decreased\n22% sequentially and 11% year-on-year. Full year revenue of $11.4 billion\nincreased 9% year-on-year, and demand remains strong throughout the\nholiday. However, Q4 shipments were impacted by supply constraints. We\nexpect strong sequential growth in Q1 as supply increases.\n\n\nThe new GeForce RTX 50 Series desktop and laptop GPUs are here. Built for\ngamers, creators and developers they fuse AI and graphics redefining visual\ncomputing, powered by the Blackwell architecture, fifth generation Tensor\nCores and fourth generation RT Cores and featuring up to 3,400 AI TOPS.\nThese GPUs deliver a 2x performance leap and new AI-driven rendering\nincluding Neural Shaders, digital human technologies, geometry and\nlighting.\n\n\nThe new DLSS 4 boost frame rates up to 8x with AI-driven frame generation,\nturning 1 rendered frame into 3. It also features the industry's first real-\ntime application of transformer models packing 2x more parameters and 4x\nthe compute for unprecedented visual fidelity.\n\n\nWe also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA\nMax-Q technology that extends battery life by up to an incredible 40%.\nThese laptops will be available starting in March from the world's top\nmanufacturers.\n\n\nMoving to our Professional Visualization business. Revenue of $511 million\nwas up 5% sequentially and 10% year-on-year. Full year revenue of $1.9\nbillion increased 21% year-on-year. Key industry verticals driving demand\ninclude automotive and health care.\n\n\nNVIDIA technologies and generative AI are reshaping design, engineering and\nsimulation workloads. Increasingly, these technologies are being leveraged\nin leading software platforms from ANSYS, Cadence and Siemens fueling\ndemand for NVIDIA RTX workstations.\n\n\nNow moving to Automotive. Revenue was a record $570 million, up 27%\nsequentially and up 103% year-on-year. Full year revenue of $1.7 billion\nincreased 55% year-on-year. Strong growth was driven by the continued ramp\nin autonomous vehicles, including cars and robotaxis. At CES, we announced\nToyota, the world's largest auto maker will build its next-generation\nvehicles on NVIDIA Orin running the safety certified NVIDIA DriveOS. We\nannounced Aurora and Continental will deploy driverless trucks at scale\npowered by NVIDIA DRIVE Thor.\n\n\nFinally, our end-to-end autonomous vehicle platform NVIDIA DRIVE Hyperion\nhas passed industry safety assessments like TÜV SÜD and TÜV Rheinland, 2 of\nthe industry's foremost authorities for automotive-grade safety and\ncybersecurity. NVIDIA is the first AV platform to receive a comprehensive\nset of third-party assessments.\n\n\nOkay. Moving to the rest of the P&L. GAAP gross margin was 73% and non-GAAP\ngross margin was 73.5%, down sequentially as expected with our first\ndeliveries of the Blackwell architecture. As discussed last quarter,\nBlackwell is a customizable AI infrastructure with several different types\nof NVIDIA build chips, multiple networking options and for air and liquid-\ncooled data center.\n\n\nWe exceeded our expectations in Q4 in ramping Blackwell, increasing system\navailability, providing several configurations to our customers. As\nBlackwell ramps, we expect gross margins to be in the low 70s.\n\n\nInitially, we are focused on expediting the manufacturing of Blackwell\nsystems to meet strong customer demand as they race to build out Blackwell\ninfrastructure. When fully ramped, we have many opportunities to improve\nthe cost and gross margin will improve and return to the mid-70s, late this\nfiscal year.\n\n\nSequentially, GAAP operating expenses were up 9% and non-GAAP operating\nexpenses were 11%, reflecting higher engineering development costs and\nhigher compute and infrastructure costs for new product introductions. In\nQ4, we returned $8.1 billion to shareholders in the form of share\nrepurchases and cash dividends.\n\n\nLet me turn to the outlook in the first quarter. Total revenue is expected\nto be $43 billion, plus or minus 2%. Continuing with its strong demand, we\nexpect a significant ramp of Blackwell in Q1. We expect sequential growth\nin both Data Center and Gaming. Within Data Center, we expect sequential\ngrowth from both compute and networking.\n\n\nGAAP and non-GAAP gross margins are expected to be 70.6% and 71%,\nrespectively, plus or minus 50 basis points. GAAP and non-GAAP operating\nexpenses are expected to be approximately $5.2 billion and $3.6 billion,\nrespectively. We expect full year fiscal year '26 operating expenses to\ngrow to be in the mid-30s.\n\n\nGAAP and non-GAAP other income and expenses are expected to be an income of\napproximately $400 million, excluding gains and losses from nonmarketable\nand publicly held equity securities. GAAP and non-GAAP tax rates are\nexpected to be 17%, plus or minus 1%, excluding any discrete items. Further\nfinancial details are included in the CFO commentary and other information\navailable on our IR website, including a new financial information AI\nagent.\n\n\nIn closing, let me highlight upcoming events for the financial community.\nWe will be at the TD Cowen Health Care Conference in Boston on March 3 and\nat the Morgan Stanley Technology, Media and Telecom Conference in San\nFrancisco on March 5. Please join us for our Annual GTC Conference starting\nMonday, March 17 in San Jose, California. Jensen will deliver a news-packed\nkeynote on March 18, and we will host a Q&A session for our financial\nanalysts the next day, March 19. We look forward to seeing you at these\nevents. Our earnings call to discuss the results for our first quarter of\nfiscal 2026 is scheduled for May 28, 2025.\nWe are going to open up the call, operator, to questions. If you could\nstart that, that would be great.\n\n\nQuestion and Answer\n\n\nOperator\n\n[Operator Instructions] And your first question comes from C.J. Muse with\nCantor Fitzgerald.\n\n\nChristopher James Muse\nCantor Fitzgerald & Co., Research Division\n\nI guess for me, Jensen, as TestCon compute and reinforcement learning shows\nsuch promise, we're clearly seeing an increasing blurring of the lines\nbetween training and inference, what does this mean for the potential\nfuture of potentially inference dedicated clusters? And how do you think\nabout the overall impact to NVIDIA and your customers?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes, I appreciate that C.J. There are now multiple scaling loss. There's\nthe pre-training scaling law, and that's going to continue to scale because\nwe have multimodality, we have data that came from reasoning that are now\nused to do pretraining.\n\n\nAnd then the second is post-training scaling law, using reinforcement\nlearning human feedback, reinforcement learning AI feedback, reinforcement\nlearning, verifiable rewards. The amount of computation you use for post\ntraining is actually higher than pretraining. And it's kind of sensible in\nthe sense that you could, while you're using reinforcement learning,\ngenerate an enormous amount of synthetic data or synthetically generated\ntokens. AI models are basically generating tokens to train AI models. And\nthat's post-training.\n\n\nAnd the third part, this is the part that you mentioned is test-time\ncompute or reasoning, long thinking, inference scaling. They're all\nbasically the same ideas. And there you have a chain of thought, you've\nsearch. The amount of tokens generated, the amount of inference compute\nneeded is already 100x more than the one-shot examples and the one-shot\ncapabilities of large language models in the beginning. And that's just the\nbeginning. This is just the beginning.\n\n\nThe idea that the next generation could have thousands times and even\nhopefully, extremely thoughtful and simulation-based and search-based\nmodels that could be hundreds of thousands, millions of times more compute\nthan today is in our future.\n\n\nAnd so the question is how do you design such an architecture? Some of it --\n some of the models are auto regressive. Some of the models are diffusion-\nbased. Some of it -- some of the times you want your data center to have\ndisaggregated inference. Sometimes it is compacted.\n\n\nAnd so it's hard to figure out what is the best configuration of a data\ncenter, which is the reason why NVIDIA's architecture is so popular. We run\nevery model. We are great at training. The vast majority of our compute\ntoday is actually inference and Blackwell takes all of that to a new level.\nWe designed Blackwell with the idea of reasoning models in mind. And when\nyou look at training, it's many times more performant.\n\n\nBut what's really amazing is for long thinking test-time scaling, reasoning\nAI models were tens of times faster, 25x higher throughput. And so\nBlackwell is going to be incredible across the board. And when you have a\ndata center that allows you to configure and use your data center based on\nare you doing more pretraining now, post training now or scaling out your\ninference, our architecture is fungible and easy to use in all of those\ndifferent ways. And so we're seeing, in fact, much, much more concentration\nof a unified architecture than ever before.\n\n\nOperator\n\nYour next question comes from the line of Joe Moore with JPMorgan (sic) [\nMorgan Stanley ].\n\n\nJoseph Lawrence Moore\nMorgan Stanley, Research Division\n\nMorgan Stanley, actually. I wonder if you could talk about GB200. At CES,\nyou sort of talked about the complexity of the rack level systems and the\nchallenges you have. And then as you said in the prepared remarks, we've\nseen a lot of general availability. Where are you in terms of that ramp?\n\n\nAre there still bottlenecks to consider at a systems level above and beyond\nthe chip level? And just have you maintained your enthusiasm for the NVL72\nplatforms?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, I'm more enthusiastic today than I was at CES. And the reason for\nthat is because we've shipped a lot more since CES. We have some 350 plants\nmanufacturing the 1.5 million components that go into each one of the\nBlackwell racks, Grace Blackwell racks.\n\n\nYes, it's extremely complicated. And we successfully and incredibly ramped\nup Grace Blackwell, delivering some $11 billion of revenues last quarter.\nWe're going to have to continue to scale as demand is quite high, and\ncustomers are anxious and impatient to get their Blackwell systems.\n\n\nYou've probably seen on the web, a fair number of celebrations about Grace\nBlackwell systems coming online and we have them, of course. We have a\nfairly large installation of Grace Blackwells for our own engineering and\nour own design teams and software teams.\n\n\nCoreWeave has now been quite public about the successful bring up of\ntheirs. Microsoft has. Of course, OpenAI has. And you're starting to see\nmany come online. And so I think the answer to your question is nothing is\neasy about what we're doing, but we're doing great, and all of our partners\nare doing great.\n\n\nOperator\n\nYour next question comes from the line of Vivek Arya with Bank of America\nSecurities.\n\n\nVivek  Arya\nBofA Securities, Research Division\n\nColette, if you wouldn't mind confirming if Q1 is the bottom for gross\nmargins? And then Jensen, my question is for you. What is on your dashboard\nto give you the confidence that the strong demand can sustain into next\nyear? And has DeepSeek and whatever innovations they came up with, has that\nchanged that view in any way?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nLet me first take the first part of the question there regarding the gross\nmargin. During our Blackwell ramp, our gross margins will be in the low\n70s. At this point, we are focusing on expediting our manufacturing,\nexpediting our manufacturing to make sure that we can provide to customers\nas soon as possible. Our Blackwell is fully ramped. And once it does -- I'm\nsorry, once our Blackwell fully ramps, we can improve our cost and our\ngross margin. So we expect to probably be in the mid-70s later this year.\n\n\nWalking through what you heard Jensen speak about the systems and their\ncomplexity, they are customizable in some cases. They've got multiple\nnetworking options. They have liquid cooled and water cooled. So we know\nthere is an opportunity for us to improve these gross margins going\nforward. But right now, we are going to focus on getting the manufacturing\ncomplete and to our customers as soon as possible.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWe know several things, Vivek. We have a fairly good line of sight of the\namount of capital investment that data centers are building out towards. We\nknow that going forward, the vast majority of software is going to be based\non machine learning. And so accelerated computing and generative AI,\nreasoning AI are going to be the type of architecture you want in your data\ncenter.\n\n\nWe have, of course, forecasts and plans from our top partners. And we also\nknow that there are many innovative, really exciting start-ups that are\nstill coming online as new opportunities for developing the next\nbreakthroughs in AI, whether it's agentic AIs, reasoning AI or physical\nAIs. The number of start-ups are still quite vibrant and each one of them\nneed a fair amount of computing infrastructure.\n\n\nAnd so I think the -- whether it's the near-term signals or the mid-term\nsignals, near-term signals, of course, are POs and forecasts and things\nlike that. Mid-term signals would be the level of infrastructure and CapEx\nscale-out compared to previous years. And then the long-term signals has to\ndo with the fact that we know fundamentally software has changed from hand\ncoding that runs on CPUs, to machine learning and AI-based software that\nruns on GPUs and accelerated computing systems.\n\n\nAnd so we have a fairly good sense that this is the future of software. And\nthen maybe as you roll it out, another way to think about that is we've\nreally only tapped consumer AI and search and some amount of consumer\ngenerative AI, advertising, recommenders, kind of the early days of\nsoftware. The next wave is coming, agentic AI for enterprise, physical AI\nfor robotics and sovereign AI as different regions build out their AI for\ntheir own ecosystems.\n\n\nAnd so each one of these are barely off the ground, and we can see them. We\ncan see them because, obviously, we're in the center of much of this\ndevelopment and we can see great activity happening in all these different\nplaces and these will happen. So near term, mid-term, long term.\n\n\nOperator\n\nYour next question comes from the line of Harlan Sur with JPMorgan.\n\n\nHarlan L.  Sur\nJPMorgan Chase & Co, Research Division\n\nYour next-generation Blackwell Ultra is set to launch in the second half of\nthis year, in line with the team's annual product cadence. Jensen, can you\nhelp us understand the demand dynamics for Ultra given that you'll still be\nramping the current generation Blackwell solutions? How do your customers\nand the supply chain also manage the simultaneous ramps of these 2\nproducts? And is the team still on track to execute Blackwell Ultra in the\nsecond half of this year?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes. Blackwell Ultra is second half. As you know, the first Blackwell was --\n we had a hiccup that probably cost us a couple of months. We're fully\nrecovered, of course. The team did an amazing job recovering and all of our\nsupply chain partners and just so many people helped us recover at the\nspeed of light. And so now we've successfully ramped production of\nBlackwell.\n\n\nBut that doesn't stop the next train. The next train is on an annual rhythm\nand Blackwell Ultra with new networking, new memories and of course, new\nprocessors, and all of that is coming online. We've have been working with\nall of our partners and customers, laying this out. They have all of the\nnecessary information, and we'll work with everybody to do the proper\ntransition.\n\n\nThis time between Blackwell and Blackwell Ultra, the system architecture is\nexactly the same. It's a lot harder going from Hopper to Blackwell because\nwe went from an NVLink 8 system to a NVLink 72-based system. So the\nchassis, the architecture of the system, the hardware, the power delivery,\nall of that had to change. This was quite a challenging transition.\n\n\nBut the next transition will slot right in. Blackwell Ultra will slot right\nin. We've also already revealed and been working very closely with all of\nour partners on the click after that. And the click after that is called\nVera Rubin and all of our partners are getting up to speed on the\ntransition of that and so preparing for that transition. And again, we're\ngoing to provide a big, huge step-up. And so come to GTC, and I'll talk to\nyou about Blackwell Ultra, Vera Rubin and then show you what's the one\nclick after that. Really exciting new products, so to come to GTC, please.\n\n\nOperator\n\nYour next question comes from the line of Timothy Arcuri with UBS.\n\n\nTimothy Michael Arcuri\nUBS Investment Bank, Research Division\n\nJensen, we heard a lot about custom ASICs. Can you kind of speak to the\nbalance between custom ASIC and merchant GPU? We hear about some of these\nheterogeneous superclusters to use both GPU and ASIC. Is that something\ncustomers are planning on building? Or will these infrastructures remain\nfairly distinct?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, we build very different things than ASICs, in some ways, completely\ndifferent in some areas we intercept. We're different in several ways. One,\nNVIDIA'S architecture is general whether you're -- you've optimized for\nauto regressive models or diffusion-based models or vision-based models or\nmultimodal models or text models. We're great in all of it.\n\n\nWe're great at all of it because our software stack is so -- our\narchitecture is flexible, our software stack ecosystem is so rich that\nwe're the initial target of most exciting innovations and algorithms. And\nso by definition, we're much, much more general than narrow. We're also\nreally good from the end-to-end from data processing, the curation of the\ntraining data, to the training of the data, of course, to reinforcement\nlearning used in post training, all the way to inference with test-time\nscaling.\n\n\nSo we're general, we're end-to-end, and we're everywhere. And because we're\nnot in just one cloud, we're in every cloud, we could be on-prem. We could\nbe in a robot. Our architecture is much more accessible and a great target\n-- initial target for anybody who's starting up a new company. And so we're\neverywhere.\n\n\nAnd the third thing I would say is that our performance and our rhythm is\nso incredibly fast. Remember that these data centers are always fixed in\nsize. They're fixed in size or they're fixed in power. And if our\nperformance per watt is anywhere from 2x to 4x to 8x, which is not unusual,\nit translates directly to revenues. And so if you have a 100-megawatt data\ncenter, if the performance or the throughput in that 100-megawatt or the\ngigawatt data center is 4x or 8x higher, your revenues for that gigawatt\ndata center is 8x higher.\n\n\nAnd the reason that is so different than data centers of the past is\nbecause AI factories are directly monetizable through its tokens generated.\nAnd so the token throughput of our architecture being so incredibly fast is\njust incredibly valuable to all of the companies that are building these\nthings for revenue generation reasons and capturing the fast ROI. And so I\nthink the third reason is performance.\n\n\nAnd then the last thing that I would say is the software stack is\nincredibly hard. Building an ASIC is no different than what we do. We build\na new architecture. And the ecosystem that sits on top of our architecture\nis 10x more complex today than it was 2 years ago. And that's fairly\nobvious because the amount of software that the world is building on top of\narchitecture is growing exponentially and AI is advancing very quickly.\n\n\nSo bringing that whole ecosystem on top of multiple chips is hard. And so I\nwould say that those 4 reasons. And then finally, I will say this, just\nbecause the chip is designed doesn't mean it gets deployed. And you've seen\nthis over and over again. There are a lot of chips that gets built, but\nwhen the time comes, a business decision has to be made, and that business\ndecision is about deploying a new engine, a new processor into a limited AI\nfactory in size, in power and in time.\n\n\nAnd our technology is not only more advanced, more performant, it has much,\nmuch better software capability and very importantly, our ability to deploy\nis lightning fast. And so these things are enough for the faint of heart,\nas everybody knows now. And so there's a lot of different reasons why we do\nwell, why we win.\n\n\nOperator\n\nYour next question comes from the line of Ben Reitzes with Melius Research.\n\n\n\nBenjamin Alexander Reitzes\nMelius Research LLC\n\nBen Reitzes here. Jensen, it's a geography-related question. you did a\ngreat job explaining some of the demand underlying factors here on the\nstrength. But U.S. was up about $5 billion or so sequentially. And I think\nthere is a concern about whether U.S. can pick up the slack if there's\nregulations towards other geographies. And I was just wondering, as we go\nthroughout the year, if this kind of surge in the U.S. continues and it's\ngoing to be -- whether that's okay. And if that underlies your growth rate,\nhow can you keep growing so fast with this mix shift towards the U.S.? Your\nguidance looks like China is probably up sequentially. So just wondering if\nyou could go through that dynamic and maybe Colette can weigh in.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nChina is approximately the same percentage as Q4 and as previous quarters.\nIt's about half of what it was before the export control. But it's\napproximately the same in percentage.\n\n\nWith respect to geographies, the takeaway is that AI is software. It's\nmodern software. It's incredible modern software, but it's modern software\nand AI has gone mainstream. AI is used in delivery services everywhere,\nshopping services everywhere. If you were to buy a quarter of milk and\ndelivered to you, AI was involved.\n\n\nAnd so almost everything that a consumer service provides, AI is at the\ncore of it. Every student will use AI as a tutor, health care services use\nAI, financial services use AI. No fintech company will not use AI. Every\nfintech company will. Climate tech company use AI. Mineral discovery now\nuses AI. The number of -- every higher education, every university uses AI\nand so I think it is fairly safe to say that AI has gone mainstream and\nthat it's being integrated into every application.\n\n\nAnd our hope is that, of course, the technology continues to advance safely\nand advance in a helpful way to society. And with that, I do believe that\nwe're at the beginning of this new transition. And what I mean by that in\nthe beginning is, remember, behind us has been decades of data centers and\ndecades of computers that have been built. And they've been built for a\nworld of hand coding and general purpose computing and CPUs and so on and\nso forth.\n\n\nAnd going forward, I think it's fairly safe to say that world is going to\nbe almost all software will be infused with AI. All software and all\nservices will be based on -- ultimately, based on machine learning, the\ndata flywheel is going to be part of improving software and services and\nthat the future computers will be accelerated, the future computers will be\nbased on AI. And we're really 2 years into that journey. And in modernizing\ncomputers that have taken decades to build out. And so I'm fairly sure that\nwe're in the beginning of this new era.\n\n\nAnd then lastly, no technology has ever had the opportunity to address a\nlarger part of the world's GDP than AI. No software tool ever has. And so\nthis is now a software tool that can address a much larger part of the\nworld's GDP more than any time in history. And so the way we think about\ngrowth and the way we think about whether something is big or small, has to\nbe in the context of that. And when you take a step back and look at it\nfrom that perspective, we're really just in the beginning.\n\n\nOperator\n\nYour next question comes from the line of Aaron Rakers with Wells Fargo.\nYour next question comes from Mark Lipacis with Evercore ISI.\n\n\nMark John Lipacis\nEvercore ISI Institutional Equities, Research Division\n\nThat's Mark Lipacis. I had a clarification and a question. Colette, up for\nthe clarification. Did you say that enterprise within the data center grew\n2x year-on-year for the January quarter? And if so, does that -- would that\nmake it the fast faster growing than the hyperscalers?\n\n\nAnd then, Jensen, for you, the question, hyperscalers are the biggest\npurchasers of your solutions, but they buy equipment for both internal and\nexternal workloads, external workflows being cloud services that enterprise\nis used. So the question is, can you give us a sense of how that\nhyperscaler spend splits between that external workload and internal? And\nas these new AI workflows and applications come up, would you expect\nenterprises to become a larger part of that consumption mix? And does that\nimpact how you develop your service, your ecosystem?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nSure. Thanks for the question regarding our Enterprise business. Yes, it\ngrew 2x and very similar to what we were seeing with our large CSPs. Keep\nin mind, these are both important areas to understand working with the CSPs\nand be working on large language models, can be working on inference and\ntheir own work. But keep in mind, that is also where the enterprises are\nsurfacing. Your enterprises are both with your CSPs as well as in terms of\nbuilding on their own. They're both correct, growing quite well.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThe CSPs are about half of our business. And the CSPs have internal\nconsumption and external consumption, as you say. And we're using -- of\ncourse, used for internal consumption. We work very closely with all of\nthem to optimize workloads that are internal to them because they have a\nlarge infrastructure of NVIDIA gear that they could take advantage of.\n\n\nAnd the fact that we could be used for AI on the one hand, video processing\non the other hand, data processing like Spark, we're fungible. And so the\nuseful life of our infrastructure is much better. If the useful life is\nmuch longer, then the TCO is also lower.\n\n\nAnd so -- the second part is how do we see the growth of enterprise or not\nCSPs, if you will, going forward? And the answer is, I believe, long term,\nit is by far larger and the reason for that is because if you look at the\ncomputer industry today and what is not served by the computer industry is\nlargely industrial.\n\n\nSo let me give you an example. When we say enterprise, and let's use the\ncar company as an example because they make both soft things and hard\nthings. And so in the case of a car company, the employees will be what we\ncall enterprise and agenetic AI and software planning systems and tools,\nand we have some really exciting things to share with you guys at GTC,\nbuild agentic systems are for employees to make employees more productive\nto design, to market, to plan, to operate their company. That's agenetic\nAI.\n\n\nOn the other hand, the cars that they manufacture also need AI. They need\nan AI system that trains the cars, treats this entire giant fleet of cars.\nAnd today, there's 1 billion cars on the road. Someday, there will be 1\nbillion cars on the road, and every single one of those cars will be\nrobotic cars, and they'll all be collecting data, and we'll be improving\nthem using an AI factory. Whereas they have a car factory today, in the\nfuture, they'll have a car factory and an AI factory.\n\n\nAnd then inside the car itself is a robotic system. And so as you can see,\nthere are 3 computers involved and there's the computer that helps the\npeople. There's the computer that build the AI for the machineries that\ncould be, of course, it could be a tractor, it could be a lawn mower. It\ncould be a humanoid robot that's being developed today. It could be a\nbuilding. It could be a warehouse.\n\n\nThese physical systems require new type of AI we call physical AI. They\ncan't just understand the meaning of words and languages, but they have to\nunderstand the meaning of the world, friction and inertia, object\npermanence and cause and effect. And all of those type of things that are\ncommon sense to you and I, but AIs have to go learn those physical effects.\nSo we call that physical AI.\n\n\nThat whole part of using agentic AI to revolutionize the way we work inside\ncompanies, that's just starting. This is now the beginning of the agentic\nAI era, and you hear a lot of people talking about it and we've got some\nreally great things going on. And then there's the physical AI after that,\nand then the robotic systems after that.\n\n\nAnd so these 3 computers are all brand new. And my sense is that long term,\nthis will be by far the larger of them all, which kind of makes sense. The\nworld's GDP is represented by either heavy industries or industrials and\ncompanies that are providing for those.\n\n\nOperator\n\nYour next question comes from the line of Aaron Rakers with Wells Fargo.\n\n\nAaron Christopher Rakers\nWells Fargo Securities, LLC, Research Division\n\nJensen, I'm curious as we now approach the 2-year anniversary of really the\nHopper inflection that you saw in 2023 in GenAI in general. And when we\nthink about the road map you have in front of us, how do you think about\nthe infrastructure that's been deployed from a replacement cycle\nperspective? And whether if it's GB300 or if it's the Rubin cycle where we\nstart to see maybe some refresh opportunity. I'm just curious to how you\nlook at that.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nI appreciate it. First of all, people are still using Voltas and Pascals\nand Amperes. And the reason for that is because there are always things\nthat -- because CUDA is so programmable you could use it -- one of the\nmajor use cases right now is data processing and data curation. You find a\ncircumstance that an AI model is not very good at. You present that\ncircumstance to a vision language model, let's say, it's a car. You present\nthat circumstance to a vision language model.\n\n\nThe vision language model actually looks at the circumstances and said,\n\"This is what happened and I wasn't very good at it.\" You then take that\nresponse -- the prompt and you go and prompt an AI model to go find in your\nwhole lake of data, other circumstances like that, whatever that\ncircumstance was. And then you use an AI to do domain randomization and\ngenerate a whole bunch of other examples.\n\n\nAnd then from that, you can go train the model. And so you could use the\nAmperes to go and do data processing and data curation and machine learning-\nbased search. And then you create the training data set, which you then\npresent to your Hopper systems for training. And so each one of these\narchitectures are completely -- they're all CUDA-compatible and so\neverything runs on everything. But if you have infrastructure in place,\nthen you can put the less intensive workloads onto the installed base of\nthe past. All of our GPUs are very well employed.\n\n\nOperator\n\nWe have time for one more question, and that question comes from Atif Malik\nwith Citi.\n\n\nAtif  Malik\nCitigroup Inc., Research Division\n\nI have a follow-up question on gross margins for Colette. Colette, I\nunderstand there are many moving parts the Blackwell yields, NVLink 72 and\nEthernet mix. And you kind of tipped to the earlier question, the April\nquarter is the bottom. But second half would have to ramp like 200 basis\npoints per quarter to get to the mid-70s range that you're giving for the\nend of the fiscal year. And we still don't know much about tariff impact to\nbroader semiconductor. So what kind of gives you the confidence in that\ntrajectory in the back half of this year?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nYes. Thanks for the question. Our gross margins, they're quite complex in\nterms of the material and everything that we put together in a Blackwell\nsystem, a tremendous amount of opportunity to look at a lot of different\npieces of that on how we can better improve our gross margins over time.\n\n\nRemember, we have many different configurations as well on Blackwell that\nwill be able to help us do that. So together, working after we get some of\nthese really strong ramping completed for our customers, we can begin a lot\nof that work. If not, we're going to probably start as soon as possible if\nwe can. If we can improve it in the short term, we will also do that.\n\n\nTariff, at this point, it's a little bit of an unknown it's an unknown\nuntil we understand further what the U.S. government's plan is, both its\ntiming, it's where and how much. So at this time, we are awaiting, but\nagain, we would, of course, always follow export controls and/or tariffs in\nthat manner.\n\n\nOperator\n\nLadies and gentlemen, that does conclude our question-and-answer session.\nI'm sorry.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThank you.\n\n\nColette M. Kress\nExecutive VP & CFO\n\nWe are going to open up to Jensen, and I believe he has a couple of things.\n\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nI just wanted to thank you. Thank you, Colette. The demand for Blackwell is\nextraordinary. AI is evolving beyond perception and generative AI into\nreasoning. With reasoning AI, we're observing another scaling law,\ninference time or test-time scaling.\n\n\nThe more computation, the more the model thinks the smarter the answer.\nModels like OpenAI, Grok 3, DeepSeek-R1 are reasoning models that apply\ninference time scaling. Reasoning models can consume 100x more compute.\nFuture reasoning models can consume much more compute. DeepSeek-R1 has\nignited global enthusiast. It's an excellent innovation. But even more\nimportantly, it has open source a world-class reasoning AI model. Nearly\nevery AI developer is applying R1 or chain of thought and reinforcement\nlearning techniques like R1 to scale their model's performance.\n\n\nWe now have 3 scaling laws, as I mentioned earlier, driving the demand for\nAI computing. The traditional scaling loss of AI remains intact. Foundation\nmodels are being enhanced with multimodality, and pretraining is still\ngrowing. But it's no longer enough. We have 2 additional scaling\ndimensions.\n\n\nPost-training scaling, where reinforcement learning, fine-tuning, model\ndistillation require orders of magnitude more compute than pretraining\nalone. Inference time scaling and reasoning where a single query and demand\n100x more compute. We designed Blackwell for this moment, a single platform\nthat can easily transition from pre-training, post-training and test-time\nscaling.\n\n\nBlackwell's FP4 transformer engine and NVLink 72 scale-up fabric and new\nsoftware technologies led Blackwell process reasoning AI models, 25x faster\nthan Hopper. Blackwell in all of this configuration is in full production.\nEach Grace Blackwell NVLink 72 rack is an engineering marvel. 1.5 million\ncomponents produced across 350 manufacturing sites by nearly 100,000\nfactory operators.\n\n\nAI is advancing at light speed. We're at the beginning of reasoning AI and\ninference time scaling. But we're just at the start of the age of AI,\nmultimodal AIs, enterprise AI, sovereign AI and physical AI are right\naround the corner. We will grow strongly in 2025.\n\n\nGoing forward, data centers will dedicate most of CapEx to accelerated\ncomputing and AI. Data centers will increasingly become AI factories and\nevery company will have them either renting or self-operated.\n\n\nI want to thank all of you for joining us today. Come join us at GTC in a\ncouple of weeks. We're going to be talking about Blackwell Ultra, Rubin and\nother new computing, networking, reasoning AI, physical AI products and a\nwhole bunch more. Thank you.\n\n\nOperatorThis concludes today's conference call. You may now disconnect.\nCopyright © 2025 by S&P Global Market Intelligence, a division of S&P\nGlobal Inc. All rights reserved.\n\n\nThese materials have been prepared solely for information purposes based\nupon information generally available to the public and from sources\nbelieved to be reliable. No content (including index data, ratings, credit-\nrelated analyses and data, research, model, software or other application\nor output therefrom) or any part thereof (Content) may be modified, reverse\nengineered, reproduced or distributed in any form by any means, or stored\nin a database or retrieval system, without the prior written permission of\nS&P Global Market Intelligence or its affiliates (collectively, S&P\nGlobal). The Content shall not be used for any unlawful or unauthorized\npurposes. S&P Global and any third-party providers, (collectively S&P\nGlobal Parties) do not guarantee the accuracy, completeness, timeliness or\navailability of the Content. S&P Global Parties are not responsible for any\nerrors or omissions, regardless of the cause, for the results obtained from\nthe use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P\nGLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS\nFOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR\nDEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE\nCONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no\nevent shall S&P Global Parties be liable to any party for any direct,\nindirect, incidental, exemplary, compensatory, punitive, special or\nconsequential damages, costs, expenses, legal fees, or losses (including,\nwithout limitation, lost income or lost profits and opportunity costs or\nlosses caused by negligence) in connection with any use of the Content even\nif advised of the possibility of such damages. S&P Global Market\nIntelligence's opinions, quotes and credit-related and other analyses are\nstatements of opinion as of the date they are expressed and not statements\nof fact or recommendations to purchase, hold, or sell any securities or to\nmake any investment decisions, and do not address the suitability of any\nsecurity. S&P Global Market Intelligence may provide index data. Direct\ninvestment in an index is not possible. Exposure to an asset class\nrepresented by an index is available through investable instruments based\non that index. S&P Global Market Intelligence assumes no obligation to\nupdate the Content following publication in any form or format. The Content\nshould not be relied on and is not a substitute for the skill, judgment and\nexperience of the user, its management, employees, advisors and/or clients\nwhen making investment and other business decisions. S&P Global Market\nIntelligence does not act as a fiduciary or an investment advisor except\nwhere registered as such. S&P Global keeps certain activities of its\ndivisions separate from each other in order to preserve the independence\nand objectivity of their respective activities. As a result, certain\ndivisions of S&P Global may have information that is not available to other\nS&P Global divisions. S&P Global has established policies and procedures to\nmaintain the confidentiality of certain nonpublic information received in\nconnection with each analytical process.\n\n\nS&P Global may receive compensation for its ratings and certain analyses,\nnormally from issuers or underwriters of securities or from obligors. S&P\nGlobal reserves the right to disseminate its opinions and analyses. S&P\nGlobal's public ratings and analyses are made available on its Web sites,\nwww.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and\nwww.globalcreditportal.com (subscription), and may be distributed through\nother means, including via S&P Global publications and third-party\nredistributors. Additional information about our ratings fees is available\nat www.standardandpoors.com/usratingsfees.\n© 2025 S&P Global Market Intelligence.\n\n",
  "presentation_text": "Operator\n\nGood afternoon. My name is Krista, and I will be your conference operator\ntoday. At this time, I would like to welcome everyone to NVIDIA's fourth\nquarter earnings call. [Operator Instructions]\n\n\nStewart Stecker, you may begin your conference.\n\n\nStewart  Stecker\nDirector of Investor Relations\n\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference\ncall for the fourth quarter of fiscal 2025. With me today from NVIDIA are\nJensen Huang, President and Chief Executive Officer; and Colette Kress,\nExecutive Vice President and Chief Financial Officer.\n\n\nI'd like to remind you that our call is being webcast live on NVIDIA's\nInvestor Relations website. The webcast will be available for replay until\nthe conference call to discuss our financial results for the first quarter\nof fiscal 2026. The content of today's call is NVIDIA's property. It can't\nbe reproduced or transcribed without prior written consent.\n\n\nDuring this call, we may make forward-looking statements based on current\nexpectations. These are subject to a number of significant risks and\nuncertainties, and our actual results may differ materially. For a\ndiscussion of factors that could affect our future financial results and\nbusiness, please refer to the disclosure in today's earnings release, our\nmost recent Forms 10-K and 10-Q and the reports that we may file on Form 8-\nK with the Securities and Exchange Commission. All our statements are made\nas of today, February 26, 2025, based on information currently available to\nus. Except as required by law, we assume no obligation to update any such\nstatements.\n\n\nDuring this call, we will discuss non-GAAP financial measures. You can find\na reconciliation of these non-GAAP financial measures to GAAP financial\nmeasures in our CFO commentary, which is posted on our website.\n\n\nWith that, let me turn the call over to Colette.\n\n\nColette M. Kress\nExecutive VP & CFO\n\nThanks, Stewart. Q4 was another record quarter. Revenue of $39.3 billion\nwas up 12% sequentially and up 78% year-on-year and above our outlook of\n$37.5 billion. For fiscal 2025 revenue was $130.5 billion, up 114% from the\nprior year.\n\n\nLet's start with Data Center. Data center revenue for fiscal 2025 was\n$115.2 billion, more than doubling from the prior year. In the fourth\nquarter, Data Center revenue of $35.6 billion was a record, up 16%\nsequentially and 93% year-on-year, as the Blackwell ramp commenced and\nHopper 200 continued sequential growth.\n\n\nIn Q4, Blackwell sales exceeded our expectations. We delivered $11 billion\nof Blackwell revenue to meet strong demand. This is the fastest product\nramp in our company's history, unprecedented in its speed and scale.\nBlackwell production is in full gear across multiple configurations, and we\nare increasing supply quickly expanding customer adoption.\n\n\nOur Q4 Data Center compute revenue jumped 18% sequentially and over 2x year-\non-year. Customers are racing to scale infrastructure to train the next\ngeneration of cutting-edge models and unlock the next level of AI\ncapabilities. With Blackwell, it will be common for these clusters to start\nwith 100,000 GPUs or more. Shipments have already started for multiple\ninfrastructures of this size.\n\n\nPost-training and model customization are fueling demand for NVIDIA\ninfrastructure and software as developers and enterprises leverage\ntechniques such as fine-tuning, reinforcement learning and distillation to\ntailor models for domain-specific use cases. Hugging Face alone hosts over\n90,000 derivatives created from the Llama foundation model.\n\n\nThe scale of post-training and model customization is massive and can\ncollectively demand orders of magnitude, more compute than pretraining. Our\ninference demand is accelerating, driven by test-time scaling and new\nreasoning models like OpenAI o3, DeepSeek-R1 and Grok 3. Long thinking\nreasoning AI can require 100x more compute per task compared to one-shot\ninferences. Blackwell was architected for reasoning AI inference. Blackwell\nsupercharges reasoning AI models with up to 25x higher token throughput and\n20x lower cost versus Hopper 100. It is revolutionary. Transformer engine\nis built for LLM and mixture of experts inference.\n\n\nAnd its NVLink domain delivers 14x the throughput of PCIe Gen 5, ensuring\nthe response time, throughput and cost efficiency needed to tackle the\ngrowing complexity of inference at scale. Companies across industries are\ntapping into NVIDIA's full stack inference platform to boost performance\nand slash costs. Now tripled inference throughput and cut costs by 66%\nusing NVIDIA TensorRT for its screenshot feature. Perplexity sees 435\nmillion monthly queries and reduced its inference costs 3x with NVIDIA\nTriton Inference Server and TensorRT-LLM.\n\n\nMicrosoft Bing achieved a 5x speed up at major TCO savings for Visual\nSearch across billions of images with NVIDIA TensorRT and acceleration\nlibraries. Blackwell has great demand for inference. Many of the early\nGB200 deployments are earmarked for inference, a first for a new\narchitecture. Blackwell addresses the entire AI market from pretraining,\npost-training to inference across cloud, to on-premise, to enterprise.\nCUDA's programmable architecture accelerates every AI model and over 4,400\napplications, ensuring large infrastructure investments against\nobsolescence in rapidly evolving market.\n\n\nOur performance and pace of innovation is unmatched. We're driven to a 200x\nreduction in inference costs in just the last 2 years. We delivered the\nlowest TCO and the highest ROI. And full stack optimizations for NVIDIA and\nour large ecosystem, including 5.9 million developers continuously improve\nour customers' economics.\n\n\nIn Q4, large CSPs represented about half of our data center revenue, and\nthese sales increased nearly 2x year-on-year. Large CSPs were some of the\nfirst to stand up Blackwell with Azure, GCP, AWS and OCI bringing GB200\nsystems to cloud regions around the world to meet surging customer demand\nfor AI. Regional cloud hosting NVIDIA GPUs increased as a percentage of\ndata center revenue, reflecting continued AI factory build-outs globally\nand rapidly rising demand for AI reasoning models and agents where we've\nlaunched a 100,000 GB200 cluster-based incidents with NVLink Switch and\nQuantum-2 InfiniBand.\n\n\nConsumer Internet revenue grew 3x year-on-year, driven by an expanding set\nof generative AI and deep learning use cases. These include recommender\nsystems, vision-language understanding, synthetic data generation, search\nand agentic AI. For example, xAI is adopting the GB200 to train and\ninference its next generation of Grok AI models. Meta's cutting-edge\nAndromeda advertising engine runs on NVIDIA's Grace Hopper Superchip\nserving vast quantities of ads across Instagram, Facebook applications.\nAndromeda harnesses Grace Hopper's fast interconnect and large memory to\nboost inference throughput by 3x, enhanced ad personalization and deliver\nmeaningful jumps in monetization and ROI.\n\n\nEnterprise revenue increased nearly 2x year on accelerating demand for\nmodel fine-tuning, RAG and agentic AI workflows and GPU accelerated data\nprocessing. We introduced NVIDIA Llama Nemotron model family NIMs to help\ndevelopers create and deploy AI agents across a range of applications,\nincluding customer support, fraud detection and product supply chain and\ninventory management.\n\n\nLeading AI agent platform providers, including SAP and ServiceNow are among\nthe first to use new models. Health care leaders, IQVIA, Illumina and Mayo\nClinic as well as ARC Institute are using NVIDIA AI to speed drug\ndiscovery, enhance genomic research and pioneer advanced health care\nservices with generative and agentic AI.\n\n\nAs AI expands beyond the digital world, NVIDIA infrastructure and software\nplatforms are increasingly being adopted to power robotics and physical AI\ndevelopment. One of the early and largest robotics applications in\nautonomous vehicles where virtually every AV company is developing on\nNVIDIA in the data center, the car or both.\n\n\nNVIDIA's automotive vertical revenue is expected to grow to approximately\n$5 billion this fiscal year. At CES, Hyundai Motor Group announced it is\nadopting NVIDIA technologies to accelerate AV and robotics development and\nsmart factory initiatives. Vision transformers, self-supervised learning,\nmultimodal sensor fusion and high fidelity simulation are driving\nbreakthroughs in AV development and will require 10x more compute. At CES,\nwe announced the NVIDIA Cosmos World Foundation Model Platform. Just as\nlanguage, foundation models have revolutionized language AI, Cosmos is a\nphysical AI to revolutionize robotics. Leading robotics and automotive\ncompanies, including ridesharing giant Uber, are among the first to adopt\nthe platform.\n\n\nFrom a geographic perspective, sequential growth in our Data Center revenue\nwas strongest in the U.S., driven by the initial ramp up Blackwell.\nCountries across the globe are building their AI ecosystems and demand for\ncompute infrastructure is surging. France's EUR 200 billion AI investment\nand the EU's EUR 200 billion InvestAI initiatives offer a glimpse into the\nbuild-out to set redefined global AI infrastructure in the coming years.\n\n\nNow as a percentage of total data center revenue, data center sales in\nChina remained well below levels seen on the onset of export controls.\nAbsent any change in regulations, we believe that China shipments will\nremain roughly at the current percentage. The market in China for data\ncenter solutions remains very competitive. We will continue to comply with\nexport controls while serving our customers.\n\n\nNetworking revenue declined 3% sequentially. Our networking attached to GPU\ncompute systems is robust at over 75%. We are transitioning from small\nNVLink 8 with InfiniBand to large NVLink 72 with Spectrum-X. Spectrum-X and\nNVLink Switch revenue increased and represents a major new growth vector.\nWe expect networking to return to growth in Q1.\n\n\nAI requires a new class of networking. NVIDIA offers NVLink Switch systems\nfor scale-up compute. For scale out, we offer Quantum InfiniBand for HPC\nsupercomputers and Spectrum-X for Ethernet environments. Spectrum-X\nenhances the Ethernet for AI computing and has been a huge success.\nMicrosoft Azure, OCI, CoreWeave and others are building large AI factories\nwith Spectrum-X.\n\n\nThe first Stargate data centers will use Spectrum-X. Yesterday, Cisco\nannounced integrating Spectrum-X into their networking portfolio to help\nenterprises build AI infrastructure. With its large enterprise footprint\nand global reach, Cisco will bring NVIDIA Ethernet to every industry.\n\n\nNow moving to Gaming and AI PCs. Gaming revenue of $2.5 billion decreased\n22% sequentially and 11% year-on-year. Full year revenue of $11.4 billion\nincreased 9% year-on-year, and demand remains strong throughout the\nholiday. However, Q4 shipments were impacted by supply constraints. We\nexpect strong sequential growth in Q1 as supply increases.\n\n\nThe new GeForce RTX 50 Series desktop and laptop GPUs are here. Built for\ngamers, creators and developers they fuse AI and graphics redefining visual\ncomputing, powered by the Blackwell architecture, fifth generation Tensor\nCores and fourth generation RT Cores and featuring up to 3,400 AI TOPS.\nThese GPUs deliver a 2x performance leap and new AI-driven rendering\nincluding Neural Shaders, digital human technologies, geometry and\nlighting.\n\n\nThe new DLSS 4 boost frame rates up to 8x with AI-driven frame generation,\nturning 1 rendered frame into 3. It also features the industry's first real-\ntime application of transformer models packing 2x more parameters and 4x\nthe compute for unprecedented visual fidelity.\n\n\nWe also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA\nMax-Q technology that extends battery life by up to an incredible 40%.\nThese laptops will be available starting in March from the world's top\nmanufacturers.\n\n\nMoving to our Professional Visualization business. Revenue of $511 million\nwas up 5% sequentially and 10% year-on-year. Full year revenue of $1.9\nbillion increased 21% year-on-year. Key industry verticals driving demand\ninclude automotive and health care.\n\n\nNVIDIA technologies and generative AI are reshaping design, engineering and\nsimulation workloads. Increasingly, these technologies are being leveraged\nin leading software platforms from ANSYS, Cadence and Siemens fueling\ndemand for NVIDIA RTX workstations.\n\n\nNow moving to Automotive. Revenue was a record $570 million, up 27%\nsequentially and up 103% year-on-year. Full year revenue of $1.7 billion\nincreased 55% year-on-year. Strong growth was driven by the continued ramp\nin autonomous vehicles, including cars and robotaxis. At CES, we announced\nToyota, the world's largest auto maker will build its next-generation\nvehicles on NVIDIA Orin running the safety certified NVIDIA DriveOS. We\nannounced Aurora and Continental will deploy driverless trucks at scale\npowered by NVIDIA DRIVE Thor.\n\n\nFinally, our end-to-end autonomous vehicle platform NVIDIA DRIVE Hyperion\nhas passed industry safety assessments like TÜV SÜD and TÜV Rheinland, 2 of\nthe industry's foremost authorities for automotive-grade safety and\ncybersecurity. NVIDIA is the first AV platform to receive a comprehensive\nset of third-party assessments.\n\n\nOkay. Moving to the rest of the P&L. GAAP gross margin was 73% and non-GAAP\ngross margin was 73.5%, down sequentially as expected with our first\ndeliveries of the Blackwell architecture. As discussed last quarter,\nBlackwell is a customizable AI infrastructure with several different types\nof NVIDIA build chips, multiple networking options and for air and liquid-\ncooled data center.\n\n\nWe exceeded our expectations in Q4 in ramping Blackwell, increasing system\navailability, providing several configurations to our customers. As\nBlackwell ramps, we expect gross margins to be in the low 70s.\n\n\nInitially, we are focused on expediting the manufacturing of Blackwell\nsystems to meet strong customer demand as they race to build out Blackwell\ninfrastructure. When fully ramped, we have many opportunities to improve\nthe cost and gross margin will improve and return to the mid-70s, late this\nfiscal year.\n\n\nSequentially, GAAP operating expenses were up 9% and non-GAAP operating\nexpenses were 11%, reflecting higher engineering development costs and\nhigher compute and infrastructure costs for new product introductions. In\nQ4, we returned $8.1 billion to shareholders in the form of share\nrepurchases and cash dividends.\n\n\nLet me turn to the outlook in the first quarter. Total revenue is expected\nto be $43 billion, plus or minus 2%. Continuing with its strong demand, we\nexpect a significant ramp of Blackwell in Q1. We expect sequential growth\nin both Data Center and Gaming. Within Data Center, we expect sequential\ngrowth from both compute and networking.\n\n\nGAAP and non-GAAP gross margins are expected to be 70.6% and 71%,\nrespectively, plus or minus 50 basis points. GAAP and non-GAAP operating\nexpenses are expected to be approximately $5.2 billion and $3.6 billion,\nrespectively. We expect full year fiscal year '26 operating expenses to\ngrow to be in the mid-30s.\n\n\nGAAP and non-GAAP other income and expenses are expected to be an income of\napproximately $400 million, excluding gains and losses from nonmarketable\nand publicly held equity securities. GAAP and non-GAAP tax rates are\nexpected to be 17%, plus or minus 1%, excluding any discrete items. Further\nfinancial details are included in the CFO commentary and other information\navailable on our IR website, including a new financial information AI\nagent.\n\n\nIn closing, let me highlight upcoming events for the financial community.\nWe will be at the TD Cowen Health Care Conference in Boston on March 3 and\nat the Morgan Stanley Technology, Media and Telecom Conference in San\nFrancisco on March 5. Please join us for our Annual GTC Conference starting\nMonday, March 17 in San Jose, California. Jensen will deliver a news-packed\nkeynote on March 18, and we will host a Q&A session for our financial\nanalysts the next day, March 19. We look forward to seeing you at these\nevents. Our earnings call to discuss the results for our first quarter of\nfiscal 2026 is scheduled for May 28, 2025.\nWe are going to open up the call, operator, to questions. If you could\nstart that, that would be great.",
  "qa_text": "Operator\n\n[Operator Instructions] And your first question comes from C.J. Muse with\nCantor Fitzgerald.\n\n\nChristopher James Muse\nCantor Fitzgerald & Co., Research Division\n\nI guess for me, Jensen, as TestCon compute and reinforcement learning shows\nsuch promise, we're clearly seeing an increasing blurring of the lines\nbetween training and inference, what does this mean for the potential\nfuture of potentially inference dedicated clusters? And how do you think\nabout the overall impact to NVIDIA and your customers?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes, I appreciate that C.J. There are now multiple scaling loss. There's\nthe pre-training scaling law, and that's going to continue to scale because\nwe have multimodality, we have data that came from reasoning that are now\nused to do pretraining.\n\n\nAnd then the second is post-training scaling law, using reinforcement\nlearning human feedback, reinforcement learning AI feedback, reinforcement\nlearning, verifiable rewards. The amount of computation you use for post\ntraining is actually higher than pretraining. And it's kind of sensible in\nthe sense that you could, while you're using reinforcement learning,\ngenerate an enormous amount of synthetic data or synthetically generated\ntokens. AI models are basically generating tokens to train AI models. And\nthat's post-training.\n\n\nAnd the third part, this is the part that you mentioned is test-time\ncompute or reasoning, long thinking, inference scaling. They're all\nbasically the same ideas. And there you have a chain of thought, you've\nsearch. The amount of tokens generated, the amount of inference compute\nneeded is already 100x more than the one-shot examples and the one-shot\ncapabilities of large language models in the beginning. And that's just the\nbeginning. This is just the beginning.\n\n\nThe idea that the next generation could have thousands times and even\nhopefully, extremely thoughtful and simulation-based and search-based\nmodels that could be hundreds of thousands, millions of times more compute\nthan today is in our future.\n\n\nAnd so the question is how do you design such an architecture? Some of it --\n some of the models are auto regressive. Some of the models are diffusion-\nbased. Some of it -- some of the times you want your data center to have\ndisaggregated inference. Sometimes it is compacted.\n\n\nAnd so it's hard to figure out what is the best configuration of a data\ncenter, which is the reason why NVIDIA's architecture is so popular. We run\nevery model. We are great at training. The vast majority of our compute\ntoday is actually inference and Blackwell takes all of that to a new level.\nWe designed Blackwell with the idea of reasoning models in mind. And when\nyou look at training, it's many times more performant.\n\n\nBut what's really amazing is for long thinking test-time scaling, reasoning\nAI models were tens of times faster, 25x higher throughput. And so\nBlackwell is going to be incredible across the board. And when you have a\ndata center that allows you to configure and use your data center based on\nare you doing more pretraining now, post training now or scaling out your\ninference, our architecture is fungible and easy to use in all of those\ndifferent ways. And so we're seeing, in fact, much, much more concentration\nof a unified architecture than ever before.\n\n\nOperator\n\nYour next question comes from the line of Joe Moore with JPMorgan (sic) [\nMorgan Stanley ].\n\n\nJoseph Lawrence Moore\nMorgan Stanley, Research Division\n\nMorgan Stanley, actually. I wonder if you could talk about GB200. At CES,\nyou sort of talked about the complexity of the rack level systems and the\nchallenges you have. And then as you said in the prepared remarks, we've\nseen a lot of general availability. Where are you in terms of that ramp?\n\n\nAre there still bottlenecks to consider at a systems level above and beyond\nthe chip level? And just have you maintained your enthusiasm for the NVL72\nplatforms?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, I'm more enthusiastic today than I was at CES. And the reason for\nthat is because we've shipped a lot more since CES. We have some 350 plants\nmanufacturing the 1.5 million components that go into each one of the\nBlackwell racks, Grace Blackwell racks.\n\n\nYes, it's extremely complicated. And we successfully and incredibly ramped\nup Grace Blackwell, delivering some $11 billion of revenues last quarter.\nWe're going to have to continue to scale as demand is quite high, and\ncustomers are anxious and impatient to get their Blackwell systems.\n\n\nYou've probably seen on the web, a fair number of celebrations about Grace\nBlackwell systems coming online and we have them, of course. We have a\nfairly large installation of Grace Blackwells for our own engineering and\nour own design teams and software teams.\n\n\nCoreWeave has now been quite public about the successful bring up of\ntheirs. Microsoft has. Of course, OpenAI has. And you're starting to see\nmany come online. And so I think the answer to your question is nothing is\neasy about what we're doing, but we're doing great, and all of our partners\nare doing great.\n\n\nOperator\n\nYour next question comes from the line of Vivek Arya with Bank of America\nSecurities.\n\n\nVivek  Arya\nBofA Securities, Research Division\n\nColette, if you wouldn't mind confirming if Q1 is the bottom for gross\nmargins? And then Jensen, my question is for you. What is on your dashboard\nto give you the confidence that the strong demand can sustain into next\nyear? And has DeepSeek and whatever innovations they came up with, has that\nchanged that view in any way?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nLet me first take the first part of the question there regarding the gross\nmargin. During our Blackwell ramp, our gross margins will be in the low\n70s. At this point, we are focusing on expediting our manufacturing,\nexpediting our manufacturing to make sure that we can provide to customers\nas soon as possible. Our Blackwell is fully ramped. And once it does -- I'm\nsorry, once our Blackwell fully ramps, we can improve our cost and our\ngross margin. So we expect to probably be in the mid-70s later this year.\n\n\nWalking through what you heard Jensen speak about the systems and their\ncomplexity, they are customizable in some cases. They've got multiple\nnetworking options. They have liquid cooled and water cooled. So we know\nthere is an opportunity for us to improve these gross margins going\nforward. But right now, we are going to focus on getting the manufacturing\ncomplete and to our customers as soon as possible.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWe know several things, Vivek. We have a fairly good line of sight of the\namount of capital investment that data centers are building out towards. We\nknow that going forward, the vast majority of software is going to be based\non machine learning. And so accelerated computing and generative AI,\nreasoning AI are going to be the type of architecture you want in your data\ncenter.\n\n\nWe have, of course, forecasts and plans from our top partners. And we also\nknow that there are many innovative, really exciting start-ups that are\nstill coming online as new opportunities for developing the next\nbreakthroughs in AI, whether it's agentic AIs, reasoning AI or physical\nAIs. The number of start-ups are still quite vibrant and each one of them\nneed a fair amount of computing infrastructure.\n\n\nAnd so I think the -- whether it's the near-term signals or the mid-term\nsignals, near-term signals, of course, are POs and forecasts and things\nlike that. Mid-term signals would be the level of infrastructure and CapEx\nscale-out compared to previous years. And then the long-term signals has to\ndo with the fact that we know fundamentally software has changed from hand\ncoding that runs on CPUs, to machine learning and AI-based software that\nruns on GPUs and accelerated computing systems.\n\n\nAnd so we have a fairly good sense that this is the future of software. And\nthen maybe as you roll it out, another way to think about that is we've\nreally only tapped consumer AI and search and some amount of consumer\ngenerative AI, advertising, recommenders, kind of the early days of\nsoftware. The next wave is coming, agentic AI for enterprise, physical AI\nfor robotics and sovereign AI as different regions build out their AI for\ntheir own ecosystems.\n\n\nAnd so each one of these are barely off the ground, and we can see them. We\ncan see them because, obviously, we're in the center of much of this\ndevelopment and we can see great activity happening in all these different\nplaces and these will happen. So near term, mid-term, long term.\n\n\nOperator\n\nYour next question comes from the line of Harlan Sur with JPMorgan.\n\n\nHarlan L.  Sur\nJPMorgan Chase & Co, Research Division\n\nYour next-generation Blackwell Ultra is set to launch in the second half of\nthis year, in line with the team's annual product cadence. Jensen, can you\nhelp us understand the demand dynamics for Ultra given that you'll still be\nramping the current generation Blackwell solutions? How do your customers\nand the supply chain also manage the simultaneous ramps of these 2\nproducts? And is the team still on track to execute Blackwell Ultra in the\nsecond half of this year?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes. Blackwell Ultra is second half. As you know, the first Blackwell was --\n we had a hiccup that probably cost us a couple of months. We're fully\nrecovered, of course. The team did an amazing job recovering and all of our\nsupply chain partners and just so many people helped us recover at the\nspeed of light. And so now we've successfully ramped production of\nBlackwell.\n\n\nBut that doesn't stop the next train. The next train is on an annual rhythm\nand Blackwell Ultra with new networking, new memories and of course, new\nprocessors, and all of that is coming online. We've have been working with\nall of our partners and customers, laying this out. They have all of the\nnecessary information, and we'll work with everybody to do the proper\ntransition.\n\n\nThis time between Blackwell and Blackwell Ultra, the system architecture is\nexactly the same. It's a lot harder going from Hopper to Blackwell because\nwe went from an NVLink 8 system to a NVLink 72-based system. So the\nchassis, the architecture of the system, the hardware, the power delivery,\nall of that had to change. This was quite a challenging transition.\n\n\nBut the next transition will slot right in. Blackwell Ultra will slot right\nin. We've also already revealed and been working very closely with all of\nour partners on the click after that. And the click after that is called\nVera Rubin and all of our partners are getting up to speed on the\ntransition of that and so preparing for that transition. And again, we're\ngoing to provide a big, huge step-up. And so come to GTC, and I'll talk to\nyou about Blackwell Ultra, Vera Rubin and then show you what's the one\nclick after that. Really exciting new products, so to come to GTC, please.\n\n\nOperator\n\nYour next question comes from the line of Timothy Arcuri with UBS.\n\n\nTimothy Michael Arcuri\nUBS Investment Bank, Research Division\n\nJensen, we heard a lot about custom ASICs. Can you kind of speak to the\nbalance between custom ASIC and merchant GPU? We hear about some of these\nheterogeneous superclusters to use both GPU and ASIC. Is that something\ncustomers are planning on building? Or will these infrastructures remain\nfairly distinct?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, we build very different things than ASICs, in some ways, completely\ndifferent in some areas we intercept. We're different in several ways. One,\nNVIDIA'S architecture is general whether you're -- you've optimized for\nauto regressive models or diffusion-based models or vision-based models or\nmultimodal models or text models. We're great in all of it.\n\n\nWe're great at all of it because our software stack is so -- our\narchitecture is flexible, our software stack ecosystem is so rich that\nwe're the initial target of most exciting innovations and algorithms. And\nso by definition, we're much, much more general than narrow. We're also\nreally good from the end-to-end from data processing, the curation of the\ntraining data, to the training of the data, of course, to reinforcement\nlearning used in post training, all the way to inference with test-time\nscaling.\n\n\nSo we're general, we're end-to-end, and we're everywhere. And because we're\nnot in just one cloud, we're in every cloud, we could be on-prem. We could\nbe in a robot. Our architecture is much more accessible and a great target\n-- initial target for anybody who's starting up a new company. And so we're\neverywhere.\n\n\nAnd the third thing I would say is that our performance and our rhythm is\nso incredibly fast. Remember that these data centers are always fixed in\nsize. They're fixed in size or they're fixed in power. And if our\nperformance per watt is anywhere from 2x to 4x to 8x, which is not unusual,\nit translates directly to revenues. And so if you have a 100-megawatt data\ncenter, if the performance or the throughput in that 100-megawatt or the\ngigawatt data center is 4x or 8x higher, your revenues for that gigawatt\ndata center is 8x higher.\n\n\nAnd the reason that is so different than data centers of the past is\nbecause AI factories are directly monetizable through its tokens generated.\nAnd so the token throughput of our architecture being so incredibly fast is\njust incredibly valuable to all of the companies that are building these\nthings for revenue generation reasons and capturing the fast ROI. And so I\nthink the third reason is performance.\n\n\nAnd then the last thing that I would say is the software stack is\nincredibly hard. Building an ASIC is no different than what we do. We build\na new architecture. And the ecosystem that sits on top of our architecture\nis 10x more complex today than it was 2 years ago. And that's fairly\nobvious because the amount of software that the world is building on top of\narchitecture is growing exponentially and AI is advancing very quickly.\n\n\nSo bringing that whole ecosystem on top of multiple chips is hard. And so I\nwould say that those 4 reasons. And then finally, I will say this, just\nbecause the chip is designed doesn't mean it gets deployed. And you've seen\nthis over and over again. There are a lot of chips that gets built, but\nwhen the time comes, a business decision has to be made, and that business\ndecision is about deploying a new engine, a new processor into a limited AI\nfactory in size, in power and in time.\n\n\nAnd our technology is not only more advanced, more performant, it has much,\nmuch better software capability and very importantly, our ability to deploy\nis lightning fast. And so these things are enough for the faint of heart,\nas everybody knows now. And so there's a lot of different reasons why we do\nwell, why we win.\n\n\nOperator\n\nYour next question comes from the line of Ben Reitzes with Melius Research.\n\n\n\nBenjamin Alexander Reitzes\nMelius Research LLC\n\nBen Reitzes here. Jensen, it's a geography-related question. you did a\ngreat job explaining some of the demand underlying factors here on the\nstrength. But U.S. was up about $5 billion or so sequentially. And I think\nthere is a concern about whether U.S. can pick up the slack if there's\nregulations towards other geographies. And I was just wondering, as we go\nthroughout the year, if this kind of surge in the U.S. continues and it's\ngoing to be -- whether that's okay. And if that underlies your growth rate,\nhow can you keep growing so fast with this mix shift towards the U.S.? Your\nguidance looks like China is probably up sequentially. So just wondering if\nyou could go through that dynamic and maybe Colette can weigh in.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nChina is approximately the same percentage as Q4 and as previous quarters.\nIt's about half of what it was before the export control. But it's\napproximately the same in percentage.\n\n\nWith respect to geographies, the takeaway is that AI is software. It's\nmodern software. It's incredible modern software, but it's modern software\nand AI has gone mainstream. AI is used in delivery services everywhere,\nshopping services everywhere. If you were to buy a quarter of milk and\ndelivered to you, AI was involved.\n\n\nAnd so almost everything that a consumer service provides, AI is at the\ncore of it. Every student will use AI as a tutor, health care services use\nAI, financial services use AI. No fintech company will not use AI. Every\nfintech company will. Climate tech company use AI. Mineral discovery now\nuses AI. The number of -- every higher education, every university uses AI\nand so I think it is fairly safe to say that AI has gone mainstream and\nthat it's being integrated into every application.\n\n\nAnd our hope is that, of course, the technology continues to advance safely\nand advance in a helpful way to society. And with that, I do believe that\nwe're at the beginning of this new transition. And what I mean by that in\nthe beginning is, remember, behind us has been decades of data centers and\ndecades of computers that have been built. And they've been built for a\nworld of hand coding and general purpose computing and CPUs and so on and\nso forth.\n\n\nAnd going forward, I think it's fairly safe to say that world is going to\nbe almost all software will be infused with AI. All software and all\nservices will be based on -- ultimately, based on machine learning, the\ndata flywheel is going to be part of improving software and services and\nthat the future computers will be accelerated, the future computers will be\nbased on AI. And we're really 2 years into that journey. And in modernizing\ncomputers that have taken decades to build out. And so I'm fairly sure that\nwe're in the beginning of this new era.\n\n\nAnd then lastly, no technology has ever had the opportunity to address a\nlarger part of the world's GDP than AI. No software tool ever has. And so\nthis is now a software tool that can address a much larger part of the\nworld's GDP more than any time in history. And so the way we think about\ngrowth and the way we think about whether something is big or small, has to\nbe in the context of that. And when you take a step back and look at it\nfrom that perspective, we're really just in the beginning.\n\n\nOperator\n\nYour next question comes from the line of Aaron Rakers with Wells Fargo.\nYour next question comes from Mark Lipacis with Evercore ISI.\n\n\nMark John Lipacis\nEvercore ISI Institutional Equities, Research Division\n\nThat's Mark Lipacis. I had a clarification and a question. Colette, up for\nthe clarification. Did you say that enterprise within the data center grew\n2x year-on-year for the January quarter? And if so, does that -- would that\nmake it the fast faster growing than the hyperscalers?\n\n\nAnd then, Jensen, for you, the question, hyperscalers are the biggest\npurchasers of your solutions, but they buy equipment for both internal and\nexternal workloads, external workflows being cloud services that enterprise\nis used. So the question is, can you give us a sense of how that\nhyperscaler spend splits between that external workload and internal? And\nas these new AI workflows and applications come up, would you expect\nenterprises to become a larger part of that consumption mix? And does that\nimpact how you develop your service, your ecosystem?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nSure. Thanks for the question regarding our Enterprise business. Yes, it\ngrew 2x and very similar to what we were seeing with our large CSPs. Keep\nin mind, these are both important areas to understand working with the CSPs\nand be working on large language models, can be working on inference and\ntheir own work. But keep in mind, that is also where the enterprises are\nsurfacing. Your enterprises are both with your CSPs as well as in terms of\nbuilding on their own. They're both correct, growing quite well.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThe CSPs are about half of our business. And the CSPs have internal\nconsumption and external consumption, as you say. And we're using -- of\ncourse, used for internal consumption. We work very closely with all of\nthem to optimize workloads that are internal to them because they have a\nlarge infrastructure of NVIDIA gear that they could take advantage of.\n\n\nAnd the fact that we could be used for AI on the one hand, video processing\non the other hand, data processing like Spark, we're fungible. And so the\nuseful life of our infrastructure is much better. If the useful life is\nmuch longer, then the TCO is also lower.\n\n\nAnd so -- the second part is how do we see the growth of enterprise or not\nCSPs, if you will, going forward? And the answer is, I believe, long term,\nit is by far larger and the reason for that is because if you look at the\ncomputer industry today and what is not served by the computer industry is\nlargely industrial.\n\n\nSo let me give you an example. When we say enterprise, and let's use the\ncar company as an example because they make both soft things and hard\nthings. And so in the case of a car company, the employees will be what we\ncall enterprise and agenetic AI and software planning systems and tools,\nand we have some really exciting things to share with you guys at GTC,\nbuild agentic systems are for employees to make employees more productive\nto design, to market, to plan, to operate their company. That's agenetic\nAI.\n\n\nOn the other hand, the cars that they manufacture also need AI. They need\nan AI system that trains the cars, treats this entire giant fleet of cars.\nAnd today, there's 1 billion cars on the road. Someday, there will be 1\nbillion cars on the road, and every single one of those cars will be\nrobotic cars, and they'll all be collecting data, and we'll be improving\nthem using an AI factory. Whereas they have a car factory today, in the\nfuture, they'll have a car factory and an AI factory.\n\n\nAnd then inside the car itself is a robotic system. And so as you can see,\nthere are 3 computers involved and there's the computer that helps the\npeople. There's the computer that build the AI for the machineries that\ncould be, of course, it could be a tractor, it could be a lawn mower. It\ncould be a humanoid robot that's being developed today. It could be a\nbuilding. It could be a warehouse.\n\n\nThese physical systems require new type of AI we call physical AI. They\ncan't just understand the meaning of words and languages, but they have to\nunderstand the meaning of the world, friction and inertia, object\npermanence and cause and effect. And all of those type of things that are\ncommon sense to you and I, but AIs have to go learn those physical effects.\nSo we call that physical AI.\n\n\nThat whole part of using agentic AI to revolutionize the way we work inside\ncompanies, that's just starting. This is now the beginning of the agentic\nAI era, and you hear a lot of people talking about it and we've got some\nreally great things going on. And then there's the physical AI after that,\nand then the robotic systems after that.\n\n\nAnd so these 3 computers are all brand new. And my sense is that long term,\nthis will be by far the larger of them all, which kind of makes sense. The\nworld's GDP is represented by either heavy industries or industrials and\ncompanies that are providing for those.\n\n\nOperator\n\nYour next question comes from the line of Aaron Rakers with Wells Fargo.\n\n\nAaron Christopher Rakers\nWells Fargo Securities, LLC, Research Division\n\nJensen, I'm curious as we now approach the 2-year anniversary of really the\nHopper inflection that you saw in 2023 in GenAI in general. And when we\nthink about the road map you have in front of us, how do you think about\nthe infrastructure that's been deployed from a replacement cycle\nperspective? And whether if it's GB300 or if it's the Rubin cycle where we\nstart to see maybe some refresh opportunity. I'm just curious to how you\nlook at that.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nI appreciate it. First of all, people are still using Voltas and Pascals\nand Amperes. And the reason for that is because there are always things\nthat -- because CUDA is so programmable you could use it -- one of the\nmajor use cases right now is data processing and data curation. You find a\ncircumstance that an AI model is not very good at. You present that\ncircumstance to a vision language model, let's say, it's a car. You present\nthat circumstance to a vision language model.\n\n\nThe vision language model actually looks at the circumstances and said,\n\"This is what happened and I wasn't very good at it.\" You then take that\nresponse -- the prompt and you go and prompt an AI model to go find in your\nwhole lake of data, other circumstances like that, whatever that\ncircumstance was. And then you use an AI to do domain randomization and\ngenerate a whole bunch of other examples.\n\n\nAnd then from that, you can go train the model. And so you could use the\nAmperes to go and do data processing and data curation and machine learning-\nbased search. And then you create the training data set, which you then\npresent to your Hopper systems for training. And so each one of these\narchitectures are completely -- they're all CUDA-compatible and so\neverything runs on everything. But if you have infrastructure in place,\nthen you can put the less intensive workloads onto the installed base of\nthe past. All of our GPUs are very well employed.\n\n\nOperator\n\nWe have time for one more question, and that question comes from Atif Malik\nwith Citi.\n\n\nAtif  Malik\nCitigroup Inc., Research Division\n\nI have a follow-up question on gross margins for Colette. Colette, I\nunderstand there are many moving parts the Blackwell yields, NVLink 72 and\nEthernet mix. And you kind of tipped to the earlier question, the April\nquarter is the bottom. But second half would have to ramp like 200 basis\npoints per quarter to get to the mid-70s range that you're giving for the\nend of the fiscal year. And we still don't know much about tariff impact to\nbroader semiconductor. So what kind of gives you the confidence in that\ntrajectory in the back half of this year?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nYes. Thanks for the question. Our gross margins, they're quite complex in\nterms of the material and everything that we put together in a Blackwell\nsystem, a tremendous amount of opportunity to look at a lot of different\npieces of that on how we can better improve our gross margins over time.\n\n\nRemember, we have many different configurations as well on Blackwell that\nwill be able to help us do that. So together, working after we get some of\nthese really strong ramping completed for our customers, we can begin a lot\nof that work. If not, we're going to probably start as soon as possible if\nwe can. If we can improve it in the short term, we will also do that.\n\n\nTariff, at this point, it's a little bit of an unknown it's an unknown\nuntil we understand further what the U.S. government's plan is, both its\ntiming, it's where and how much. So at this time, we are awaiting, but\nagain, we would, of course, always follow export controls and/or tariffs in\nthat manner.\n\n\nOperator\n\nLadies and gentlemen, that does conclude our question-and-answer session.\nI'm sorry.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThank you.\n\n\nColette M. Kress\nExecutive VP & CFO\n\nWe are going to open up to Jensen, and I believe he has a couple of things.\n\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nI just wanted to thank you. Thank you, Colette. The demand for Blackwell is\nextraordinary. AI is evolving beyond perception and generative AI into\nreasoning. With reasoning AI, we're observing another scaling law,\ninference time or test-time scaling.\n\n\nThe more computation, the more the model thinks the smarter the answer.\nModels like OpenAI, Grok 3, DeepSeek-R1 are reasoning models that apply\ninference time scaling. Reasoning models can consume 100x more compute.\nFuture reasoning models can consume much more compute. DeepSeek-R1 has\nignited global enthusiast. It's an excellent innovation. But even more\nimportantly, it has open source a world-class reasoning AI model. Nearly\nevery AI developer is applying R1 or chain of thought and reinforcement\nlearning techniques like R1 to scale their model's performance.\n\n\nWe now have 3 scaling laws, as I mentioned earlier, driving the demand for\nAI computing. The traditional scaling loss of AI remains intact. Foundation\nmodels are being enhanced with multimodality, and pretraining is still\ngrowing. But it's no longer enough. We have 2 additional scaling\ndimensions.\n\n\nPost-training scaling, where reinforcement learning, fine-tuning, model\ndistillation require orders of magnitude more compute than pretraining\nalone. Inference time scaling and reasoning where a single query and demand\n100x more compute. We designed Blackwell for this moment, a single platform\nthat can easily transition from pre-training, post-training and test-time\nscaling.\n\n\nBlackwell's FP4 transformer engine and NVLink 72 scale-up fabric and new\nsoftware technologies led Blackwell process reasoning AI models, 25x faster\nthan Hopper. Blackwell in all of this configuration is in full production.\nEach Grace Blackwell NVLink 72 rack is an engineering marvel. 1.5 million\ncomponents produced across 350 manufacturing sites by nearly 100,000\nfactory operators.\n\n\nAI is advancing at light speed. We're at the beginning of reasoning AI and\ninference time scaling. But we're just at the start of the age of AI,\nmultimodal AIs, enterprise AI, sovereign AI and physical AI are right\naround the corner. We will grow strongly in 2025.\n\n\nGoing forward, data centers will dedicate most of CapEx to accelerated\ncomputing and AI. Data centers will increasingly become AI factories and\nevery company will have them either renting or self-operated.\n\n\nI want to thank all of you for joining us today. Come join us at GTC in a\ncouple of weeks. We're going to be talking about Blackwell Ultra, Rubin and\nother new computing, networking, reasoning AI, physical AI products and a\nwhole bunch more. Thank you.\n\n\nOperatorThis concludes today's conference call. You may now disconnect.\nCopyright © 2025 by S&P Global Market Intelligence, a division of S&P\nGlobal Inc. All rights reserved.\n\n\nThese materials have been prepared solely for information purposes based\nupon information generally available to the public and from sources\nbelieved to be reliable. No content (including index data, ratings, credit-\nrelated analyses and data, research, model, software or other application\nor output therefrom) or any part thereof (Content) may be modified, reverse\nengineered, reproduced or distributed in any form by any means, or stored\nin a database or retrieval system, without the prior written permission of\nS&P Global Market Intelligence or its affiliates (collectively, S&P\nGlobal). The Content shall not be used for any unlawful or unauthorized\npurposes. S&P Global and any third-party providers, (collectively S&P\nGlobal Parties) do not guarantee the accuracy, completeness, timeliness or\navailability of the Content. S&P Global Parties are not responsible for any\nerrors or omissions, regardless of the cause, for the results obtained from\nthe use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P\nGLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS\nFOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR\nDEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE\nCONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no\nevent shall S&P Global Parties be liable to any party for any direct,\nindirect, incidental, exemplary, compensatory, punitive, special or\nconsequential damages, costs, expenses, legal fees, or losses (including,\nwithout limitation, lost income or lost profits and opportunity costs or\nlosses caused by negligence) in connection with any use of the Content even\nif advised of the possibility of such damages. S&P Global Market\nIntelligence's opinions, quotes and credit-related and other analyses are\nstatements of opinion as of the date they are expressed and not statements\nof fact or recommendations to purchase, hold, or sell any securities or to\nmake any investment decisions, and do not address the suitability of any\nsecurity. S&P Global Market Intelligence may provide index data. Direct\ninvestment in an index is not possible. Exposure to an asset class\nrepresented by an index is available through investable instruments based\non that index. S&P Global Market Intelligence assumes no obligation to\nupdate the Content following publication in any form or format. The Content\nshould not be relied on and is not a substitute for the skill, judgment and\nexperience of the user, its management, employees, advisors and/or clients\nwhen making investment and other business decisions. S&P Global Market\nIntelligence does not act as a fiduciary or an investment advisor except\nwhere registered as such. S&P Global keeps certain activities of its\ndivisions separate from each other in order to preserve the independence\nand objectivity of their respective activities. As a result, certain\ndivisions of S&P Global may have information that is not available to other\nS&P Global divisions. S&P Global has established policies and procedures to\nmaintain the confidentiality of certain nonpublic information received in\nconnection with each analytical process.\n\n\nS&P Global may receive compensation for its ratings and certain analyses,\nnormally from issuers or underwriters of securities or from obligors. S&P\nGlobal reserves the right to disseminate its opinions and analyses. S&P\nGlobal's public ratings and analyses are made available on its Web sites,\nwww.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and\nwww.globalcreditportal.com (subscription), and may be distributed through\nother means, including via S&P Global publications and third-party\nredistributors. Additional information about our ratings fees is available\nat www.standardandpoors.com/usratingsfees.\n© 2025 S&P Global Market Intelligence.",
  "has_qa": 1,
  "speaker_turns": [
    {
      "speaker": "Unknown",
      "role": "",
      "text": "NVIDIA Corporation NasdaqGS:NVDA FQ4 2025 Earnings Call Transcripts Wednesday, February 26, 2025 10:00 PM GMT S&P Global Market Intelligence Estimates Presentation"
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Good afternoon. My name is Krista, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's fourth quarter earnings call. [Operator Instructions] Stewart Stecker, you may begin your conference."
    },
    {
      "speaker": "Stewart  Stecker",
      "role": "Director of Investor Relations",
      "text": "Director of Investor Relations Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the fourth quarter of fiscal 2025. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8- K with the Securities and Exchange Commission. All our statements are made as of today, February 26, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette."
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO Thanks, Stewart. Q4 was another record quarter. Revenue of $39.3 billion was up 12% sequentially and up 78% year-on-year and above our outlook of $37.5 billion. For fiscal 2025 revenue was $130.5 billion, up 114% from the prior year. Let's start with Data Center. Data center revenue for fiscal 2025 was $115.2 billion, more than doubling from the prior year. In the fourth quarter, Data Center revenue of $35.6 billion was a record, up 16% sequentially and 93% year-on-year, as the Blackwell ramp commenced and Hopper 200 continued sequential growth. In Q4, Blackwell sales exceeded our expectations. We delivered $11 billion of Blackwell revenue to meet strong demand. This is the fastest product ramp in our company's history, unprecedented in its speed and scale. Blackwell production is in full gear across multiple configurations, and we are increasing supply quickly expanding customer adoption. Our Q4 Data Center compute revenue jumped 18% sequentially and over 2x year- on-year. Customers are racing to scale infrastructure to train the next generation of cutting-edge models and unlock the next level of AI capabilities. With Blackwell, it will be common for these clusters to start with 100,000 GPUs or more. Shipments have already started for multiple infrastructures of this size. Post-training and model customization are fueling demand for NVIDIA infrastructure and software as developers and enterprises leverage techniques such as fine-tuning, reinforcement learning and distillation to tailor models for domain-specific use cases. Hugging Face alone hosts over 90,000 derivatives created from the Llama foundation model. The scale of post-training and model customization is massive and can collectively demand orders of magnitude, more compute than pretraining. Our inference demand is accelerating, driven by test-time scaling and new reasoning models like OpenAI o3, DeepSeek-R1 and Grok 3. Long thinking reasoning AI can require 100x more compute per task compared to one-shot inferences. Blackwell was architected for reasoning AI inference. Blackwell supercharges reasoning AI models with up to 25x higher token throughput and 20x lower cost versus Hopper 100. It is revolutionary. Transformer engine is built for LLM and mixture of experts inference. And its NVLink domain delivers 14x the throughput of PCIe Gen 5, ensuring the response time, throughput and cost efficiency needed to tackle the growing complexity of inference at scale. Companies across industries are tapping into NVIDIA's full stack inference platform to boost performance and slash costs. Now tripled inference throughput and cut costs by 66% using NVIDIA TensorRT for its screenshot feature. Perplexity sees 435 million monthly queries and reduced its inference costs 3x with NVIDIA Triton Inference Server and TensorRT-LLM. Microsoft Bing achieved a 5x speed up at major TCO savings for Visual Search across billions of images with NVIDIA TensorRT and acceleration libraries. Blackwell has great demand for inference. Many of the early GB200 deployments are earmarked for inference, a first for a new architecture. Blackwell addresses the entire AI market from pretraining, post-training to inference across cloud, to on-premise, to enterprise. CUDA's programmable architecture accelerates every AI model and over 4,400 applications, ensuring large infrastructure investments against obsolescence in rapidly evolving market. Our performance and pace of innovation is unmatched. We're driven to a 200x reduction in inference costs in just the last 2 years. We delivered the lowest TCO and the highest ROI. And full stack optimizations for NVIDIA and our large ecosystem, including 5.9 million developers continuously improve our customers' economics. In Q4, large CSPs represented about half of our data center revenue, and these sales increased nearly 2x year-on-year. Large CSPs were some of the first to stand up Blackwell with Azure, GCP, AWS and OCI bringing GB200 systems to cloud regions around the world to meet surging customer demand for AI. Regional cloud hosting NVIDIA GPUs increased as a percentage of data center revenue, reflecting continued AI factory build-outs globally and rapidly rising demand for AI reasoning models and agents where we've launched a 100,000 GB200 cluster-based incidents with NVLink Switch and Quantum-2 InfiniBand. Consumer Internet revenue grew 3x year-on-year, driven by an expanding set of generative AI and deep learning use cases. These include recommender systems, vision-language understanding, synthetic data generation, search and agentic AI. For example, xAI is adopting the GB200 to train and inference its next generation of Grok AI models. Meta's cutting-edge Andromeda advertising engine runs on NVIDIA's Grace Hopper Superchip serving vast quantities of ads across Instagram, Facebook applications. Andromeda harnesses Grace Hopper's fast interconnect and large memory to boost inference throughput by 3x, enhanced ad personalization and deliver meaningful jumps in monetization and ROI. Enterprise revenue increased nearly 2x year on accelerating demand for model fine-tuning, RAG and agentic AI workflows and GPU accelerated data processing. We introduced NVIDIA Llama Nemotron model family NIMs to help developers create and deploy AI agents across a range of applications, including customer support, fraud detection and product supply chain and inventory management. Leading AI agent platform providers, including SAP and ServiceNow are among the first to use new models. Health care leaders, IQVIA, Illumina and Mayo Clinic as well as ARC Institute are using NVIDIA AI to speed drug discovery, enhance genomic research and pioneer advanced health care services with generative and agentic AI. As AI expands beyond the digital world, NVIDIA infrastructure and software platforms are increasingly being adopted to power robotics and physical AI development. One of the early and largest robotics applications in autonomous vehicles where virtually every AV company is developing on NVIDIA in the data center, the car or both. NVIDIA's automotive vertical revenue is expected to grow to approximately $5 billion this fiscal year. At CES, Hyundai Motor Group announced it is adopting NVIDIA technologies to accelerate AV and robotics development and smart factory initiatives. Vision transformers, self-supervised learning, multimodal sensor fusion and high fidelity simulation are driving breakthroughs in AV development and will require 10x more compute. At CES, we announced the NVIDIA Cosmos World Foundation Model Platform. Just as language, foundation models have revolutionized language AI, Cosmos is a physical AI to revolutionize robotics. Leading robotics and automotive companies, including ridesharing giant Uber, are among the first to adopt the platform. From a geographic perspective, sequential growth in our Data Center revenue was strongest in the U.S., driven by the initial ramp up Blackwell. Countries across the globe are building their AI ecosystems and demand for compute infrastructure is surging. France's EUR 200 billion AI investment and the EU's EUR 200 billion InvestAI initiatives offer a glimpse into the build-out to set redefined global AI infrastructure in the coming years. Now as a percentage of total data center revenue, data center sales in China remained well below levels seen on the onset of export controls. Absent any change in regulations, we believe that China shipments will remain roughly at the current percentage. The market in China for data center solutions remains very competitive. We will continue to comply with export controls while serving our customers. Networking revenue declined 3% sequentially. Our networking attached to GPU compute systems is robust at over 75%. We are transitioning from small NVLink 8 with InfiniBand to large NVLink 72 with Spectrum-X. Spectrum-X and NVLink Switch revenue increased and represents a major new growth vector. We expect networking to return to growth in Q1. AI requires a new class of networking. NVIDIA offers NVLink Switch systems for scale-up compute. For scale out, we offer Quantum InfiniBand for HPC supercomputers and Spectrum-X for Ethernet environments. Spectrum-X enhances the Ethernet for AI computing and has been a huge success. Microsoft Azure, OCI, CoreWeave and others are building large AI factories with Spectrum-X. The first Stargate data centers will use Spectrum-X. Yesterday, Cisco announced integrating Spectrum-X into their networking portfolio to help enterprises build AI infrastructure. With its large enterprise footprint and global reach, Cisco will bring NVIDIA Ethernet to every industry. Now moving to Gaming and AI PCs. Gaming revenue of $2.5 billion decreased 22% sequentially and 11% year-on-year. Full year revenue of $11.4 billion increased 9% year-on-year, and demand remains strong throughout the holiday. However, Q4 shipments were impacted by supply constraints. We expect strong sequential growth in Q1 as supply increases. The new GeForce RTX 50 Series desktop and laptop GPUs are here. Built for gamers, creators and developers they fuse AI and graphics redefining visual computing, powered by the Blackwell architecture, fifth generation Tensor Cores and fourth generation RT Cores and featuring up to 3,400 AI TOPS. These GPUs deliver a 2x performance leap and new AI-driven rendering including Neural Shaders, digital human technologies, geometry and lighting. The new DLSS 4 boost frame rates up to 8x with AI-driven frame generation, turning 1 rendered frame into 3. It also features the industry's first real- time application of transformer models packing 2x more parameters and 4x the compute for unprecedented visual fidelity. We also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA Max-Q technology that extends battery life by up to an incredible 40%. These laptops will be available starting in March from the world's top manufacturers. Moving to our Professional Visualization business. Revenue of $511 million was up 5% sequentially and 10% year-on-year. Full year revenue of $1.9 billion increased 21% year-on-year. Key industry verticals driving demand include automotive and health care. NVIDIA technologies and generative AI are reshaping design, engineering and simulation workloads. Increasingly, these technologies are being leveraged in leading software platforms from ANSYS, Cadence and Siemens fueling demand for NVIDIA RTX workstations. Now moving to Automotive. Revenue was a record $570 million, up 27% sequentially and up 103% year-on-year. Full year revenue of $1.7 billion increased 55% year-on-year. Strong growth was driven by the continued ramp in autonomous vehicles, including cars and robotaxis. At CES, we announced Toyota, the world's largest auto maker will build its next-generation vehicles on NVIDIA Orin running the safety certified NVIDIA DriveOS. We announced Aurora and Continental will deploy driverless trucks at scale powered by NVIDIA DRIVE Thor. Finally, our end-to-end autonomous vehicle platform NVIDIA DRIVE Hyperion has passed industry safety assessments like TÜV SÜD and TÜV Rheinland, 2 of the industry's foremost authorities for automotive-grade safety and cybersecurity. NVIDIA is the first AV platform to receive a comprehensive set of third-party assessments. Okay. Moving to the rest of the P&L. GAAP gross margin was 73% and non-GAAP gross margin was 73.5%, down sequentially as expected with our first deliveries of the Blackwell architecture. As discussed last quarter, Blackwell is a customizable AI infrastructure with several different types of NVIDIA build chips, multiple networking options and for air and liquid- cooled data center. We exceeded our expectations in Q4 in ramping Blackwell, increasing system availability, providing several configurations to our customers. As Blackwell ramps, we expect gross margins to be in the low 70s. Initially, we are focused on expediting the manufacturing of Blackwell systems to meet strong customer demand as they race to build out Blackwell infrastructure. When fully ramped, we have many opportunities to improve the cost and gross margin will improve and return to the mid-70s, late this fiscal year. Sequentially, GAAP operating expenses were up 9% and non-GAAP operating expenses were 11%, reflecting higher engineering development costs and higher compute and infrastructure costs for new product introductions. In Q4, we returned $8.1 billion to shareholders in the form of share repurchases and cash dividends. Let me turn to the outlook in the first quarter. Total revenue is expected to be $43 billion, plus or minus 2%. Continuing with its strong demand, we expect a significant ramp of Blackwell in Q1. We expect sequential growth in both Data Center and Gaming. Within Data Center, we expect sequential growth from both compute and networking. GAAP and non-GAAP gross margins are expected to be 70.6% and 71%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $5.2 billion and $3.6 billion, respectively. We expect full year fiscal year '26 operating expenses to grow to be in the mid-30s. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $400 million, excluding gains and losses from nonmarketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new financial information AI agent. In closing, let me highlight upcoming events for the financial community. We will be at the TD Cowen Health Care Conference in Boston on March 3 and at the Morgan Stanley Technology, Media and Telecom Conference in San Francisco on March 5. Please join us for our Annual GTC Conference starting Monday, March 17 in San Jose, California. Jensen will deliver a news-packed keynote on March 18, and we will host a Q&A session for our financial analysts the next day, March 19. We look forward to seeing you at these events. Our earnings call to discuss the results for our first quarter of fiscal 2026 is scheduled for May 28, 2025. We are going to open up the call, operator, to questions. If you could start that, that would be great. Question and Answer"
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "[Operator Instructions] And your first question comes from C.J. Muse with Cantor Fitzgerald."
    },
    {
      "speaker": "Christopher James Muse",
      "role": "Cantor Fitzgerald & Co., Research Division",
      "text": "Cantor Fitzgerald & Co., Research Division I guess for me, Jensen, as TestCon compute and reinforcement learning shows such promise, we're clearly seeing an increasing blurring of the lines between training and inference, what does this mean for the potential future of potentially inference dedicated clusters? And how do you think about the overall impact to NVIDIA and your customers?"
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Yes, I appreciate that C.J. There are now multiple scaling loss. There's the pre-training scaling law, and that's going to continue to scale because we have multimodality, we have data that came from reasoning that are now used to do pretraining. And then the second is post-training scaling law, using reinforcement learning human feedback, reinforcement learning AI feedback, reinforcement learning, verifiable rewards. The amount of computation you use for post training is actually higher than pretraining. And it's kind of sensible in the sense that you could, while you're using reinforcement learning, generate an enormous amount of synthetic data or synthetically generated tokens. AI models are basically generating tokens to train AI models. And that's post-training. And the third part, this is the part that you mentioned is test-time compute or reasoning, long thinking, inference scaling. They're all basically the same ideas. And there you have a chain of thought, you've search. The amount of tokens generated, the amount of inference compute needed is already 100x more than the one-shot examples and the one-shot capabilities of large language models in the beginning. And that's just the beginning. This is just the beginning. The idea that the next generation could have thousands times and even hopefully, extremely thoughtful and simulation-based and search-based models that could be hundreds of thousands, millions of times more compute than today is in our future. And so the question is how do you design such an architecture? Some of it -- some of the models are auto regressive. Some of the models are diffusion- based. Some of it -- some of the times you want your data center to have disaggregated inference. Sometimes it is compacted. And so it's hard to figure out what is the best configuration of a data center, which is the reason why NVIDIA's architecture is so popular. We run every model. We are great at training. The vast majority of our compute today is actually inference and Blackwell takes all of that to a new level. We designed Blackwell with the idea of reasoning models in mind. And when you look at training, it's many times more performant. But what's really amazing is for long thinking test-time scaling, reasoning AI models were tens of times faster, 25x higher throughput. And so Blackwell is going to be incredible across the board. And when you have a data center that allows you to configure and use your data center based on are you doing more pretraining now, post training now or scaling out your inference, our architecture is fungible and easy to use in all of those different ways. And so we're seeing, in fact, much, much more concentration of a unified architecture than ever before."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from the line of Joe Moore with JPMorgan (sic) [ Morgan Stanley ]."
    },
    {
      "speaker": "Joseph Lawrence Moore",
      "role": "Morgan Stanley, Research Division",
      "text": "Morgan Stanley, Research Division Morgan Stanley, actually. I wonder if you could talk about GB200. At CES, you sort of talked about the complexity of the rack level systems and the challenges you have. And then as you said in the prepared remarks, we've seen a lot of general availability. Where are you in terms of that ramp? Are there still bottlenecks to consider at a systems level above and beyond the chip level? And just have you maintained your enthusiasm for the NVL72 platforms?"
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Well, I'm more enthusiastic today than I was at CES. And the reason for that is because we've shipped a lot more since CES. We have some 350 plants manufacturing the 1.5 million components that go into each one of the Blackwell racks, Grace Blackwell racks. Yes, it's extremely complicated. And we successfully and incredibly ramped up Grace Blackwell, delivering some $11 billion of revenues last quarter. We're going to have to continue to scale as demand is quite high, and customers are anxious and impatient to get their Blackwell systems. You've probably seen on the web, a fair number of celebrations about Grace Blackwell systems coming online and we have them, of course. We have a fairly large installation of Grace Blackwells for our own engineering and our own design teams and software teams. CoreWeave has now been quite public about the successful bring up of theirs. Microsoft has. Of course, OpenAI has. And you're starting to see many come online. And so I think the answer to your question is nothing is easy about what we're doing, but we're doing great, and all of our partners are doing great."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from the line of Vivek Arya with Bank of America Securities."
    },
    {
      "speaker": "Vivek  Arya",
      "role": "BofA Securities, Research Division",
      "text": "BofA Securities, Research Division Colette, if you wouldn't mind confirming if Q1 is the bottom for gross margins? And then Jensen, my question is for you. What is on your dashboard to give you the confidence that the strong demand can sustain into next year? And has DeepSeek and whatever innovations they came up with, has that changed that view in any way?"
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO Let me first take the first part of the question there regarding the gross margin. During our Blackwell ramp, our gross margins will be in the low 70s. At this point, we are focusing on expediting our manufacturing, expediting our manufacturing to make sure that we can provide to customers as soon as possible. Our Blackwell is fully ramped. And once it does -- I'm sorry, once our Blackwell fully ramps, we can improve our cost and our gross margin. So we expect to probably be in the mid-70s later this year. Walking through what you heard Jensen speak about the systems and their complexity, they are customizable in some cases. They've got multiple networking options. They have liquid cooled and water cooled. So we know there is an opportunity for us to improve these gross margins going forward. But right now, we are going to focus on getting the manufacturing complete and to our customers as soon as possible."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director We know several things, Vivek. We have a fairly good line of sight of the amount of capital investment that data centers are building out towards. We know that going forward, the vast majority of software is going to be based on machine learning. And so accelerated computing and generative AI, reasoning AI are going to be the type of architecture you want in your data center. We have, of course, forecasts and plans from our top partners. And we also know that there are many innovative, really exciting start-ups that are still coming online as new opportunities for developing the next breakthroughs in AI, whether it's agentic AIs, reasoning AI or physical AIs. The number of start-ups are still quite vibrant and each one of them need a fair amount of computing infrastructure. And so I think the -- whether it's the near-term signals or the mid-term signals, near-term signals, of course, are POs and forecasts and things like that. Mid-term signals would be the level of infrastructure and CapEx scale-out compared to previous years. And then the long-term signals has to do with the fact that we know fundamentally software has changed from hand coding that runs on CPUs, to machine learning and AI-based software that runs on GPUs and accelerated computing systems. And so we have a fairly good sense that this is the future of software. And then maybe as you roll it out, another way to think about that is we've really only tapped consumer AI and search and some amount of consumer generative AI, advertising, recommenders, kind of the early days of software. The next wave is coming, agentic AI for enterprise, physical AI for robotics and sovereign AI as different regions build out their AI for their own ecosystems. And so each one of these are barely off the ground, and we can see them. We can see them because, obviously, we're in the center of much of this development and we can see great activity happening in all these different places and these will happen. So near term, mid-term, long term."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from the line of Harlan Sur with JPMorgan."
    },
    {
      "speaker": "Harlan L.  Sur",
      "role": "JPMorgan Chase & Co, Research Division",
      "text": "JPMorgan Chase & Co, Research Division Your next-generation Blackwell Ultra is set to launch in the second half of this year, in line with the team's annual product cadence. Jensen, can you help us understand the demand dynamics for Ultra given that you'll still be ramping the current generation Blackwell solutions? How do your customers and the supply chain also manage the simultaneous ramps of these 2 products? And is the team still on track to execute Blackwell Ultra in the second half of this year?"
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Yes. Blackwell Ultra is second half. As you know, the first Blackwell was -- we had a hiccup that probably cost us a couple of months. We're fully recovered, of course. The team did an amazing job recovering and all of our supply chain partners and just so many people helped us recover at the speed of light. And so now we've successfully ramped production of Blackwell. But that doesn't stop the next train. The next train is on an annual rhythm and Blackwell Ultra with new networking, new memories and of course, new processors, and all of that is coming online. We've have been working with all of our partners and customers, laying this out. They have all of the necessary information, and we'll work with everybody to do the proper transition. This time between Blackwell and Blackwell Ultra, the system architecture is exactly the same. It's a lot harder going from Hopper to Blackwell because we went from an NVLink 8 system to a NVLink 72-based system. So the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change. This was quite a challenging transition. But the next transition will slot right in. Blackwell Ultra will slot right in. We've also already revealed and been working very closely with all of our partners on the click after that. And the click after that is called Vera Rubin and all of our partners are getting up to speed on the transition of that and so preparing for that transition. And again, we're going to provide a big, huge step-up. And so come to GTC, and I'll talk to you about Blackwell Ultra, Vera Rubin and then show you what's the one click after that. Really exciting new products, so to come to GTC, please."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from the line of Timothy Arcuri with UBS."
    },
    {
      "speaker": "Timothy Michael Arcuri",
      "role": "UBS Investment Bank, Research Division",
      "text": "UBS Investment Bank, Research Division Jensen, we heard a lot about custom ASICs. Can you kind of speak to the balance between custom ASIC and merchant GPU? We hear about some of these heterogeneous superclusters to use both GPU and ASIC. Is that something customers are planning on building? Or will these infrastructures remain fairly distinct?"
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Well, we build very different things than ASICs, in some ways, completely different in some areas we intercept. We're different in several ways. One, NVIDIA'S architecture is general whether you're -- you've optimized for auto regressive models or diffusion-based models or vision-based models or multimodal models or text models. We're great in all of it. We're great at all of it because our software stack is so -- our architecture is flexible, our software stack ecosystem is so rich that we're the initial target of most exciting innovations and algorithms. And so by definition, we're much, much more general than narrow. We're also really good from the end-to-end from data processing, the curation of the training data, to the training of the data, of course, to reinforcement learning used in post training, all the way to inference with test-time scaling. So we're general, we're end-to-end, and we're everywhere. And because we're not in just one cloud, we're in every cloud, we could be on-prem. We could be in a robot. Our architecture is much more accessible and a great target -- initial target for anybody who's starting up a new company. And so we're everywhere. And the third thing I would say is that our performance and our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixed in power. And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual, it translates directly to revenues. And so if you have a 100-megawatt data center, if the performance or the throughput in that 100-megawatt or the gigawatt data center is 4x or 8x higher, your revenues for that gigawatt data center is 8x higher. And the reason that is so different than data centers of the past is because AI factories are directly monetizable through its tokens generated. And so the token throughput of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are building these things for revenue generation reasons and capturing the fast ROI. And so I think the third reason is performance. And then the last thing that I would say is the software stack is incredibly hard. Building an ASIC is no different than what we do. We build a new architecture. And the ecosystem that sits on top of our architecture is 10x more complex today than it was 2 years ago. And that's fairly obvious because the amount of software that the world is building on top of architecture is growing exponentially and AI is advancing very quickly. So bringing that whole ecosystem on top of multiple chips is hard. And so I would say that those 4 reasons. And then finally, I will say this, just because the chip is designed doesn't mean it gets deployed. And you've seen this over and over again. There are a lot of chips that gets built, but when the time comes, a business decision has to be made, and that business decision is about deploying a new engine, a new processor into a limited AI factory in size, in power and in time. And our technology is not only more advanced, more performant, it has much, much better software capability and very importantly, our ability to deploy is lightning fast. And so these things are enough for the faint of heart, as everybody knows now. And so there's a lot of different reasons why we do well, why we win."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from the line of Ben Reitzes with Melius Research."
    },
    {
      "speaker": "Benjamin Alexander Reitzes",
      "role": "Melius Research LLC",
      "text": "Melius Research LLC Ben Reitzes here. Jensen, it's a geography-related question. you did a great job explaining some of the demand underlying factors here on the strength. But U.S. was up about $5 billion or so sequentially. And I think there is a concern about whether U.S. can pick up the slack if there's regulations towards other geographies. And I was just wondering, as we go throughout the year, if this kind of surge in the U.S. continues and it's going to be -- whether that's okay. And if that underlies your growth rate, how can you keep growing so fast with this mix shift towards the U.S.? Your guidance looks like China is probably up sequentially. So just wondering if you could go through that dynamic and maybe Colette can weigh in."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director China is approximately the same percentage as Q4 and as previous quarters. It's about half of what it was before the export control. But it's approximately the same in percentage. With respect to geographies, the takeaway is that AI is software. It's modern software. It's incredible modern software, but it's modern software and AI has gone mainstream. AI is used in delivery services everywhere, shopping services everywhere. If you were to buy a quarter of milk and delivered to you, AI was involved. And so almost everything that a consumer service provides, AI is at the core of it. Every student will use AI as a tutor, health care services use AI, financial services use AI. No fintech company will not use AI. Every fintech company will. Climate tech company use AI. Mineral discovery now uses AI. The number of -- every higher education, every university uses AI and so I think it is fairly safe to say that AI has gone mainstream and that it's being integrated into every application. And our hope is that, of course, the technology continues to advance safely and advance in a helpful way to society. And with that, I do believe that we're at the beginning of this new transition. And what I mean by that in the beginning is, remember, behind us has been decades of data centers and decades of computers that have been built. And they've been built for a world of hand coding and general purpose computing and CPUs and so on and so forth. And going forward, I think it's fairly safe to say that world is going to be almost all software will be infused with AI. All software and all services will be based on -- ultimately, based on machine learning, the data flywheel is going to be part of improving software and services and that the future computers will be accelerated, the future computers will be based on AI. And we're really 2 years into that journey. And in modernizing computers that have taken decades to build out. And so I'm fairly sure that we're in the beginning of this new era. And then lastly, no technology has ever had the opportunity to address a larger part of the world's GDP than AI. No software tool ever has. And so this is now a software tool that can address a much larger part of the world's GDP more than any time in history. And so the way we think about growth and the way we think about whether something is big or small, has to be in the context of that. And when you take a step back and look at it from that perspective, we're really just in the beginning."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from the line of Aaron Rakers with Wells Fargo. Your next question comes from Mark Lipacis with Evercore ISI."
    },
    {
      "speaker": "Mark John Lipacis",
      "role": "Evercore ISI Institutional Equities, Research Division",
      "text": "Evercore ISI Institutional Equities, Research Division That's Mark Lipacis. I had a clarification and a question. Colette, up for the clarification. Did you say that enterprise within the data center grew 2x year-on-year for the January quarter? And if so, does that -- would that make it the fast faster growing than the hyperscalers? And then, Jensen, for you, the question, hyperscalers are the biggest purchasers of your solutions, but they buy equipment for both internal and external workloads, external workflows being cloud services that enterprise is used. So the question is, can you give us a sense of how that hyperscaler spend splits between that external workload and internal? And as these new AI workflows and applications come up, would you expect enterprises to become a larger part of that consumption mix? And does that impact how you develop your service, your ecosystem?"
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO Sure. Thanks for the question regarding our Enterprise business. Yes, it grew 2x and very similar to what we were seeing with our large CSPs. Keep in mind, these are both important areas to understand working with the CSPs and be working on large language models, can be working on inference and their own work. But keep in mind, that is also where the enterprises are surfacing. Your enterprises are both with your CSPs as well as in terms of building on their own. They're both correct, growing quite well."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director The CSPs are about half of our business. And the CSPs have internal consumption and external consumption, as you say. And we're using -- of course, used for internal consumption. We work very closely with all of them to optimize workloads that are internal to them because they have a large infrastructure of NVIDIA gear that they could take advantage of. And the fact that we could be used for AI on the one hand, video processing on the other hand, data processing like Spark, we're fungible. And so the useful life of our infrastructure is much better. If the useful life is much longer, then the TCO is also lower. And so -- the second part is how do we see the growth of enterprise or not CSPs, if you will, going forward? And the answer is, I believe, long term, it is by far larger and the reason for that is because if you look at the computer industry today and what is not served by the computer industry is largely industrial. So let me give you an example. When we say enterprise, and let's use the car company as an example because they make both soft things and hard things. And so in the case of a car company, the employees will be what we call enterprise and agenetic AI and software planning systems and tools, and we have some really exciting things to share with you guys at GTC, build agentic systems are for employees to make employees more productive to design, to market, to plan, to operate their company. That's agenetic AI. On the other hand, the cars that they manufacture also need AI. They need an AI system that trains the cars, treats this entire giant fleet of cars. And today, there's 1 billion cars on the road. Someday, there will be 1 billion cars on the road, and every single one of those cars will be robotic cars, and they'll all be collecting data, and we'll be improving them using an AI factory. Whereas they have a car factory today, in the future, they'll have a car factory and an AI factory. And then inside the car itself is a robotic system. And so as you can see, there are 3 computers involved and there's the computer that helps the people. There's the computer that build the AI for the machineries that could be, of course, it could be a tractor, it could be a lawn mower. It could be a humanoid robot that's being developed today. It could be a building. It could be a warehouse. These physical systems require new type of AI we call physical AI. They can't just understand the meaning of words and languages, but they have to understand the meaning of the world, friction and inertia, object permanence and cause and effect. And all of those type of things that are common sense to you and I, but AIs have to go learn those physical effects. So we call that physical AI. That whole part of using agentic AI to revolutionize the way we work inside companies, that's just starting. This is now the beginning of the agentic AI era, and you hear a lot of people talking about it and we've got some really great things going on. And then there's the physical AI after that, and then the robotic systems after that. And so these 3 computers are all brand new. And my sense is that long term, this will be by far the larger of them all, which kind of makes sense. The world's GDP is represented by either heavy industries or industrials and companies that are providing for those."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from the line of Aaron Rakers with Wells Fargo."
    },
    {
      "speaker": "Aaron Christopher Rakers",
      "role": "Wells Fargo Securities, LLC, Research Division",
      "text": "Wells Fargo Securities, LLC, Research Division Jensen, I'm curious as we now approach the 2-year anniversary of really the Hopper inflection that you saw in 2023 in GenAI in general. And when we think about the road map you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle perspective? And whether if it's GB300 or if it's the Rubin cycle where we start to see maybe some refresh opportunity. I'm just curious to how you look at that."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director I appreciate it. First of all, people are still using Voltas and Pascals and Amperes. And the reason for that is because there are always things that -- because CUDA is so programmable you could use it -- one of the major use cases right now is data processing and data curation. You find a circumstance that an AI model is not very good at. You present that circumstance to a vision language model, let's say, it's a car. You present that circumstance to a vision language model. The vision language model actually looks at the circumstances and said, \"This is what happened and I wasn't very good at it.\" You then take that response -- the prompt and you go and prompt an AI model to go find in your whole lake of data, other circumstances like that, whatever that circumstance was. And then you use an AI to do domain randomization and generate a whole bunch of other examples. And then from that, you can go train the model. And so you could use the Amperes to go and do data processing and data curation and machine learning- based search. And then you create the training data set, which you then present to your Hopper systems for training. And so each one of these architectures are completely -- they're all CUDA-compatible and so everything runs on everything. But if you have infrastructure in place, then you can put the less intensive workloads onto the installed base of the past. All of our GPUs are very well employed."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "We have time for one more question, and that question comes from Atif Malik with Citi."
    },
    {
      "speaker": "Atif  Malik",
      "role": "Citigroup Inc., Research Division",
      "text": "Citigroup Inc., Research Division I have a follow-up question on gross margins for Colette. Colette, I understand there are many moving parts the Blackwell yields, NVLink 72 and Ethernet mix. And you kind of tipped to the earlier question, the April quarter is the bottom. But second half would have to ramp like 200 basis points per quarter to get to the mid-70s range that you're giving for the end of the fiscal year. And we still don't know much about tariff impact to broader semiconductor. So what kind of gives you the confidence in that trajectory in the back half of this year?"
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO Yes. Thanks for the question. Our gross margins, they're quite complex in terms of the material and everything that we put together in a Blackwell system, a tremendous amount of opportunity to look at a lot of different pieces of that on how we can better improve our gross margins over time. Remember, we have many different configurations as well on Blackwell that will be able to help us do that. So together, working after we get some of these really strong ramping completed for our customers, we can begin a lot of that work. If not, we're going to probably start as soon as possible if we can. If we can improve it in the short term, we will also do that. Tariff, at this point, it's a little bit of an unknown it's an unknown until we understand further what the U.S. government's plan is, both its timing, it's where and how much. So at this time, we are awaiting, but again, we would, of course, always follow export controls and/or tariffs in that manner."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Ladies and gentlemen, that does conclude our question-and-answer session. I'm sorry."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Thank you."
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO We are going to open up to Jensen, and I believe he has a couple of things."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director I just wanted to thank you. Thank you, Colette. The demand for Blackwell is extraordinary. AI is evolving beyond perception and generative AI into reasoning. With reasoning AI, we're observing another scaling law, inference time or test-time scaling. The more computation, the more the model thinks the smarter the answer. Models like OpenAI, Grok 3, DeepSeek-R1 are reasoning models that apply inference time scaling. Reasoning models can consume 100x more compute. Future reasoning models can consume much more compute. DeepSeek-R1 has ignited global enthusiast. It's an excellent innovation. But even more importantly, it has open source a world-class reasoning AI model. Nearly every AI developer is applying R1 or chain of thought and reinforcement learning techniques like R1 to scale their model's performance. We now have 3 scaling laws, as I mentioned earlier, driving the demand for AI computing. The traditional scaling loss of AI remains intact. Foundation models are being enhanced with multimodality, and pretraining is still growing. But it's no longer enough. We have 2 additional scaling dimensions. Post-training scaling, where reinforcement learning, fine-tuning, model distillation require orders of magnitude more compute than pretraining alone. Inference time scaling and reasoning where a single query and demand 100x more compute. We designed Blackwell for this moment, a single platform that can easily transition from pre-training, post-training and test-time scaling. Blackwell's FP4 transformer engine and NVLink 72 scale-up fabric and new software technologies led Blackwell process reasoning AI models, 25x faster than Hopper. Blackwell in all of this configuration is in full production. Each Grace Blackwell NVLink 72 rack is an engineering marvel. 1.5 million components produced across 350 manufacturing sites by nearly 100,000 factory operators. AI is advancing at light speed. We're at the beginning of reasoning AI and inference time scaling. But we're just at the start of the age of AI, multimodal AIs, enterprise AI, sovereign AI and physical AI are right around the corner. We will grow strongly in 2025. Going forward, data centers will dedicate most of CapEx to accelerated computing and AI. Data centers will increasingly become AI factories and every company will have them either renting or self-operated. I want to thank all of you for joining us today. Come join us at GTC in a couple of weeks. We're going to be talking about Blackwell Ultra, Rubin and other new computing, networking, reasoning AI, physical AI products and a whole bunch more. Thank you. OperatorThis concludes today's conference call. You may now disconnect. Copyright © 2025 by S&P Global Market Intelligence, a division of S&P Global Inc. All rights reserved. These materials have been prepared solely for information purposes based upon information generally available to the public and from sources believed to be reliable. No content (including index data, ratings, credit- related analyses and data, research, model, software or other application or output therefrom) or any part thereof (Content) may be modified, reverse engineered, reproduced or distributed in any form by any means, or stored in a database or retrieval system, without the prior written permission of S&P Global Market Intelligence or its affiliates (collectively, S&P Global). The Content shall not be used for any unlawful or unauthorized purposes. S&P Global and any third-party providers, (collectively S&P Global Parties) do not guarantee the accuracy, completeness, timeliness or availability of the Content. S&P Global Parties are not responsible for any errors or omissions, regardless of the cause, for the results obtained from the use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P GLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR DEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE CONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no event shall S&P Global Parties be liable to any party for any direct, indirect, incidental, exemplary, compensatory, punitive, special or consequential damages, costs, expenses, legal fees, or losses (including, without limitation, lost income or lost profits and opportunity costs or losses caused by negligence) in connection with any use of the Content even if advised of the possibility of such damages. S&P Global Market Intelligence's opinions, quotes and credit-related and other analyses are statements of opinion as of the date they are expressed and not statements of fact or recommendations to purchase, hold, or sell any securities or to make any investment decisions, and do not address the suitability of any security. S&P Global Market Intelligence may provide index data. Direct investment in an index is not possible. Exposure to an asset class represented by an index is available through investable instruments based on that index. S&P Global Market Intelligence assumes no obligation to update the Content following publication in any form or format. The Content should not be relied on and is not a substitute for the skill, judgment and experience of the user, its management, employees, advisors and/or clients when making investment and other business decisions. S&P Global Market Intelligence does not act as a fiduciary or an investment advisor except where registered as such. S&P Global keeps certain activities of its divisions separate from each other in order to preserve the independence and objectivity of their respective activities. As a result, certain divisions of S&P Global may have information that is not available to other S&P Global divisions. S&P Global has established policies and procedures to maintain the confidentiality of certain nonpublic information received in connection with each analytical process. S&P Global may receive compensation for its ratings and certain analyses, normally from issuers or underwriters of securities or from obligors. S&P Global reserves the right to disseminate its opinions and analyses. S&P Global's public ratings and analyses are made available on its Web sites, www.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and www.globalcreditportal.com (subscription), and may be distributed through other means, including via S&P Global publications and third-party redistributors. Additional information about our ratings fees is available at www.standardandpoors.com/usratingsfees. © 2025 S&P Global Market Intelligence."
    }
  ],
  "source_file": "NVIDIA Corporation, Q4 2025 Earnings Call, Feb 26, 2025.rtf"
}