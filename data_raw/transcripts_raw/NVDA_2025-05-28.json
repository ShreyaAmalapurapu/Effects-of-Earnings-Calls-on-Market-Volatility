{
  "event_id": "NVDA_2025-05-28",
  "ticker": "NVDA",
  "company": "NVIDIA Corporation",
  "quarter": 1,
  "fiscal_year": 2026,
  "call_date": "2025-05-28",
  "call_start_ts": "2025-05-28 21:00:00+00:00",
  "raw_text": "\n|[pic]                     |\n\nNVIDIA Corporation NasdaqGS:NVDA\nFQ1 2026 Earnings Call Transcripts\nWednesday, May 28, 2025 9:00 PM GMT\nS&P Global Market Intelligence Estimates\n|      |-FQ1 2026-           |-FQ2  |-FY   |-FY   |\n|      |                     |2026- |2026- |2027- |\n|                              |CONSENSUS      |ACTUAL         |SURPRISE       |\n|                   |CONSENSUS          |ACTUAL             |SURPRISE           |\n|FQ2 2025           |0.64               |0.68               |[pic]6.25 %        |\n|FQ3 2025           |0.75               |0.81               |[pic]8.00 %        |\n|FQ4 2025           |0.85               |0.89               |[pic]4.71 %        |\n|FQ1 2026           |0.75               |0.81               |[pic]8.00 %        |\n\n|Table of Contents                                     |   |\n|Call Participants          |..............................................|3      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|Presentation               |..............................................|4      |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|Question and Answer        |..............................................|11     |\n|                           |..............................................|       |\n|                           |..............................................|       |\n|                           |..............                                |       |\n|                                                                                  |\n|Call Participants                                                                 |\n|                           |                           |                           |\n|EXECUTIVES                 |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Colette M. Kress           |                           |                           |\n|Executive VP & CFO         |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Jen-Hsun  Huang            |                           |                           |\n|Co-Founder, CEO, President |                           |                           |\n|& Director                 |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Toshiya  Hari              |                           |                           |\n|Vice President of Investor |                           |                           |\n|Relations & Strategic      |                           |                           |\n|Finance                    |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|ANALYSTS                   |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Benjamin Alexander Reitzes |                           |                           |\n|                           |                           |                           |\n|Melius Research LLC        |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Christopher James Muse     |                           |                           |\n|Cantor Fitzgerald & Co.,   |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Jacob Michael Wilhelm      |                           |                           |\n|Wells Fargo Securities,    |                           |                           |\n|LLC, Research Division     |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Joseph Lawrence Moore      |                           |                           |\n|Morgan Stanley, Research   |                           |                           |\n|Division                   |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Timothy Michael Arcuri     |                           |                           |\n|UBS Investment Bank,       |                           |                           |\n|Research Division          |                           |                           |\n|                           |                           |                           |\n|                           |                           |                           |\n|Vivek  Arya                |                           |                           |\n|BofA Securities, Research  |                           |                           |\n|Division                   |                           |                           |\n|                                                                                  |\n\n\nPresentation\n\n\nOperator\n\nGood afternoon. My name is Sarah, and I will be your conference operator\ntoday. At this time, I would like to welcome everyone to NVIDIA's First\nQuarter Fiscal 2026 Financial Results Conference Call. [Operator\nInstructions]\n\n\nToshiya Hari, you may begin your conference.\n\n\nToshiya  Hari\nVice President of Investor Relations & Strategic Finance\n\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference\ncall for the first quarter of fiscal 2026. With me today from NVIDIA are\nJensen Huang, President and Chief Executive Officer; and Colette Kress,\nExecutive Vice President and Chief Financial Officer.\n\n\nI'd like to remind you that our call is being webcast live on NVIDIA's\nInvestor Relations website. The webcast will be available for replay until\nthe conference call to discuss our financial results for the second quarter\nof fiscal 2026.\n\n\nThe content of today's call is NVIDIA's property. It cannot be reproduced\nor transcribed without our prior written consent.\n\n\nDuring this call, we may make forward-looking statements based on current\nexpectations. These are subject to a number of significant risks and\nuncertainties, and our actual results may differ materially. For a\ndiscussion of factors that could affect our future financial results and\nbusiness, please refer to the disclosure in today's earnings release, our\nmost recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-\nK with the Securities and Exchange Commission. All our statements are made\nas of today, May 28, 2025, based on information currently available to us.\nExcept as required by law, we assume no obligation to update any such\nstatements.\n\n\nDuring this call, we will discuss non-GAAP financial measures. You can find\na reconciliation of these non-GAAP financial measures to GAAP financial\nmeasures in our CFO commentary, which is posted on our website.\n\n\nWith that, let me turn the call over to Colette.\n\n\nColette M. Kress\nExecutive VP & CFO\n\nThank you, Toshiya. We delivered another strong quarter with revenue of $44\nbillion, up 69% year-over-year, exceeding our outlook in what proved to be\na challenging operating environment. Data Center revenue of $39 billion\ngrew 73% year-on-year. AR workloads have transitioned strongly to inference\nand AI factory build-outs are driving significant revenue. Our customers'\ncommitments are firm.\n\n\nOn April 9, the U.S. government issued new export controls on H20, our data\ncenter GPU designed specifically for the China market. We sold H20 with the\napproval of the previous administration. Although our H20 has been in the\nmarket for over a year and does not have a market outside of China, the new\nexport controls on H20 did not provide a grace period to allow us to sell\nthrough our inventory. In Q1, we recognized $4.6 billion in H20 revenue,\nwhich occurred prior to April 9, but also recognized a $4.5 billion charge\nas we wrote down inventory and purchase obligations tied to orders we had\nreceived prior to April 9.\n\n\nWe were unable to ship $2.5 billion in H20 revenue in the first quarter due\nto the new export controls. The $4.5 billion charge was less than what we\ninitially anticipated as we were able to reuse certain materials. We are\nstill evaluating our limited options to supply Data Center compute products\ncompliant with the U.S. government's revised export control rules. Losing\naccess to the China AI accelerator market, which we believe will grow to\nnearly $50 billion, would have a material adverse impact on our business\ngoing forward and benefit our foreign competitors in China and worldwide.\n\n\nOur Blackwell ramp, the fastest in our company's history, drove a 73% year-\non-year increase in Data Center revenue. Blackwell contributed nearly 70%\nof Data Center compute revenue in the quarter with the transition from\nHopper nearly complete.\n\n\nThe introduction of GB200 NVL was a fundamental architectural change to\nenable data center-scale workloads and to achieve the lowest cost per\ninference token. While these systems are complex to build, we have seen a\nsignificant improvement in manufacturing yields, and rack shipments are\nmoving to strong rates to end customers. GB200 NVL racks are now generally\navailable for motor builders, enterprises and sovereign customers to\ndevelop and deploy AI.\n\n\nOn average, major hyperscalers are each deploying nearly 1,000 NVL72 racks\nor 72,000 Blackwell GPUs per week and are on track to further ramp output\nthis quarter. Microsoft, for example, has already deployed tens of\nthousands of Blackwell GPUs and is expected to ramp to hundreds of\nthousands of GB200s with OpenAI as one of its key customers. Key learnings\nfrom the GB200 ramp will allow for a smooth transition to the next phase of\nour product road map, Blackwell Ultra.\n\n\nSampling of GB300 systems began earlier this month at the major CSPs, and\nwe expect production shipments to commence later this quarter. GB300 will\nleverage the same architecture, same physical footprint and the same\nelectrical and mechanical specifications as GB200. The GB300 drop-in design\nwill allow CSPs to seamlessly transition their systems and manufacturing\nused for GB200 while maintaining high yields. B300 GPUs with 50% more HBM\nwill deliver another 50% increase in dense FP4 inference compute\nperformance compared to the B200.\n\n\nWe remain committed to our annual product cadence with our road map\nextending through 2028, tightly aligned with the multiple year planning\ncycles of our customers.\n\n\nWe are witnessing a sharp jump in inference demand. OpenAI, Microsoft and\nGoogle are seeing a step-function leap in token generation. Microsoft\nprocessed over 100 trillion tokens in Q1, a fivefold increase on a year-\nover-year basis. This exponential growth in Azure OpenAI is representative\nof strong demand for Azure AI foundry as well as other AI services across\nMicrosoft's platform.\n\n\nInference serving startups are now serving models using B200, tripling\ntheir token generation rate and corresponding revenues for high-value\nreasoning models such as DeepSeek-R1 as reported by artificial analysis.\n\n\nNVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by\n30x for the new reasoning models sweeping the industry. Developer\nengagements increased, with adoption ranging from LLM providers such as\nPerplexity to financial services institutions such as Capital One, who\nreduced agentic chatbot latency by 5x with Dynamo.\n\n\nIn the latest MLPerf inference results, we submitted our first results\nusing GB200 NVL72, delivering up to 30x higher inference throughput\ncompared to our 8-GPU 200 submission on the challenging Llama 3.1\nbenchmark. This feat was achieved through a combination of tripling the\nperformance per GPU as well as 9x more GPUs all connected on a single\nNVLink domain.\n\n\nAnd while Blackwell is still early in its life cycle, software\noptimizations have already improved its performance by 1.5x in the last\nmonth alone. We expect to continue improving the performance of Blackwell\nthrough its operational life as we have done with Hopper and Ampere. For\nexample, we increased the inference performance of Hopper by 4x over 2\nyears. This is the benefit of NVIDIA's programmable CUDA architecture and\nrich ecosystem.\n\n\nThe pace and scale of AI factory deployments are accelerating with nearly\n100 NVIDIA-powered AI factories in flight this quarter, a twofold increase\nyear-over-year, with the average number of GPUs powering each factory also\ndoubling in the same period. And more AI factory projects are starting\nacross industries and geographies. NVIDIA's full-stack architecture is\nunderpinning AI factory deployments as industry leaders like AT&T, BYD,\nCapital One, Foxconn, MediaTek, and Telenor, are strategically vital\nsovereign clouds like those recently announced in Saudi Arabia, Taiwan and\nthe U.A.E. We have a line of sight to projects requiring tens of gigawatts\nof NVIDIA AI infrastructure in the not-too-distant future.\n\n\nThe transition from generative to agentic AI, AI capable of perceiving,\nreasoning, planning and acting will transform every industry, every company\nand country. We envision AI agents as a new digital workforce capable of\nhandling tasks ranging from customer service to complex decision-making\nprocesses.\n\n\nWe introduced the Llama Nemotron family of open reasoning models designed\nto supercharge agentic AI platforms for enterprises. Built on the Llama\narchitecture, these models are available as NIMs, or NVIDIA inference\nmicroservices, with multiple sizes to meet diverse deployment needs. Our\npost-training enhancements have yielded a 20% accuracy boost and a 5x\nincrease in inference speed. Leading platform companies, including\nAccenture, Cadence, Deloitte, and Microsoft are transforming work with our\nreasoning models.\n\n\nNVIDIA NeMo microservices are generally available across industries that\nare being leveraged by leading enterprises to build, optimize and scale AI\napplications. With NeMo, Cisco increased model accuracy by 40% and improved\nresponse time by 10x in its code assistant. NASDAQ realized a 30%\nimprovement in accuracy and response time in its AI platform's search\ncapabilities. And Shell's custom LLM achieved a 30% increase in accuracy\nwhen trained with NVIDIA NeMo. NeMo's parallelism techniques accelerated\nmodel training time by 20% when compared to other frameworks.\n\n\nWe also announced a partnership with Yum! Brands, the world's largest\nrestaurant company to bring NVIDIA AI to 500 of its restaurants this year\nand expanding to 61,000 restaurants over time to streamline order-taking,\noptimize operations and enhance service across its restaurants.\n\n\nFor AI-powered cybersecurity, leading companies like Check Point,\nCrowdStrike and Palo Alto Networks are using NVIDIA's AI security and\nsoftware stack to build, optimize and secure agentic workflows, with\nCrowdStrike realizing 2x faster detection triage with 50% less compute\ncost.\n\n\nMoving to networking. Sequential growth in networking resumed in Q1 with\nrevenue up 64% quarter-over-quarter to $5 billion. Our customers continue\nto leverage our platform to efficiently scale up and scale out AI factory\nworkloads.\n\n\nWe created the world's fastest switch, NVLink, for scale up. Our NVLink\ncompute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen\n5. NVLink 72 carries 130 terabytes per second of bandwidth in a single\nrack, equivalent to the entirety of the world's peak Internet traffic.\nNVLink is a new growth vector and is off to a great start with Q1 shipments\nexceeding $1 billion.\n\n\nAt COMPUTEX, we announced NVLink Fusion. Hyperscale customers can now build\nsemi-custom CCUs and accelerators that connect directly to the NVIDIA\nplatform with NVLink. We are now enabling key partners, including ASIC\nproviders such as MediaTek, Marvell, Alchip Technologies and Astera Labs as\nwell as CPU suppliers such as Fujitsu and Qualcomm, to leverage and relink\nFusion to connect our respective ecosystems.\n\n\nFor scale out, our enhanced Ethernet offerings deliver the highest\nthroughput, lowest latency networking for AI. Spectrum-X posted strong\nsequential and year-on-year growth and is now annualizing over $8 billion\nin revenue. Adoption is widespread across major CSPs and consumer Internet\ncompanies, including CoreWeave, Microsoft Azure and Oracle Cloud and xAI.\nThis quarter, we added Google Cloud and Meta to the growing list of\nSpectrum-X customers.\n\n\nWe introduced Spectrum-X and Quantum-X silicon photonics switches,\nfeaturing the world's most advanced co-packaged optics. These platforms\nwill enable next-level AI factory scaling to millions of GPUs through the\nincreasingly power efficiency by 3.5x and network resiliency by 10x, while\naccelerating customer time to market by 1.3x.\n\n\nTransitioning to a quick summary of our revenue by geography. China as a\npercentage of our Data Center revenue was slightly below our expectations\nand down sequentially due to H20 export licensing controls. For Q2, we\nexpect a meaningful decrease in China data center revenue. As a reminder,\nwhile Singapore represented nearly 20% of our Q1 billed revenue as many of\nour large customers use Singapore for centralized invoicing, our products\nare almost always shipped elsewhere. Note that over 99% of H100, H200, and\nBlackwell Data Center compute revenue billed to Singapore was for orders\nfrom U.S.-based customers.\n\n\nMoving to gaming and AI PCs. Gaming revenue was a record $3.8 billion,\nincreasing 48% sequentially and 42% year-on-year. Strong adoption by\ngamers, creators and AI enthusiasts have made Blackwell our fastest ramp\never. Against the backdrop of robust demand, we greatly improved our supply\nand availability in Q1 and expect to continue these efforts in Q2.\n\n\nAI is transforming PC and creator and gamers. With a 100 million user\ninstalled base, GeForce represents the largest footprint for PC developers.\nThis quarter, we added to our AI PC laptop offerings, including models\ncapable of running Microsoft's CoPilot+. This past quarter, we brought\nBlackwell architecture to mainstream gaming with its launch of GeForce RTX\n5060 and 5060 Ti, starting at just $299. The RTX 5060 also debuted in\nlaptops, starting at $1,099. These systems doubled the frame rate and\nslashed latency. These GeForce RTX 5060 and 5060 Ti desktop GPUs and\nlaptops are now available.\n\n\nIn console gaming, the recently unveiled Nintendo Switch 2 leverages\nNVIDIA's neural rendering and AI technologies, including next-generation\ncustom RTX GPUs with DLSS technology to deliver a giant leap in gaming\nperformance to millions of players worldwide. Nintendo has shipped over 150\nmillion switch consoles to date, making it one of the most successful\ngaming systems in history.\n\n\nMoving to Pro Visualization. Revenue of $509 million was flat sequentially\nand up 19% year-on-year. Tariff-related uncertainty temporarily impacted Q1\nsystems and demand for our AI workstations is strong, and we expect\nsequential revenue growth to resume in Q2.\n\n\nNVIDIA DGX Spark and Station revolutionized personal computing. By putting\nthe power of an AI supercomputer in a desktop form factor. DGX Spark\ndelivers up to 1 petaflop of AI compute while DGX Station offers an\nincredible 20 petaflops and is powered by the GB300 Superchip. DGX Spark\nwill be available in calendar Q3 and DGX Station later this year.\n\n\nWe have deepened Omniverse's integration and adoption into some of the\nworld's leading software platforms, including Databricks, SAP and Schneider\nElectric. New Omniverse Blueprint such as Mega for at-scale robotic fleet\nmanagement are being leveraged in KION Group, Pegatron, Accenture and other\nleading companies to enhance industrial operations.\n\n\nAt COMPUTEX, we showcased Omniverse's great traction with technology\nmanufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron. Using\nOmniverse, TSMC saves months in work by designing fabs virtually, Foxconn\naccelerates thermal simulations by 150x, and Pegatron reduced assembly line\ndefect rates by 67%.\n\n\nLastly, with our Automotive group. Revenue was $567 million, down 1%\nsequentially but up 72% year-on-year. Year-on-year growth was driven by the\nramp of self-driving across a number of customers and robust end demand for\nNEVs. We are partnering with GM to build the next-gen vehicles, factories\nand robots using NVIDIA AI, simulation and accelerated computing. And we\nare now in production with our full-stack solution for Mercedes-Benz\nstarting with the new CLA, hitting roads in the next few months.\n\n\nWe announced Isaac GR00T N1, the world's first open fully customizable\nfoundation model for humanoid robots, enabling generalized reasoning and\nskill development. We also launched new open NVIDIA Cosmos World Foundation\nmodels. Leading companies include 1X, Agility Robotics, Figure AI, Uber and\nWaabi. We've begun integrating Cosmos into their operations for synthetic\ndata generation, while Agility Robotics, Boston Dynamics, and XPENG\nRobotics are harnessing Isaac's simulation to advance their humanoid\nefforts. GE Healthcare is using the new NVIDIA Isaac platform for health\ncare simulation built on NVIDIA Omniverse and using NVIDIA Cosmos for\nplatform speed, development of robotic imaging and surgery systems.\n\n\nThe era of robotics is here, billions of robots, hundreds of millions of\nautonomous vehicles and hundreds of thousands of robotic factories and\nwarehouses will be developed.\n\n\nAll right. Moving to the rest of the P&L. GAAP gross margins and non-GAAP\ngross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion\ncharge, Q1 non-GAAP gross margins would have been 71.3%, slightly above our\noutlook at the beginning of the quarter. Sequentially, GAAP operating\nexpenses were up 7% and non-GAAP operating expenses were up 6%, reflecting\nhigher compensation and employee growth. Our investments include expanding\nour infrastructure capabilities and AI solutions, and we plan to grow these\ninvestments throughout the fiscal year.\n\n\nIn Q1, we returned a record $14.3 billion to shareholders in the form of\nshare repurchases and cash dividends. Our capital return program continues\nto be a key element of our capital allocation strategy.\n\n\nLet me turn to the outlook for the second quarter. Total revenue is\nexpected to be $45 billion, plus or minus 2%. We expect modest sequential\ngrowth across all of our platforms. In Data Center, we anticipate the\ncontinued ramp of Blackwell to be partially offset by a decline in China\nrevenue. Note, our outlook reflects a loss in H20 revenue of approximately\n$8 billion for the second quarter.\n\n\nGAAP and non-GAAP gross margins are expected to be 71.8% and 72%,\nrespectively, plus or minus 50 basis points. We expect better Blackwell\nprofitability to drive modest sequential improvement in gross margins. We\nare continuing to work towards achieving gross margins in the mid-70s range\nlate this year.\n\n\nGAAP and non-GAAP operating expenses are expected to be approximately $5.7\nbillion and $4 billion, respectively, and we continue to expect full year\nfiscal year '26 operating expense growth to be in the mid-30% range. GAAP\nand non-GAAP other income and expenses are expected to be an income of\napproximately $450 million, excluding gains and losses from nonmarketable\nand publicly held equity securities. GAAP and non-GAAP tax rates are\nexpected to be 16.5%, plus or minus 1%, excluding any discrete items.\n\n\nFurther financial details are included in the CFO commentary and other\ninformation available on our IR website, including a new financial\ninformation AI agent.\n\n\nLet me highlight upcoming events for the financial community. We will be at\nthe BofA Global Technology Conference in San Francisco on June 4. The\nRosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on\nJune 10, and GTC Paris at VivaTech on June 11 in Paris. We look forward to\nseeing you at these events.\n\n\nOur earnings call to discuss the results of our second quarter of fiscal\n2026 is scheduled for August 27.\n\n\nWell, now let me turn it over to Jensen to make some remarks.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThanks, Colette. We've had a busy and productive year. Let me share my\nperspective on some topics we're frequently asked.\n\n\nOn export control, China is one of the world's largest AI markets and a\nspringboard to global success. With half of the world's AI researchers\nbased there, the platform that wins China is positioned to lead globally.\nToday, however, the $50 billion China market is effectively closed to U.S.\nindustry. The H20 export ban ended our Hopper Data Center business in\nChina. We cannot reduce Hopper further to comply. As a result, we are\ntaking a multibillion-dollar write-off on inventory that cannot be sold or\nrepurposed. We are exploring limited ways to compete, but Hopper is no\nlonger an option.\n\n\nChina's AI moves on with or without U.S. chips. It has the compute to train\nand deploy advanced models. The question is not whether China will have AI,\nit already does. The question is whether one of the world's largest AI\nmarkets will run on American platforms. Shielding Chinese chip makers from\nU.S. competition only strengthens them abroad and weakens America's\nposition. Export restrictions have spurred China's innovation and scale.\n\n\nThe AI race is not just about chips. It's about which stack the world runs\non. As that stack grows to include 6G and quantum, U.S. global\ninfrastructure leadership is at stake. The U.S. has based its policy on the\nassumption that China cannot make AI chips. That assumption was always\nquestionable, and now it's clearly wrong. China has enormous manufacturing\ncapability. In the end, the platform that wins the AI developers win AI\nwins AI. Export controls should strengthen U.S. platforms, not drive half\nof the world's AI talent to rivals.\n\n\nOn DeepSeek, DeepSeek and Qwen from China are among the most -- among the\nbest open source AI models. Released freely, they've gained traction across\nthe U.S., Europe and beyond. DeepSeek-R1, like ChatGPT, introduce reasoning\nAI that produces better answers the longer it thinks. Reasoning AI enables\nstep-by-step problem-solving, planning and tool use, turning models into\nintelligent agents.\n\n\nReasoning is compute-intensive, requires hundreds to thousands more --\nthousands of times more tokens per task than previous one-shot inference.\nReasoning models are driving a step-function surge in inference demand. AI\nscaling laws remain firmly intact, not only for training, but now inference\ntoo requires massive scale compute.\n\n\nDeepSeek also underscores the strategic value of open source AI. When\npopular models are trained and optimized on U.S. platforms, it drives\nusage, feedback and continuous improvement, reinforcing American leadership\nacross the stack. U.S. platforms must remain the preferred platform for\nopen source AI. That means supporting collaboration with top developers\nglobally, including in China. America wins when models like DeepSeek and\nQwen runs best on American infrastructure.\n\n\nRegarding onshore manufacturing, President Trump has outlined a bold vision\nto reshore advanced manufacturing, create jobs and strengthen national\nsecurity. Future plants will be highly computerized in robotics. We share\nthis vision. TSMC is building 6 fabs and 2 advanced packaging plants in\nArizona to make chips for NVIDIA. Process qualification is underway with\nvolume production expected by year-end. SPIL and Amkor are also investing\nin Arizona, constructing packaging, assembly and test facilities. In\nHouston, we're partnering with Foxconn to construct a 1 million square foot\nfactory to build AI supercomputers. Wistron is building a similar plant in\nFort Worth, Texas. To encourage and support these investments, we've made\nsubstantial long-term purchase commitments, a deep investment in America's\nAI manufacturing future.\n\n\nOur goal from chip to supercomputer built in America within a year. Each\nGB200 NVLink 72 racks contains 1.2 million components and weighs nearly 2\ntons. No one has produced supercomputers on this scale. Our partners are\ndoing an extraordinary job.\n\n\nOn AI Diffusion Rule, President Trump rescinded the AI Diffusion Rule,\ncalling it counterproductive, and proposed a new policy to promote U.S. AI\ntech with trusted partners. On his Middle East tour, he announced historic\ninvestments. I was honored to join him in announcing a 500-megawatt AI\ninfrastructure project in Saudi Arabia and a 5-gigawatt AI campus in the\nU.A.E. President Trump wants U.S. tech to lead. The deals he announced are\nwins for America, creating jobs, advancing infrastructure, generating tax\nrevenue and reducing the U.S. trade deficit. The U.S. will always be\nNVIDIA's largest market and home to the largest installed base of our\ninfrastructure.\n\n\nEvery nation now sees AI as core to the next industrial revolution, a new\nindustry that produces intelligence and essential infrastructure for every\neconomy. Countries are racing to build national AI platforms to elevate\ntheir digital capabilities. At COMPUTEX, we announced Taiwan's first AI\nfactory in partnership with Foxconn and the Taiwan government. Last week, I\nwas in Sweden to launch its first national AI infrastructure. Japan, Korea,\nIndia, Canada, France, the U.K., Germany, Italy, Spain and more are now\nbuilding national AI factories to empower start-ups, industries and\nsocieties. Sovereign AI is a new growth engine for NVIDIA.\n\n\nToshiya, back to you. Thank you.\n\n\nToshiya  Hari\nVice President of Investor Relations & Strategic FinanceOperator, we will\nnow open the call for questions. Would you please poll for questions?\n\n\nQuestion and Answer\n\n\nOperator\n\n[Operator Instructions] Your first question comes from the line of Joe\nMoore with Morgan Stanley.\n\n\nJoseph Lawrence Moore\nMorgan Stanley, Research Division\n\nYou guys have talked about this scaling up of inference around reasoning\nmodels for at least a year now. And we've really seen that come to fruition\nas you talked about. We've heard it from your customers. Can you give us a\nsense for how much of that demand you're able to serve? And give us a sense\nfor maybe how big the inference business is for you guys. And do we need\nfull on NVL72 rack scale solutions for reasoning inference going forward?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, we would like to serve all of it. And I think we're on track to serve\nmost of it. Grace Blackwell NVLink 72 is the ideal engine today, the ideal\ncomputer thinking machine, if you will, for reasoning AI. There's a couple\nof reasons for that. The first reason is that the token generation amount,\nthe number of tokens reasoning goes through, is 100, 1,000x more than a one-\nshot chatbot. It's essentially thinking to itself, breaking down a problem\nstep by step. It might be planning multiple paths to an answer. It could be\nusing tools, reading PDFs, reading web pages, watching videos and then\nproducing a result, an answer. The longer it thinks, the better the answer,\nthe smarter the answer is. And so what we would like to do and the reason\nwhy Grace Blackwell was designed to give such a giant step-up in inference\nperformance is so that you could do all this and still get a response as\nquickly as possible.\n\n\nCompared to Hopper, Grace Blackwell is some 40x higher speed and\nthroughput, compared. And so this is going to be a huge, huge benefit in\ndriving down the cost while improving the quality of response with\nexcellent quality of service at the same time. So that's the fundamental\nreason. That was the core driving reason for Grace Blackwell and NVLink 72.\nOf course, in order to do that, we had to reinvent, literally redesign the\nentire way that these supercomputers are built. But now we're in full\nproduction. It's going to be exciting. It's going to be incredibly\nexciting.\n\n\nOperator\n\nThe next question comes from Vivek Arya with Bank of America Securities.\n\n\nVivek  Arya\nBofA Securities, Research Division\n\nJust a clarification for Colette first. So on the China impact, I think\npreviously, it was mentioned at about $15 billion. So you had the $8\nbillion in Q2. So is there still some left as a headwind for the remaining\nquarters, just for how to model that.\n\n\nAnd then a question, Jensen, for you. Back at GTC, you had outlined a path\ntowards almost $1 trillion of AI spending over the next few years. Where\nare we in that build-out? And do you think it's going to be uniform that\nyou will see every spender, whether it's CSPs, sovereigns, enterprises, all\nbuild out? Should we expect some periods of digestion in between? Just what\nare your customer discussions telling you about how to model growth for\nnext year?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nYes, Vivek. Thanks so much for the question regarding H20. Yes, we\nrecognized $4.6 billion H20 in Q1. We were unable to ship $2.5 billion. So\nthe total for Q1 should have been $7 billion. When we look at our Q2, our\nQ2 is going to be meaningfully down in terms of China data center revenue,\nand we had highlighted in terms of the amount of orders that we had planned\nfor H20 in Q2, and that was $8 billion.\n\n\nNow going forward, we did have other orders going forward that we will not\nbe able to fulfill. That is what was incorporated, therefore, in the amount\nthat we wrote down of the $4.5 billion. That write-down was about inventory\nand purchase commitments and our purchase commitments were about what we\nexpected regarding the orders that we had received. Going forward, though,\nit's a bigger issue regarding the amount of the market that we will not be\nable to serve. We assess that TAM to be close to about $50 billion in the\nfuture as we don't have a product to enable for China.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nVivek, probably the best way to think through it is that AI is several\nthings. Of course, we know that AI is this incredible technology that's\ngoing to transform every industry from, of course, the way we do software\nto health care and financial services, to retail, to, I guess, every\nindustry, transportation, manufacturing. And we're at the beginning of\nthat.\n\n\nBut maybe another way to think about that is where do we need intelligence?\nWhere do we need digital intelligence? And it's in every country, it's in\nevery industry. And we know, because of that, we recognize that AI is also\nan infrastructure. It's a way of developing -- delivering a technology that\nrequires factories. And these factories produce tokens. And they, as I\nmentioned, are important to every single industry and every single country.\nAnd so on that basis, we're really at the very beginning of it because the\nadoption of this technology is really kind of in its early, early stages.\n\n\nNow we've reached an extraordinary milestone with AIs that are reasoning,\nare thinking, what people call inference time scaling. And of course, it\ncreated a whole new -- we've entered an era where inference is going to be\na significant part of the compute workload. But anyhow, it's going to be a\nnew infrastructure, and we're building it out in the cloud. The United\nStates is really the early starter and available in U.S. clouds. And this\nis our largest market, our largest installed base, and we continue to see\nthat happening.\n\n\nBut beyond that, we're going to have to -- we're going to see AI go into\nenterprise, which is on-prem. Because so much of the data is still on-prem,\naccess control is really important, it's really hard to move all of --\nevery company's data into the cloud. And so we're going to move AI into the\nenterprise. And you saw that we announced a couple of really exciting new\nproducts: our RTX Pro enterprise AI server that runs everything enterprise\nand AI; our DGX Spark and DGX Station, which is designed for developers who\nwant to work on-prem. And so enterprise AI is just taking off.\n\n\nTelcos, today, a lot of the telco infrastructure will be, in the future,\nsoftware-defined and built on AI. And so 6G is going to be built on AI. And\nthat infrastructure needs to be built out and, as I said, it's very, very\nearly stages. And then, of course, every factory today that makes things\nwill have an AI factory that sits with it. And the AI factory is going to\nbe creating AI and operating AI for the factory itself, but also to power\nthe products and the things that are made by the factory. So it's very\nclear that every car company will have AI factories. And very soon,\nthere'll be robotics companies, robot companies, and those companies will\nbe also building AIs to drive the robots. And so we're at the beginning of\nall of this build-out.\n\n\nOperator\n\nThe next question comes from C.J. Muse with Cantor Fitzgerald.\n\n\nChristopher James Muse\nCantor Fitzgerald & Co., Research Division\n\nThere have been many large GPU cluster investment announcements in the last\nmonth, and you alluded to a few of them with Saudi Arabia, the U.A.E., and\nthen also we heard from Oracle and xAI, just to name a few. So my question,\nare there others that have yet to be announced of the same kind of scale\nand magnitude? And perhaps more importantly, how are these orders impacting\nyour lead times for Blackwell and your current visibility sitting here\ntoday, almost halfway through 2025?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, we have more orders today than we did at the last time I spoke about\norders at GTC. However, we're also increasing our supply chain and building\nout our supply chain. They're doing a fantastic job. We're building it here\nonshore in the United States, but we're going to keep our supply chain\nquite busy for several many more years coming.\n\n\nAnd with respect to further announcements, I'm going to be on the road next\nweek through Europe. And just about every country needs to build out AI\ninfrastructure, and their umpteenth AI factories being planned. I think in\nthe remarks, Colette mentioned there's some 100 AI factories being built.\nThere's a whole bunch that haven't been announced. And I think the\nimportant concept here, which makes it easier to understand, is that like\nother technologies that impact literally every single industry, of course,\nelectricity was one, and it became infrastructure; of course, the\ninformation infrastructure, which we now know as the Internet, affects\nevery single industry, every country, every society, intelligence is surely\none of those things. I don't know any company, industry, country who thinks\nthat intelligence is optional. It's essential infrastructure.\n\n\nAnd so we've now digitalized intelligence. And so I think we're clearly in\nthe beginning of the build-out of this infrastructure. And every country\nwill have it. I'm certain of that. Every industry will use it, that I'm\ncertain of. And what's unique about this infrastructure is that it needs\nfactories. It's a little bit like the energy infrastructure, electricity.\nIt needs factories. We need factories to produce this intelligence. And the\nintelligence is getting more sophisticated. We were talking about earlier\nthat we had a huge breakthrough in the last couple of years with reasoning\nAI and now there are agents that reason and there's super agents that use a\nwhole bunch of tools and then there's clusters of super agents where agents\nare working with agents, solving problems.\n\n\nAnd so you could just imagine, compared to one-shot chatbots and the agents\nthat are now using AI built on these large language models, how much more\ncompute-intensive they really need to be and are. And so I think we're in\nthe beginning of the build-out. And there should be many, many more\nannouncements in the future.\n\n\nOperator\n\nYour next question comes from Ben Reitzes with Melius.\n\n\nBenjamin Alexander Reitzes\nMelius Research LLC\n\nI wanted to ask first to Colette, just a little clarification around the\nguidance and maybe putting it in a different way. The $8 billion for H20\njust seems like it's roughly $3 billion more than most people thought with\nregard to what you'd be foregoing in the second quarter. So that would mean\nthat with regard to your guidance, the rest of the business, in order to\nhit $45 billion, is doing $2 billion to $3 billion or so better. So I was\nwondering if that math made sense to you. And then in terms of the\nguidance, that would imply the non-China business is doing a bit better\nthan the Street expected. So wondering what the primary driver was there in\nyour view?\n\n\nAnd then the second part of my question, Jensen, I know you guide one\nquarter at a time. But with regard to the AI Diffusion Rule being lifted\nand this momentum with sovereign, there's been times in your history where\nyou guys have said on calls like this where you have more conviction in\nsequential growth throughout the year, et cetera. And given the unleashing\nof demand with AI diffusion being revoked and the supply chain increasing,\ndoes the environment give you more conviction in sequential growth as we go\nthroughout the year? So first one for Colette and then next one for Jensen.\n\n\n\nColette M. Kress\nExecutive VP & CFO\n\nThanks, Ben, for the question. When we look at our Q2 guidance and our\ncommentary that we provided that, had the export controls not occurred, we\nwould have had orders of about $8 billion for H20. That's correct. That was\na possibility for what we would have had in our outlook for this quarter in\nQ2. So what we also have talked about here is the growth that we've seen in\nBlackwell, Blackwell across many of our customers, as well as the growth\nthat we continue to have in terms of supply that we need for our customers.\nSo putting those together, that's where we came through with the guidance\nthat we provided.\n\n\nI'm going to turn the rest over to Jensen to see how he wants to...\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes. Thanks, Ben. I would say compared to the beginning of the year,\ncompared to GTC time frame, there are 4 positive surprises. The first\npositive surprise is the step-function demand increase of reasoning AI. I\nthink it is fairly clear now that AI is going through an exponential growth\nand reasoning AI really busted through. Concerns about hallucination or its\nability to really solve problems, I think a lot of people are crossing that\nbarrier and realizing how incredible, incredibly effective agentic AI is\nand reasoning AI is. So number one is inference reasoning and the\nexponential growth there, demand growth.\n\n\nThe second one, you mentioned AI diffusion. It's really terrific to see\nthat the AI Diffusion Rule was rescinded. President Trump wants America to\nwin. And he also realizes that we're not the only country in the race. And\nhe wants the United States to win and recognizes that we have to get the\nAmerican stack out to the world and have the world build on top of American\nstacks instead of alternatives. And so AI diffusion happened, the\nrescinding of it happened at almost precisely the time that countries\naround the world are awakening to the importance of AI as an\ninfrastructure, not just as a technology of great curiosity and great\nimportance, but infrastructure for their industries and start-ups and\nsociety. Just as they had to build out infrastructure for electricity and\nInternet, you got to build out infrastructure for AI. I think that, that's\nan awakening, and that creates a lot of opportunity.\n\n\nThe third is enterprise AI. Agents work. And agents are doing -- these\nagents are really quite successful. Much more than generative AI, agentic\nAI is game changing. Agents can understand ambiguous and rather implicit\ninstructions and are able to problem solve and use tools and have memory\nand so on. And so I think this is -- enterprise AI is ready to take off.\nAnd it's taken us a few years to build a computing system that is able to\nintegrate, run enterprise AI stacks -- run enterprise IT stacks, but add AI\nto it. And this is the RTX Pro enterprise server that we announced at\nCOMPUTEX just last week. And just about every major IT company has joined\nus and super excited about that. And so computing is one stack, one part of\nit. But remember, enterprise IT is really 3 pillars. It's compute, storage\nand networking. And we've now put all 3 of them together for finally, and\nwe're going to market with that.\n\n\nAnd then lastly, industrial AI. Remember, one of the implications of the\nworld reordering, if you will, is regions onshoring manufacturing and\nbuilding plants everywhere. In addition to AI factories, of course, there\nare new electronics manufacturing, chip manufacturing being built around\nthe world. And all of these new plants and these new factories are creating\nexactly the right time when Omniverse and AI and all the work that we're\ndoing with robotics is emerging. And so this fourth pillar is quite\nimportant.\n\n\nEvery factory will have an AI factory associated with it. And in order to\ncreate these physical AI systems, you really have to train a vast amount of\ndata. So back to more data, more training, more AIs to be created, more\ncomputers. And so these 4 drivers are really kicking into turbocharge.\n\n\nOperator\n\nYour next question comes from Timothy Arcuri with UBS.\n\n\nTimothy Michael Arcuri\nUBS Investment Bank, Research Division\n\nJensen, I wanted to ask about China. It sounds like the July guidance\nassumes there's no SKU replacement for the H20. But if the President wants\nthe U.S. to win, it seems like you're going to have to be allowed to ship\nsomething into China. So I guess I had 2 points on that. First of all, have\nyou been approved to ship a new modified version into China and you're\ncurrently building it, but you just can't ship it in fiscal Q2? And then\nyou were sort of run-rating $7 billion to $8 billion a quarter into China.\nCan we get back to those sorts of quarterly run rates once you get\nsomething that you're allowed to ship back into China? I think we're all\ntrying to figure out how much to add back to our models and when. So\nwhatever you can say there would be great.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThe President has a plan. He has a vision, and I trust him. With respect to\nour export controls, it's a set of limits. And the new set of limits pretty\nmuch make it impossible for us to reduce Hopper any further for any\nproductive use. And so the new limits, it's kind of the end of the road for\nhopper. We have limited options. And so we just -- the key is to understand\nthe limits. The key is to understand the limits and see if we can come up\nwith interesting products that could continue to serve the Chinese market.\nWe don't have anything at the moment, but we're considering it. We're\nthinking about it. Obviously, the limits are quite stringent at the moment.\nAnd we have nothing to announce today. And when the time comes, we'll\nengage the administration and discuss that.\n\n\nOperator\n\nYour final question comes from the line of Aaron Rakers with Wells Fargo.\n\n\nJacob Michael Wilhelm\nWells Fargo Securities, LLC, Research Division\n\nThis is Jake on for Aaron. Congrats on the great quarter. I was wondering\nif you could give some additional color around the strength you saw within\nthe networking business, particularly around the adoption of your Ethernet\nsolutions at CSPs as well as any change you're seeing in network attach\nrates.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes. Thank you for that. We now have 3 networking platforms, maybe 4. The\nfirst one is the scale-up platform to turn a computer into a much larger\ncomputer. Scaling up is incredibly hard to do. Scaling out is easier to do,\nbut scaling up is hard to do. And that platform is called NVLink. And\nNVLink comes with it chips and switches and NVLink spines. It's really\ncomplicated. But anyways, that's our new platform, scale-up platform.\n\n\nIn addition to InfiniBand, we also have Spectrum-X. We've been fairly\nconsistent that Ethernet was designed for a lot of traffic that are\nindependent. But in the case of AI, you have a lot of computers working\ntogether. And the traffic of AI is insanely bursty. Latency matters a lot\nbecause the AI is thinking and it wants to get work done as quickly as\npossible, and you've got a whole bunch of nodes working together.\n\n\nAnd so we enhanced Ethernet, added capabilities like extremely low latency,\ncongestion control, adaptive routing, the type of technologies that were\navailable only in InfiniBand to Ethernet. And as a result, we improved the\nutilization of Ethernet in these clusters, these clusters are gigantic,\nfrom as low as 50% to as high as 85%, 90%. And so the difference is, if you\nhad a cluster that's $10 billion, and you improved its effectiveness by\n40%, that's worth $4 billion. It's incredible. And so Spectrum-X has been\nreally, quite frankly, a home run. And this last quarter, as we said in the\nprepared remarks, we added 2 very significant CSPs to the Spectrum-X\nadoption.\n\n\nAnd then the last one is BlueField, which is our control plane. And so in\nthose 4 -- the control plane of the network, which is used for storage,\nit's used for security. And for many of these clusters that want to achieve\nisolation among its users, multi-tenant clusters and still be able to use\nand have extremely high-performance bare-metal performance, BlueField is\nideal for that and is used in a lot of these cases. And so we have these 4\nnetworking platforms. They're all growing, and we're doing really well. I'm\nvery proud of the team.\n\n\nOperator\n\nThat is all the time we have for questions. Jensen, I will turn the call\nback to you.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThank you. This is the start of a powerful new wave of growth. Grace\nBlackwell is in full production. We're off to the races. We now have\nmultiple significant growth engines. Inference, one's delighted workload is\nsurging with revenue-generating AI services. AI is growing faster and will\nbe larger than any platform shifts before, including the Internet, mobile\nand cloud. Blackwell is built to power the full AI life cycle from training\nfrontier models to running complex inference and reasoning agents at scale.\nTraining demand continues to rise with breakthroughs in post-training and\nlike reinforcement learning and synthetic data generation, but inference is\nexploding. Reasoning AI agents require orders of magnitude more compute.\n\n\nThe foundations of our next growth platforms are in place and ready to\nscale. Sovereign AI, nations are investing in AI infrastructure like they\nonce did for electricity and Internet. Enterprise AI, AI must be deployable\non-prem and integrated with existing IT. Our RTX Pro, DGX Spark and DGX\nStation enterprise AI systems are ready to modernize the $500 billion IT\ninfrastructure on-prem or in the cloud. Every major IT provider is\npartnering with us. Industrial AI from training to digital twin simulation\nto deployment, NVIDIA Omniverse and Isaac GR00T are powering next-\ngeneration factories and humanoid robotic systems worldwide.\n\n\nThe age of AI is here from AI infrastructures, inference at scale,\nsovereign AI, enterprise AI and industrial AI. NVIDIA is ready.\n\n\nJoin us at GTC Paris. I'll keynote at VivaTech on June 11, talking about\nquantum GPU computing, robotic factories and robots and celebrate our\npartnerships building AI factories across the region. The NVIDIA Band will\ntour France, the U.K., Germany and Belgium.\n\n\nThank you for joining us at the earnings call today. See you in Paris.\n\n\nOperatorThis concludes today's conference call. You may now disconnect.\nCopyright  2025 by S&P Global Market Intelligence, a division of S&P\nGlobal Inc. All rights reserved.\n\n\nThese materials have been prepared solely for information purposes based\nupon information generally available to the public and from sources\nbelieved to be reliable. No content (including index data, ratings, credit-\nrelated analyses and data, research, model, software or other application\nor output therefrom) or any part thereof (Content) may be modified, reverse\nengineered, reproduced or distributed in any form by any means, or stored\nin a database or retrieval system, without the prior written permission of\nS&P Global Market Intelligence or its affiliates (collectively, S&P\nGlobal). The Content shall not be used for any unlawful or unauthorized\npurposes. S&P Global and any third-party providers, (collectively S&P\nGlobal Parties) do not guarantee the accuracy, completeness, timeliness or\navailability of the Content. S&P Global Parties are not responsible for any\nerrors or omissions, regardless of the cause, for the results obtained from\nthe use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P\nGLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS\nFOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR\nDEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE\nCONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no\nevent shall S&P Global Parties be liable to any party for any direct,\nindirect, incidental, exemplary, compensatory, punitive, special or\nconsequential damages, costs, expenses, legal fees, or losses (including,\nwithout limitation, lost income or lost profits and opportunity costs or\nlosses caused by negligence) in connection with any use of the Content even\nif advised of the possibility of such damages. S&P Global Market\nIntelligence's opinions, quotes and credit-related and other analyses are\nstatements of opinion as of the date they are expressed and not statements\nof fact or recommendations to purchase, hold, or sell any securities or to\nmake any investment decisions, and do not address the suitability of any\nsecurity. S&P Global Market Intelligence may provide index data. Direct\ninvestment in an index is not possible. Exposure to an asset class\nrepresented by an index is available through investable instruments based\non that index. S&P Global Market Intelligence assumes no obligation to\nupdate the Content following publication in any form or format. The Content\nshould not be relied on and is not a substitute for the skill, judgment and\nexperience of the user, its management, employees, advisors and/or clients\nwhen making investment and other business decisions. S&P Global Market\nIntelligence does not act as a fiduciary or an investment advisor except\nwhere registered as such. S&P Global keeps certain activities of its\ndivisions separate from each other in order to preserve the independence\nand objectivity of their respective activities. As a result, certain\ndivisions of S&P Global may have information that is not available to other\nS&P Global divisions. S&P Global has established policies and procedures to\nmaintain the confidentiality of certain nonpublic information received in\nconnection with each analytical process.\n\n\nS&P Global may receive compensation for its ratings and certain analyses,\nnormally from issuers or underwriters of securities or from obligors. S&P\nGlobal reserves the right to disseminate its opinions and analyses. S&P\nGlobal's public ratings and analyses are made available on its Web sites,\nwww.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and\nwww.globalcreditportal.com (subscription), and may be distributed through\nother means, including via S&P Global publications and third-party\nredistributors. Additional information about our ratings fees is available\nat www.standardandpoors.com/usratingsfees.\n 2025 S&P Global Market Intelligence.\n\n",
  "presentation_text": "Operator\n\nGood afternoon. My name is Sarah, and I will be your conference operator\ntoday. At this time, I would like to welcome everyone to NVIDIA's First\nQuarter Fiscal 2026 Financial Results Conference Call. [Operator\nInstructions]\n\n\nToshiya Hari, you may begin your conference.\n\n\nToshiya  Hari\nVice President of Investor Relations & Strategic Finance\n\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference\ncall for the first quarter of fiscal 2026. With me today from NVIDIA are\nJensen Huang, President and Chief Executive Officer; and Colette Kress,\nExecutive Vice President and Chief Financial Officer.\n\n\nI'd like to remind you that our call is being webcast live on NVIDIA's\nInvestor Relations website. The webcast will be available for replay until\nthe conference call to discuss our financial results for the second quarter\nof fiscal 2026.\n\n\nThe content of today's call is NVIDIA's property. It cannot be reproduced\nor transcribed without our prior written consent.\n\n\nDuring this call, we may make forward-looking statements based on current\nexpectations. These are subject to a number of significant risks and\nuncertainties, and our actual results may differ materially. For a\ndiscussion of factors that could affect our future financial results and\nbusiness, please refer to the disclosure in today's earnings release, our\nmost recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-\nK with the Securities and Exchange Commission. All our statements are made\nas of today, May 28, 2025, based on information currently available to us.\nExcept as required by law, we assume no obligation to update any such\nstatements.\n\n\nDuring this call, we will discuss non-GAAP financial measures. You can find\na reconciliation of these non-GAAP financial measures to GAAP financial\nmeasures in our CFO commentary, which is posted on our website.\n\n\nWith that, let me turn the call over to Colette.\n\n\nColette M. Kress\nExecutive VP & CFO\n\nThank you, Toshiya. We delivered another strong quarter with revenue of $44\nbillion, up 69% year-over-year, exceeding our outlook in what proved to be\na challenging operating environment. Data Center revenue of $39 billion\ngrew 73% year-on-year. AR workloads have transitioned strongly to inference\nand AI factory build-outs are driving significant revenue. Our customers'\ncommitments are firm.\n\n\nOn April 9, the U.S. government issued new export controls on H20, our data\ncenter GPU designed specifically for the China market. We sold H20 with the\napproval of the previous administration. Although our H20 has been in the\nmarket for over a year and does not have a market outside of China, the new\nexport controls on H20 did not provide a grace period to allow us to sell\nthrough our inventory. In Q1, we recognized $4.6 billion in H20 revenue,\nwhich occurred prior to April 9, but also recognized a $4.5 billion charge\nas we wrote down inventory and purchase obligations tied to orders we had\nreceived prior to April 9.\n\n\nWe were unable to ship $2.5 billion in H20 revenue in the first quarter due\nto the new export controls. The $4.5 billion charge was less than what we\ninitially anticipated as we were able to reuse certain materials. We are\nstill evaluating our limited options to supply Data Center compute products\ncompliant with the U.S. government's revised export control rules. Losing\naccess to the China AI accelerator market, which we believe will grow to\nnearly $50 billion, would have a material adverse impact on our business\ngoing forward and benefit our foreign competitors in China and worldwide.\n\n\nOur Blackwell ramp, the fastest in our company's history, drove a 73% year-\non-year increase in Data Center revenue. Blackwell contributed nearly 70%\nof Data Center compute revenue in the quarter with the transition from\nHopper nearly complete.\n\n\nThe introduction of GB200 NVL was a fundamental architectural change to\nenable data center-scale workloads and to achieve the lowest cost per\ninference token. While these systems are complex to build, we have seen a\nsignificant improvement in manufacturing yields, and rack shipments are\nmoving to strong rates to end customers. GB200 NVL racks are now generally\navailable for motor builders, enterprises and sovereign customers to\ndevelop and deploy AI.\n\n\nOn average, major hyperscalers are each deploying nearly 1,000 NVL72 racks\nor 72,000 Blackwell GPUs per week and are on track to further ramp output\nthis quarter. Microsoft, for example, has already deployed tens of\nthousands of Blackwell GPUs and is expected to ramp to hundreds of\nthousands of GB200s with OpenAI as one of its key customers. Key learnings\nfrom the GB200 ramp will allow for a smooth transition to the next phase of\nour product road map, Blackwell Ultra.\n\n\nSampling of GB300 systems began earlier this month at the major CSPs, and\nwe expect production shipments to commence later this quarter. GB300 will\nleverage the same architecture, same physical footprint and the same\nelectrical and mechanical specifications as GB200. The GB300 drop-in design\nwill allow CSPs to seamlessly transition their systems and manufacturing\nused for GB200 while maintaining high yields. B300 GPUs with 50% more HBM\nwill deliver another 50% increase in dense FP4 inference compute\nperformance compared to the B200.\n\n\nWe remain committed to our annual product cadence with our road map\nextending through 2028, tightly aligned with the multiple year planning\ncycles of our customers.\n\n\nWe are witnessing a sharp jump in inference demand. OpenAI, Microsoft and\nGoogle are seeing a step-function leap in token generation. Microsoft\nprocessed over 100 trillion tokens in Q1, a fivefold increase on a year-\nover-year basis. This exponential growth in Azure OpenAI is representative\nof strong demand for Azure AI foundry as well as other AI services across\nMicrosoft's platform.\n\n\nInference serving startups are now serving models using B200, tripling\ntheir token generation rate and corresponding revenues for high-value\nreasoning models such as DeepSeek-R1 as reported by artificial analysis.\n\n\nNVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by\n30x for the new reasoning models sweeping the industry. Developer\nengagements increased, with adoption ranging from LLM providers such as\nPerplexity to financial services institutions such as Capital One, who\nreduced agentic chatbot latency by 5x with Dynamo.\n\n\nIn the latest MLPerf inference results, we submitted our first results\nusing GB200 NVL72, delivering up to 30x higher inference throughput\ncompared to our 8-GPU 200 submission on the challenging Llama 3.1\nbenchmark. This feat was achieved through a combination of tripling the\nperformance per GPU as well as 9x more GPUs all connected on a single\nNVLink domain.\n\n\nAnd while Blackwell is still early in its life cycle, software\noptimizations have already improved its performance by 1.5x in the last\nmonth alone. We expect to continue improving the performance of Blackwell\nthrough its operational life as we have done with Hopper and Ampere. For\nexample, we increased the inference performance of Hopper by 4x over 2\nyears. This is the benefit of NVIDIA's programmable CUDA architecture and\nrich ecosystem.\n\n\nThe pace and scale of AI factory deployments are accelerating with nearly\n100 NVIDIA-powered AI factories in flight this quarter, a twofold increase\nyear-over-year, with the average number of GPUs powering each factory also\ndoubling in the same period. And more AI factory projects are starting\nacross industries and geographies. NVIDIA's full-stack architecture is\nunderpinning AI factory deployments as industry leaders like AT&T, BYD,\nCapital One, Foxconn, MediaTek, and Telenor, are strategically vital\nsovereign clouds like those recently announced in Saudi Arabia, Taiwan and\nthe U.A.E. We have a line of sight to projects requiring tens of gigawatts\nof NVIDIA AI infrastructure in the not-too-distant future.\n\n\nThe transition from generative to agentic AI, AI capable of perceiving,\nreasoning, planning and acting will transform every industry, every company\nand country. We envision AI agents as a new digital workforce capable of\nhandling tasks ranging from customer service to complex decision-making\nprocesses.\n\n\nWe introduced the Llama Nemotron family of open reasoning models designed\nto supercharge agentic AI platforms for enterprises. Built on the Llama\narchitecture, these models are available as NIMs, or NVIDIA inference\nmicroservices, with multiple sizes to meet diverse deployment needs. Our\npost-training enhancements have yielded a 20% accuracy boost and a 5x\nincrease in inference speed. Leading platform companies, including\nAccenture, Cadence, Deloitte, and Microsoft are transforming work with our\nreasoning models.\n\n\nNVIDIA NeMo microservices are generally available across industries that\nare being leveraged by leading enterprises to build, optimize and scale AI\napplications. With NeMo, Cisco increased model accuracy by 40% and improved\nresponse time by 10x in its code assistant. NASDAQ realized a 30%\nimprovement in accuracy and response time in its AI platform's search\ncapabilities. And Shell's custom LLM achieved a 30% increase in accuracy\nwhen trained with NVIDIA NeMo. NeMo's parallelism techniques accelerated\nmodel training time by 20% when compared to other frameworks.\n\n\nWe also announced a partnership with Yum! Brands, the world's largest\nrestaurant company to bring NVIDIA AI to 500 of its restaurants this year\nand expanding to 61,000 restaurants over time to streamline order-taking,\noptimize operations and enhance service across its restaurants.\n\n\nFor AI-powered cybersecurity, leading companies like Check Point,\nCrowdStrike and Palo Alto Networks are using NVIDIA's AI security and\nsoftware stack to build, optimize and secure agentic workflows, with\nCrowdStrike realizing 2x faster detection triage with 50% less compute\ncost.\n\n\nMoving to networking. Sequential growth in networking resumed in Q1 with\nrevenue up 64% quarter-over-quarter to $5 billion. Our customers continue\nto leverage our platform to efficiently scale up and scale out AI factory\nworkloads.\n\n\nWe created the world's fastest switch, NVLink, for scale up. Our NVLink\ncompute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen\n5. NVLink 72 carries 130 terabytes per second of bandwidth in a single\nrack, equivalent to the entirety of the world's peak Internet traffic.\nNVLink is a new growth vector and is off to a great start with Q1 shipments\nexceeding $1 billion.\n\n\nAt COMPUTEX, we announced NVLink Fusion. Hyperscale customers can now build\nsemi-custom CCUs and accelerators that connect directly to the NVIDIA\nplatform with NVLink. We are now enabling key partners, including ASIC\nproviders such as MediaTek, Marvell, Alchip Technologies and Astera Labs as\nwell as CPU suppliers such as Fujitsu and Qualcomm, to leverage and relink\nFusion to connect our respective ecosystems.\n\n\nFor scale out, our enhanced Ethernet offerings deliver the highest\nthroughput, lowest latency networking for AI. Spectrum-X posted strong\nsequential and year-on-year growth and is now annualizing over $8 billion\nin revenue. Adoption is widespread across major CSPs and consumer Internet\ncompanies, including CoreWeave, Microsoft Azure and Oracle Cloud and xAI.\nThis quarter, we added Google Cloud and Meta to the growing list of\nSpectrum-X customers.\n\n\nWe introduced Spectrum-X and Quantum-X silicon photonics switches,\nfeaturing the world's most advanced co-packaged optics. These platforms\nwill enable next-level AI factory scaling to millions of GPUs through the\nincreasingly power efficiency by 3.5x and network resiliency by 10x, while\naccelerating customer time to market by 1.3x.\n\n\nTransitioning to a quick summary of our revenue by geography. China as a\npercentage of our Data Center revenue was slightly below our expectations\nand down sequentially due to H20 export licensing controls. For Q2, we\nexpect a meaningful decrease in China data center revenue. As a reminder,\nwhile Singapore represented nearly 20% of our Q1 billed revenue as many of\nour large customers use Singapore for centralized invoicing, our products\nare almost always shipped elsewhere. Note that over 99% of H100, H200, and\nBlackwell Data Center compute revenue billed to Singapore was for orders\nfrom U.S.-based customers.\n\n\nMoving to gaming and AI PCs. Gaming revenue was a record $3.8 billion,\nincreasing 48% sequentially and 42% year-on-year. Strong adoption by\ngamers, creators and AI enthusiasts have made Blackwell our fastest ramp\never. Against the backdrop of robust demand, we greatly improved our supply\nand availability in Q1 and expect to continue these efforts in Q2.\n\n\nAI is transforming PC and creator and gamers. With a 100 million user\ninstalled base, GeForce represents the largest footprint for PC developers.\nThis quarter, we added to our AI PC laptop offerings, including models\ncapable of running Microsoft's CoPilot+. This past quarter, we brought\nBlackwell architecture to mainstream gaming with its launch of GeForce RTX\n5060 and 5060 Ti, starting at just $299. The RTX 5060 also debuted in\nlaptops, starting at $1,099. These systems doubled the frame rate and\nslashed latency. These GeForce RTX 5060 and 5060 Ti desktop GPUs and\nlaptops are now available.\n\n\nIn console gaming, the recently unveiled Nintendo Switch 2 leverages\nNVIDIA's neural rendering and AI technologies, including next-generation\ncustom RTX GPUs with DLSS technology to deliver a giant leap in gaming\nperformance to millions of players worldwide. Nintendo has shipped over 150\nmillion switch consoles to date, making it one of the most successful\ngaming systems in history.\n\n\nMoving to Pro Visualization. Revenue of $509 million was flat sequentially\nand up 19% year-on-year. Tariff-related uncertainty temporarily impacted Q1\nsystems and demand for our AI workstations is strong, and we expect\nsequential revenue growth to resume in Q2.\n\n\nNVIDIA DGX Spark and Station revolutionized personal computing. By putting\nthe power of an AI supercomputer in a desktop form factor. DGX Spark\ndelivers up to 1 petaflop of AI compute while DGX Station offers an\nincredible 20 petaflops and is powered by the GB300 Superchip. DGX Spark\nwill be available in calendar Q3 and DGX Station later this year.\n\n\nWe have deepened Omniverse's integration and adoption into some of the\nworld's leading software platforms, including Databricks, SAP and Schneider\nElectric. New Omniverse Blueprint such as Mega for at-scale robotic fleet\nmanagement are being leveraged in KION Group, Pegatron, Accenture and other\nleading companies to enhance industrial operations.\n\n\nAt COMPUTEX, we showcased Omniverse's great traction with technology\nmanufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron. Using\nOmniverse, TSMC saves months in work by designing fabs virtually, Foxconn\naccelerates thermal simulations by 150x, and Pegatron reduced assembly line\ndefect rates by 67%.\n\n\nLastly, with our Automotive group. Revenue was $567 million, down 1%\nsequentially but up 72% year-on-year. Year-on-year growth was driven by the\nramp of self-driving across a number of customers and robust end demand for\nNEVs. We are partnering with GM to build the next-gen vehicles, factories\nand robots using NVIDIA AI, simulation and accelerated computing. And we\nare now in production with our full-stack solution for Mercedes-Benz\nstarting with the new CLA, hitting roads in the next few months.\n\n\nWe announced Isaac GR00T N1, the world's first open fully customizable\nfoundation model for humanoid robots, enabling generalized reasoning and\nskill development. We also launched new open NVIDIA Cosmos World Foundation\nmodels. Leading companies include 1X, Agility Robotics, Figure AI, Uber and\nWaabi. We've begun integrating Cosmos into their operations for synthetic\ndata generation, while Agility Robotics, Boston Dynamics, and XPENG\nRobotics are harnessing Isaac's simulation to advance their humanoid\nefforts. GE Healthcare is using the new NVIDIA Isaac platform for health\ncare simulation built on NVIDIA Omniverse and using NVIDIA Cosmos for\nplatform speed, development of robotic imaging and surgery systems.\n\n\nThe era of robotics is here, billions of robots, hundreds of millions of\nautonomous vehicles and hundreds of thousands of robotic factories and\nwarehouses will be developed.\n\n\nAll right. Moving to the rest of the P&L. GAAP gross margins and non-GAAP\ngross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion\ncharge, Q1 non-GAAP gross margins would have been 71.3%, slightly above our\noutlook at the beginning of the quarter. Sequentially, GAAP operating\nexpenses were up 7% and non-GAAP operating expenses were up 6%, reflecting\nhigher compensation and employee growth. Our investments include expanding\nour infrastructure capabilities and AI solutions, and we plan to grow these\ninvestments throughout the fiscal year.\n\n\nIn Q1, we returned a record $14.3 billion to shareholders in the form of\nshare repurchases and cash dividends. Our capital return program continues\nto be a key element of our capital allocation strategy.\n\n\nLet me turn to the outlook for the second quarter. Total revenue is\nexpected to be $45 billion, plus or minus 2%. We expect modest sequential\ngrowth across all of our platforms. In Data Center, we anticipate the\ncontinued ramp of Blackwell to be partially offset by a decline in China\nrevenue. Note, our outlook reflects a loss in H20 revenue of approximately\n$8 billion for the second quarter.\n\n\nGAAP and non-GAAP gross margins are expected to be 71.8% and 72%,\nrespectively, plus or minus 50 basis points. We expect better Blackwell\nprofitability to drive modest sequential improvement in gross margins. We\nare continuing to work towards achieving gross margins in the mid-70s range\nlate this year.\n\n\nGAAP and non-GAAP operating expenses are expected to be approximately $5.7\nbillion and $4 billion, respectively, and we continue to expect full year\nfiscal year '26 operating expense growth to be in the mid-30% range. GAAP\nand non-GAAP other income and expenses are expected to be an income of\napproximately $450 million, excluding gains and losses from nonmarketable\nand publicly held equity securities. GAAP and non-GAAP tax rates are\nexpected to be 16.5%, plus or minus 1%, excluding any discrete items.\n\n\nFurther financial details are included in the CFO commentary and other\ninformation available on our IR website, including a new financial\ninformation AI agent.\n\n\nLet me highlight upcoming events for the financial community. We will be at\nthe BofA Global Technology Conference in San Francisco on June 4. The\nRosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on\nJune 10, and GTC Paris at VivaTech on June 11 in Paris. We look forward to\nseeing you at these events.\n\n\nOur earnings call to discuss the results of our second quarter of fiscal\n2026 is scheduled for August 27.\n\n\nWell, now let me turn it over to Jensen to make some remarks.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThanks, Colette. We've had a busy and productive year. Let me share my\nperspective on some topics we're frequently asked.\n\n\nOn export control, China is one of the world's largest AI markets and a\nspringboard to global success. With half of the world's AI researchers\nbased there, the platform that wins China is positioned to lead globally.\nToday, however, the $50 billion China market is effectively closed to U.S.\nindustry. The H20 export ban ended our Hopper Data Center business in\nChina. We cannot reduce Hopper further to comply. As a result, we are\ntaking a multibillion-dollar write-off on inventory that cannot be sold or\nrepurposed. We are exploring limited ways to compete, but Hopper is no\nlonger an option.\n\n\nChina's AI moves on with or without U.S. chips. It has the compute to train\nand deploy advanced models. The question is not whether China will have AI,\nit already does. The question is whether one of the world's largest AI\nmarkets will run on American platforms. Shielding Chinese chip makers from\nU.S. competition only strengthens them abroad and weakens America's\nposition. Export restrictions have spurred China's innovation and scale.\n\n\nThe AI race is not just about chips. It's about which stack the world runs\non. As that stack grows to include 6G and quantum, U.S. global\ninfrastructure leadership is at stake. The U.S. has based its policy on the\nassumption that China cannot make AI chips. That assumption was always\nquestionable, and now it's clearly wrong. China has enormous manufacturing\ncapability. In the end, the platform that wins the AI developers win AI\nwins AI. Export controls should strengthen U.S. platforms, not drive half\nof the world's AI talent to rivals.\n\n\nOn DeepSeek, DeepSeek and Qwen from China are among the most -- among the\nbest open source AI models. Released freely, they've gained traction across\nthe U.S., Europe and beyond. DeepSeek-R1, like ChatGPT, introduce reasoning\nAI that produces better answers the longer it thinks. Reasoning AI enables\nstep-by-step problem-solving, planning and tool use, turning models into\nintelligent agents.\n\n\nReasoning is compute-intensive, requires hundreds to thousands more --\nthousands of times more tokens per task than previous one-shot inference.\nReasoning models are driving a step-function surge in inference demand. AI\nscaling laws remain firmly intact, not only for training, but now inference\ntoo requires massive scale compute.\n\n\nDeepSeek also underscores the strategic value of open source AI. When\npopular models are trained and optimized on U.S. platforms, it drives\nusage, feedback and continuous improvement, reinforcing American leadership\nacross the stack. U.S. platforms must remain the preferred platform for\nopen source AI. That means supporting collaboration with top developers\nglobally, including in China. America wins when models like DeepSeek and\nQwen runs best on American infrastructure.\n\n\nRegarding onshore manufacturing, President Trump has outlined a bold vision\nto reshore advanced manufacturing, create jobs and strengthen national\nsecurity. Future plants will be highly computerized in robotics. We share\nthis vision. TSMC is building 6 fabs and 2 advanced packaging plants in\nArizona to make chips for NVIDIA. Process qualification is underway with\nvolume production expected by year-end. SPIL and Amkor are also investing\nin Arizona, constructing packaging, assembly and test facilities. In\nHouston, we're partnering with Foxconn to construct a 1 million square foot\nfactory to build AI supercomputers. Wistron is building a similar plant in\nFort Worth, Texas. To encourage and support these investments, we've made\nsubstantial long-term purchase commitments, a deep investment in America's\nAI manufacturing future.\n\n\nOur goal from chip to supercomputer built in America within a year. Each\nGB200 NVLink 72 racks contains 1.2 million components and weighs nearly 2\ntons. No one has produced supercomputers on this scale. Our partners are\ndoing an extraordinary job.\n\n\nOn AI Diffusion Rule, President Trump rescinded the AI Diffusion Rule,\ncalling it counterproductive, and proposed a new policy to promote U.S. AI\ntech with trusted partners. On his Middle East tour, he announced historic\ninvestments. I was honored to join him in announcing a 500-megawatt AI\ninfrastructure project in Saudi Arabia and a 5-gigawatt AI campus in the\nU.A.E. President Trump wants U.S. tech to lead. The deals he announced are\nwins for America, creating jobs, advancing infrastructure, generating tax\nrevenue and reducing the U.S. trade deficit. The U.S. will always be\nNVIDIA's largest market and home to the largest installed base of our\ninfrastructure.\n\n\nEvery nation now sees AI as core to the next industrial revolution, a new\nindustry that produces intelligence and essential infrastructure for every\neconomy. Countries are racing to build national AI platforms to elevate\ntheir digital capabilities. At COMPUTEX, we announced Taiwan's first AI\nfactory in partnership with Foxconn and the Taiwan government. Last week, I\nwas in Sweden to launch its first national AI infrastructure. Japan, Korea,\nIndia, Canada, France, the U.K., Germany, Italy, Spain and more are now\nbuilding national AI factories to empower start-ups, industries and\nsocieties. Sovereign AI is a new growth engine for NVIDIA.\n\n\nToshiya, back to you. Thank you.\n\n\nToshiya  Hari\nVice President of Investor Relations & Strategic FinanceOperator, we will\nnow open the call for questions. Would you please poll for questions?",
  "qa_text": "Operator\n\n[Operator Instructions] Your first question comes from the line of Joe\nMoore with Morgan Stanley.\n\n\nJoseph Lawrence Moore\nMorgan Stanley, Research Division\n\nYou guys have talked about this scaling up of inference around reasoning\nmodels for at least a year now. And we've really seen that come to fruition\nas you talked about. We've heard it from your customers. Can you give us a\nsense for how much of that demand you're able to serve? And give us a sense\nfor maybe how big the inference business is for you guys. And do we need\nfull on NVL72 rack scale solutions for reasoning inference going forward?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, we would like to serve all of it. And I think we're on track to serve\nmost of it. Grace Blackwell NVLink 72 is the ideal engine today, the ideal\ncomputer thinking machine, if you will, for reasoning AI. There's a couple\nof reasons for that. The first reason is that the token generation amount,\nthe number of tokens reasoning goes through, is 100, 1,000x more than a one-\nshot chatbot. It's essentially thinking to itself, breaking down a problem\nstep by step. It might be planning multiple paths to an answer. It could be\nusing tools, reading PDFs, reading web pages, watching videos and then\nproducing a result, an answer. The longer it thinks, the better the answer,\nthe smarter the answer is. And so what we would like to do and the reason\nwhy Grace Blackwell was designed to give such a giant step-up in inference\nperformance is so that you could do all this and still get a response as\nquickly as possible.\n\n\nCompared to Hopper, Grace Blackwell is some 40x higher speed and\nthroughput, compared. And so this is going to be a huge, huge benefit in\ndriving down the cost while improving the quality of response with\nexcellent quality of service at the same time. So that's the fundamental\nreason. That was the core driving reason for Grace Blackwell and NVLink 72.\nOf course, in order to do that, we had to reinvent, literally redesign the\nentire way that these supercomputers are built. But now we're in full\nproduction. It's going to be exciting. It's going to be incredibly\nexciting.\n\n\nOperator\n\nThe next question comes from Vivek Arya with Bank of America Securities.\n\n\nVivek  Arya\nBofA Securities, Research Division\n\nJust a clarification for Colette first. So on the China impact, I think\npreviously, it was mentioned at about $15 billion. So you had the $8\nbillion in Q2. So is there still some left as a headwind for the remaining\nquarters, just for how to model that.\n\n\nAnd then a question, Jensen, for you. Back at GTC, you had outlined a path\ntowards almost $1 trillion of AI spending over the next few years. Where\nare we in that build-out? And do you think it's going to be uniform that\nyou will see every spender, whether it's CSPs, sovereigns, enterprises, all\nbuild out? Should we expect some periods of digestion in between? Just what\nare your customer discussions telling you about how to model growth for\nnext year?\n\n\nColette M. Kress\nExecutive VP & CFO\n\nYes, Vivek. Thanks so much for the question regarding H20. Yes, we\nrecognized $4.6 billion H20 in Q1. We were unable to ship $2.5 billion. So\nthe total for Q1 should have been $7 billion. When we look at our Q2, our\nQ2 is going to be meaningfully down in terms of China data center revenue,\nand we had highlighted in terms of the amount of orders that we had planned\nfor H20 in Q2, and that was $8 billion.\n\n\nNow going forward, we did have other orders going forward that we will not\nbe able to fulfill. That is what was incorporated, therefore, in the amount\nthat we wrote down of the $4.5 billion. That write-down was about inventory\nand purchase commitments and our purchase commitments were about what we\nexpected regarding the orders that we had received. Going forward, though,\nit's a bigger issue regarding the amount of the market that we will not be\nable to serve. We assess that TAM to be close to about $50 billion in the\nfuture as we don't have a product to enable for China.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nVivek, probably the best way to think through it is that AI is several\nthings. Of course, we know that AI is this incredible technology that's\ngoing to transform every industry from, of course, the way we do software\nto health care and financial services, to retail, to, I guess, every\nindustry, transportation, manufacturing. And we're at the beginning of\nthat.\n\n\nBut maybe another way to think about that is where do we need intelligence?\nWhere do we need digital intelligence? And it's in every country, it's in\nevery industry. And we know, because of that, we recognize that AI is also\nan infrastructure. It's a way of developing -- delivering a technology that\nrequires factories. And these factories produce tokens. And they, as I\nmentioned, are important to every single industry and every single country.\nAnd so on that basis, we're really at the very beginning of it because the\nadoption of this technology is really kind of in its early, early stages.\n\n\nNow we've reached an extraordinary milestone with AIs that are reasoning,\nare thinking, what people call inference time scaling. And of course, it\ncreated a whole new -- we've entered an era where inference is going to be\na significant part of the compute workload. But anyhow, it's going to be a\nnew infrastructure, and we're building it out in the cloud. The United\nStates is really the early starter and available in U.S. clouds. And this\nis our largest market, our largest installed base, and we continue to see\nthat happening.\n\n\nBut beyond that, we're going to have to -- we're going to see AI go into\nenterprise, which is on-prem. Because so much of the data is still on-prem,\naccess control is really important, it's really hard to move all of --\nevery company's data into the cloud. And so we're going to move AI into the\nenterprise. And you saw that we announced a couple of really exciting new\nproducts: our RTX Pro enterprise AI server that runs everything enterprise\nand AI; our DGX Spark and DGX Station, which is designed for developers who\nwant to work on-prem. And so enterprise AI is just taking off.\n\n\nTelcos, today, a lot of the telco infrastructure will be, in the future,\nsoftware-defined and built on AI. And so 6G is going to be built on AI. And\nthat infrastructure needs to be built out and, as I said, it's very, very\nearly stages. And then, of course, every factory today that makes things\nwill have an AI factory that sits with it. And the AI factory is going to\nbe creating AI and operating AI for the factory itself, but also to power\nthe products and the things that are made by the factory. So it's very\nclear that every car company will have AI factories. And very soon,\nthere'll be robotics companies, robot companies, and those companies will\nbe also building AIs to drive the robots. And so we're at the beginning of\nall of this build-out.\n\n\nOperator\n\nThe next question comes from C.J. Muse with Cantor Fitzgerald.\n\n\nChristopher James Muse\nCantor Fitzgerald & Co., Research Division\n\nThere have been many large GPU cluster investment announcements in the last\nmonth, and you alluded to a few of them with Saudi Arabia, the U.A.E., and\nthen also we heard from Oracle and xAI, just to name a few. So my question,\nare there others that have yet to be announced of the same kind of scale\nand magnitude? And perhaps more importantly, how are these orders impacting\nyour lead times for Blackwell and your current visibility sitting here\ntoday, almost halfway through 2025?\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nWell, we have more orders today than we did at the last time I spoke about\norders at GTC. However, we're also increasing our supply chain and building\nout our supply chain. They're doing a fantastic job. We're building it here\nonshore in the United States, but we're going to keep our supply chain\nquite busy for several many more years coming.\n\n\nAnd with respect to further announcements, I'm going to be on the road next\nweek through Europe. And just about every country needs to build out AI\ninfrastructure, and their umpteenth AI factories being planned. I think in\nthe remarks, Colette mentioned there's some 100 AI factories being built.\nThere's a whole bunch that haven't been announced. And I think the\nimportant concept here, which makes it easier to understand, is that like\nother technologies that impact literally every single industry, of course,\nelectricity was one, and it became infrastructure; of course, the\ninformation infrastructure, which we now know as the Internet, affects\nevery single industry, every country, every society, intelligence is surely\none of those things. I don't know any company, industry, country who thinks\nthat intelligence is optional. It's essential infrastructure.\n\n\nAnd so we've now digitalized intelligence. And so I think we're clearly in\nthe beginning of the build-out of this infrastructure. And every country\nwill have it. I'm certain of that. Every industry will use it, that I'm\ncertain of. And what's unique about this infrastructure is that it needs\nfactories. It's a little bit like the energy infrastructure, electricity.\nIt needs factories. We need factories to produce this intelligence. And the\nintelligence is getting more sophisticated. We were talking about earlier\nthat we had a huge breakthrough in the last couple of years with reasoning\nAI and now there are agents that reason and there's super agents that use a\nwhole bunch of tools and then there's clusters of super agents where agents\nare working with agents, solving problems.\n\n\nAnd so you could just imagine, compared to one-shot chatbots and the agents\nthat are now using AI built on these large language models, how much more\ncompute-intensive they really need to be and are. And so I think we're in\nthe beginning of the build-out. And there should be many, many more\nannouncements in the future.\n\n\nOperator\n\nYour next question comes from Ben Reitzes with Melius.\n\n\nBenjamin Alexander Reitzes\nMelius Research LLC\n\nI wanted to ask first to Colette, just a little clarification around the\nguidance and maybe putting it in a different way. The $8 billion for H20\njust seems like it's roughly $3 billion more than most people thought with\nregard to what you'd be foregoing in the second quarter. So that would mean\nthat with regard to your guidance, the rest of the business, in order to\nhit $45 billion, is doing $2 billion to $3 billion or so better. So I was\nwondering if that math made sense to you. And then in terms of the\nguidance, that would imply the non-China business is doing a bit better\nthan the Street expected. So wondering what the primary driver was there in\nyour view?\n\n\nAnd then the second part of my question, Jensen, I know you guide one\nquarter at a time. But with regard to the AI Diffusion Rule being lifted\nand this momentum with sovereign, there's been times in your history where\nyou guys have said on calls like this where you have more conviction in\nsequential growth throughout the year, et cetera. And given the unleashing\nof demand with AI diffusion being revoked and the supply chain increasing,\ndoes the environment give you more conviction in sequential growth as we go\nthroughout the year? So first one for Colette and then next one for Jensen.\n\n\n\nColette M. Kress\nExecutive VP & CFO\n\nThanks, Ben, for the question. When we look at our Q2 guidance and our\ncommentary that we provided that, had the export controls not occurred, we\nwould have had orders of about $8 billion for H20. That's correct. That was\na possibility for what we would have had in our outlook for this quarter in\nQ2. So what we also have talked about here is the growth that we've seen in\nBlackwell, Blackwell across many of our customers, as well as the growth\nthat we continue to have in terms of supply that we need for our customers.\nSo putting those together, that's where we came through with the guidance\nthat we provided.\n\n\nI'm going to turn the rest over to Jensen to see how he wants to...\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes. Thanks, Ben. I would say compared to the beginning of the year,\ncompared to GTC time frame, there are 4 positive surprises. The first\npositive surprise is the step-function demand increase of reasoning AI. I\nthink it is fairly clear now that AI is going through an exponential growth\nand reasoning AI really busted through. Concerns about hallucination or its\nability to really solve problems, I think a lot of people are crossing that\nbarrier and realizing how incredible, incredibly effective agentic AI is\nand reasoning AI is. So number one is inference reasoning and the\nexponential growth there, demand growth.\n\n\nThe second one, you mentioned AI diffusion. It's really terrific to see\nthat the AI Diffusion Rule was rescinded. President Trump wants America to\nwin. And he also realizes that we're not the only country in the race. And\nhe wants the United States to win and recognizes that we have to get the\nAmerican stack out to the world and have the world build on top of American\nstacks instead of alternatives. And so AI diffusion happened, the\nrescinding of it happened at almost precisely the time that countries\naround the world are awakening to the importance of AI as an\ninfrastructure, not just as a technology of great curiosity and great\nimportance, but infrastructure for their industries and start-ups and\nsociety. Just as they had to build out infrastructure for electricity and\nInternet, you got to build out infrastructure for AI. I think that, that's\nan awakening, and that creates a lot of opportunity.\n\n\nThe third is enterprise AI. Agents work. And agents are doing -- these\nagents are really quite successful. Much more than generative AI, agentic\nAI is game changing. Agents can understand ambiguous and rather implicit\ninstructions and are able to problem solve and use tools and have memory\nand so on. And so I think this is -- enterprise AI is ready to take off.\nAnd it's taken us a few years to build a computing system that is able to\nintegrate, run enterprise AI stacks -- run enterprise IT stacks, but add AI\nto it. And this is the RTX Pro enterprise server that we announced at\nCOMPUTEX just last week. And just about every major IT company has joined\nus and super excited about that. And so computing is one stack, one part of\nit. But remember, enterprise IT is really 3 pillars. It's compute, storage\nand networking. And we've now put all 3 of them together for finally, and\nwe're going to market with that.\n\n\nAnd then lastly, industrial AI. Remember, one of the implications of the\nworld reordering, if you will, is regions onshoring manufacturing and\nbuilding plants everywhere. In addition to AI factories, of course, there\nare new electronics manufacturing, chip manufacturing being built around\nthe world. And all of these new plants and these new factories are creating\nexactly the right time when Omniverse and AI and all the work that we're\ndoing with robotics is emerging. And so this fourth pillar is quite\nimportant.\n\n\nEvery factory will have an AI factory associated with it. And in order to\ncreate these physical AI systems, you really have to train a vast amount of\ndata. So back to more data, more training, more AIs to be created, more\ncomputers. And so these 4 drivers are really kicking into turbocharge.\n\n\nOperator\n\nYour next question comes from Timothy Arcuri with UBS.\n\n\nTimothy Michael Arcuri\nUBS Investment Bank, Research Division\n\nJensen, I wanted to ask about China. It sounds like the July guidance\nassumes there's no SKU replacement for the H20. But if the President wants\nthe U.S. to win, it seems like you're going to have to be allowed to ship\nsomething into China. So I guess I had 2 points on that. First of all, have\nyou been approved to ship a new modified version into China and you're\ncurrently building it, but you just can't ship it in fiscal Q2? And then\nyou were sort of run-rating $7 billion to $8 billion a quarter into China.\nCan we get back to those sorts of quarterly run rates once you get\nsomething that you're allowed to ship back into China? I think we're all\ntrying to figure out how much to add back to our models and when. So\nwhatever you can say there would be great.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThe President has a plan. He has a vision, and I trust him. With respect to\nour export controls, it's a set of limits. And the new set of limits pretty\nmuch make it impossible for us to reduce Hopper any further for any\nproductive use. And so the new limits, it's kind of the end of the road for\nhopper. We have limited options. And so we just -- the key is to understand\nthe limits. The key is to understand the limits and see if we can come up\nwith interesting products that could continue to serve the Chinese market.\nWe don't have anything at the moment, but we're considering it. We're\nthinking about it. Obviously, the limits are quite stringent at the moment.\nAnd we have nothing to announce today. And when the time comes, we'll\nengage the administration and discuss that.\n\n\nOperator\n\nYour final question comes from the line of Aaron Rakers with Wells Fargo.\n\n\nJacob Michael Wilhelm\nWells Fargo Securities, LLC, Research Division\n\nThis is Jake on for Aaron. Congrats on the great quarter. I was wondering\nif you could give some additional color around the strength you saw within\nthe networking business, particularly around the adoption of your Ethernet\nsolutions at CSPs as well as any change you're seeing in network attach\nrates.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nYes. Thank you for that. We now have 3 networking platforms, maybe 4. The\nfirst one is the scale-up platform to turn a computer into a much larger\ncomputer. Scaling up is incredibly hard to do. Scaling out is easier to do,\nbut scaling up is hard to do. And that platform is called NVLink. And\nNVLink comes with it chips and switches and NVLink spines. It's really\ncomplicated. But anyways, that's our new platform, scale-up platform.\n\n\nIn addition to InfiniBand, we also have Spectrum-X. We've been fairly\nconsistent that Ethernet was designed for a lot of traffic that are\nindependent. But in the case of AI, you have a lot of computers working\ntogether. And the traffic of AI is insanely bursty. Latency matters a lot\nbecause the AI is thinking and it wants to get work done as quickly as\npossible, and you've got a whole bunch of nodes working together.\n\n\nAnd so we enhanced Ethernet, added capabilities like extremely low latency,\ncongestion control, adaptive routing, the type of technologies that were\navailable only in InfiniBand to Ethernet. And as a result, we improved the\nutilization of Ethernet in these clusters, these clusters are gigantic,\nfrom as low as 50% to as high as 85%, 90%. And so the difference is, if you\nhad a cluster that's $10 billion, and you improved its effectiveness by\n40%, that's worth $4 billion. It's incredible. And so Spectrum-X has been\nreally, quite frankly, a home run. And this last quarter, as we said in the\nprepared remarks, we added 2 very significant CSPs to the Spectrum-X\nadoption.\n\n\nAnd then the last one is BlueField, which is our control plane. And so in\nthose 4 -- the control plane of the network, which is used for storage,\nit's used for security. And for many of these clusters that want to achieve\nisolation among its users, multi-tenant clusters and still be able to use\nand have extremely high-performance bare-metal performance, BlueField is\nideal for that and is used in a lot of these cases. And so we have these 4\nnetworking platforms. They're all growing, and we're doing really well. I'm\nvery proud of the team.\n\n\nOperator\n\nThat is all the time we have for questions. Jensen, I will turn the call\nback to you.\n\n\nJen-Hsun  Huang\nCo-Founder, CEO, President & Director\n\nThank you. This is the start of a powerful new wave of growth. Grace\nBlackwell is in full production. We're off to the races. We now have\nmultiple significant growth engines. Inference, one's delighted workload is\nsurging with revenue-generating AI services. AI is growing faster and will\nbe larger than any platform shifts before, including the Internet, mobile\nand cloud. Blackwell is built to power the full AI life cycle from training\nfrontier models to running complex inference and reasoning agents at scale.\nTraining demand continues to rise with breakthroughs in post-training and\nlike reinforcement learning and synthetic data generation, but inference is\nexploding. Reasoning AI agents require orders of magnitude more compute.\n\n\nThe foundations of our next growth platforms are in place and ready to\nscale. Sovereign AI, nations are investing in AI infrastructure like they\nonce did for electricity and Internet. Enterprise AI, AI must be deployable\non-prem and integrated with existing IT. Our RTX Pro, DGX Spark and DGX\nStation enterprise AI systems are ready to modernize the $500 billion IT\ninfrastructure on-prem or in the cloud. Every major IT provider is\npartnering with us. Industrial AI from training to digital twin simulation\nto deployment, NVIDIA Omniverse and Isaac GR00T are powering next-\ngeneration factories and humanoid robotic systems worldwide.\n\n\nThe age of AI is here from AI infrastructures, inference at scale,\nsovereign AI, enterprise AI and industrial AI. NVIDIA is ready.\n\n\nJoin us at GTC Paris. I'll keynote at VivaTech on June 11, talking about\nquantum GPU computing, robotic factories and robots and celebrate our\npartnerships building AI factories across the region. The NVIDIA Band will\ntour France, the U.K., Germany and Belgium.\n\n\nThank you for joining us at the earnings call today. See you in Paris.\n\n\nOperatorThis concludes today's conference call. You may now disconnect.\nCopyright  2025 by S&P Global Market Intelligence, a division of S&P\nGlobal Inc. All rights reserved.\n\n\nThese materials have been prepared solely for information purposes based\nupon information generally available to the public and from sources\nbelieved to be reliable. No content (including index data, ratings, credit-\nrelated analyses and data, research, model, software or other application\nor output therefrom) or any part thereof (Content) may be modified, reverse\nengineered, reproduced or distributed in any form by any means, or stored\nin a database or retrieval system, without the prior written permission of\nS&P Global Market Intelligence or its affiliates (collectively, S&P\nGlobal). The Content shall not be used for any unlawful or unauthorized\npurposes. S&P Global and any third-party providers, (collectively S&P\nGlobal Parties) do not guarantee the accuracy, completeness, timeliness or\navailability of the Content. S&P Global Parties are not responsible for any\nerrors or omissions, regardless of the cause, for the results obtained from\nthe use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P\nGLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS\nFOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR\nDEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE\nCONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no\nevent shall S&P Global Parties be liable to any party for any direct,\nindirect, incidental, exemplary, compensatory, punitive, special or\nconsequential damages, costs, expenses, legal fees, or losses (including,\nwithout limitation, lost income or lost profits and opportunity costs or\nlosses caused by negligence) in connection with any use of the Content even\nif advised of the possibility of such damages. S&P Global Market\nIntelligence's opinions, quotes and credit-related and other analyses are\nstatements of opinion as of the date they are expressed and not statements\nof fact or recommendations to purchase, hold, or sell any securities or to\nmake any investment decisions, and do not address the suitability of any\nsecurity. S&P Global Market Intelligence may provide index data. Direct\ninvestment in an index is not possible. Exposure to an asset class\nrepresented by an index is available through investable instruments based\non that index. S&P Global Market Intelligence assumes no obligation to\nupdate the Content following publication in any form or format. The Content\nshould not be relied on and is not a substitute for the skill, judgment and\nexperience of the user, its management, employees, advisors and/or clients\nwhen making investment and other business decisions. S&P Global Market\nIntelligence does not act as a fiduciary or an investment advisor except\nwhere registered as such. S&P Global keeps certain activities of its\ndivisions separate from each other in order to preserve the independence\nand objectivity of their respective activities. As a result, certain\ndivisions of S&P Global may have information that is not available to other\nS&P Global divisions. S&P Global has established policies and procedures to\nmaintain the confidentiality of certain nonpublic information received in\nconnection with each analytical process.\n\n\nS&P Global may receive compensation for its ratings and certain analyses,\nnormally from issuers or underwriters of securities or from obligors. S&P\nGlobal reserves the right to disseminate its opinions and analyses. S&P\nGlobal's public ratings and analyses are made available on its Web sites,\nwww.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and\nwww.globalcreditportal.com (subscription), and may be distributed through\nother means, including via S&P Global publications and third-party\nredistributors. Additional information about our ratings fees is available\nat www.standardandpoors.com/usratingsfees.\n 2025 S&P Global Market Intelligence.",
  "has_qa": 1,
  "speaker_turns": [
    {
      "speaker": "Unknown",
      "role": "",
      "text": "NVIDIA Corporation NasdaqGS:NVDA FQ1 2026 Earnings Call Transcripts Wednesday, May 28, 2025 9:00 PM GMT S&P Global Market Intelligence Estimates Presentation"
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Good afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's First Quarter Fiscal 2026 Financial Results Conference Call. [Operator Instructions] Toshiya Hari, you may begin your conference."
    },
    {
      "speaker": "Toshiya  Hari",
      "role": "Vice President of Investor Relations & Strategic Finance",
      "text": "Vice President of Investor Relations & Strategic Finance Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026. The content of today's call is NVIDIA's property. It cannot be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8- K with the Securities and Exchange Commission. All our statements are made as of today, May 28, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette."
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO Thank you, Toshiya. We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment. Data Center revenue of $39 billion grew 73% year-on-year. AR workloads have transitioned strongly to inference and AI factory build-outs are driving significant revenue. Our customers' commitments are firm. On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market. We sold H20 with the approval of the previous administration. Although our H20 has been in the market for over a year and does not have a market outside of China, the new export controls on H20 did not provide a grace period to allow us to sell through our inventory. In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9, but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9. We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls. The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials. We are still evaluating our limited options to supply Data Center compute products compliant with the U.S. government's revised export control rules. Losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion, would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide. Our Blackwell ramp, the fastest in our company's history, drove a 73% year- on-year increase in Data Center revenue. Blackwell contributed nearly 70% of Data Center compute revenue in the quarter with the transition from Hopper nearly complete. The introduction of GB200 NVL was a fundamental architectural change to enable data center-scale workloads and to achieve the lowest cost per inference token. While these systems are complex to build, we have seen a significant improvement in manufacturing yields, and rack shipments are moving to strong rates to end customers. GB200 NVL racks are now generally available for motor builders, enterprises and sovereign customers to develop and deploy AI. On average, major hyperscalers are each deploying nearly 1,000 NVL72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter. Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers. Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product road map, Blackwell Ultra. Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commence later this quarter. GB300 will leverage the same architecture, same physical footprint and the same electrical and mechanical specifications as GB200. The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields. B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200. We remain committed to our annual product cadence with our road map extending through 2028, tightly aligned with the multiple year planning cycles of our customers. We are witnessing a sharp jump in inference demand. OpenAI, Microsoft and Google are seeing a step-function leap in token generation. Microsoft processed over 100 trillion tokens in Q1, a fivefold increase on a year- over-year basis. This exponential growth in Azure OpenAI is representative of strong demand for Azure AI foundry as well as other AI services across Microsoft's platform. Inference serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeek-R1 as reported by artificial analysis. NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry. Developer engagements increased, with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One, who reduced agentic chatbot latency by 5x with Dynamo. In the latest MLPerf inference results, we submitted our first results using GB200 NVL72, delivering up to 30x higher inference throughput compared to our 8-GPU 200 submission on the challenging Llama 3.1 benchmark. This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs all connected on a single NVLink domain. And while Blackwell is still early in its life cycle, software optimizations have already improved its performance by 1.5x in the last month alone. We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Ampere. For example, we increased the inference performance of Hopper by 4x over 2 years. This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem. The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter, a twofold increase year-over-year, with the average number of GPUs powering each factory also doubling in the same period. And more AI factory projects are starting across industries and geographies. NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor, are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan and the U.A.E. We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future. The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning and acting will transform every industry, every company and country. We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes. We introduced the Llama Nemotron family of open reasoning models designed to supercharge agentic AI platforms for enterprises. Built on the Llama architecture, these models are available as NIMs, or NVIDIA inference microservices, with multiple sizes to meet diverse deployment needs. Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed. Leading platform companies, including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models. NVIDIA NeMo microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize and scale AI applications. With NeMo, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant. NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities. And Shell's custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NeMo. NeMo's parallelism techniques accelerated model training time by 20% when compared to other frameworks. We also announced a partnership with Yum! Brands, the world's largest restaurant company to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order-taking, optimize operations and enhance service across its restaurants. For AI-powered cybersecurity, leading companies like Check Point, CrowdStrike and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize and secure agentic workflows, with CrowdStrike realizing 2x faster detection triage with 50% less compute cost. Moving to networking. Sequential growth in networking resumed in Q1 with revenue up 64% quarter-over-quarter to $5 billion. Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads. We created the world's fastest switch, NVLink, for scale up. Our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5. NVLink 72 carries 130 terabytes per second of bandwidth in a single rack, equivalent to the entirety of the world's peak Internet traffic. NVLink is a new growth vector and is off to a great start with Q1 shipments exceeding $1 billion. At COMPUTEX, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink. We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies and Astera Labs as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage and relink Fusion to connect our respective ecosystems. For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI. Spectrum-X posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue. Adoption is widespread across major CSPs and consumer Internet companies, including CoreWeave, Microsoft Azure and Oracle Cloud and xAI. This quarter, we added Google Cloud and Meta to the growing list of Spectrum-X customers. We introduced Spectrum-X and Quantum-X silicon photonics switches, featuring the world's most advanced co-packaged optics. These platforms will enable next-level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x, while accelerating customer time to market by 1.3x. Transitioning to a quick summary of our revenue by geography. China as a percentage of our Data Center revenue was slightly below our expectations and down sequentially due to H20 export licensing controls. For Q2, we expect a meaningful decrease in China data center revenue. As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue as many of our large customers use Singapore for centralized invoicing, our products are almost always shipped elsewhere. Note that over 99% of H100, H200, and Blackwell Data Center compute revenue billed to Singapore was for orders from U.S.-based customers. Moving to gaming and AI PCs. Gaming revenue was a record $3.8 billion, increasing 48% sequentially and 42% year-on-year. Strong adoption by gamers, creators and AI enthusiasts have made Blackwell our fastest ramp ever. Against the backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2. AI is transforming PC and creator and gamers. With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's CoPilot+. This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299. The RTX 5060 also debuted in laptops, starting at $1,099. These systems doubled the frame rate and slashed latency. These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available. In console gaming, the recently unveiled Nintendo Switch 2 leverages NVIDIA's neural rendering and AI technologies, including next-generation custom RTX GPUs with DLSS technology to deliver a giant leap in gaming performance to millions of players worldwide. Nintendo has shipped over 150 million switch consoles to date, making it one of the most successful gaming systems in history. Moving to Pro Visualization. Revenue of $509 million was flat sequentially and up 19% year-on-year. Tariff-related uncertainty temporarily impacted Q1 systems and demand for our AI workstations is strong, and we expect sequential revenue growth to resume in Q2. NVIDIA DGX Spark and Station revolutionized personal computing. By putting the power of an AI supercomputer in a desktop form factor. DGX Spark delivers up to 1 petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip. DGX Spark will be available in calendar Q3 and DGX Station later this year. We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP and Schneider Electric. New Omniverse Blueprint such as Mega for at-scale robotic fleet management are being leveraged in KION Group, Pegatron, Accenture and other leading companies to enhance industrial operations. At COMPUTEX, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron. Using Omniverse, TSMC saves months in work by designing fabs virtually, Foxconn accelerates thermal simulations by 150x, and Pegatron reduced assembly line defect rates by 67%. Lastly, with our Automotive group. Revenue was $567 million, down 1% sequentially but up 72% year-on-year. Year-on-year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs. We are partnering with GM to build the next-gen vehicles, factories and robots using NVIDIA AI, simulation and accelerated computing. And we are now in production with our full-stack solution for Mercedes-Benz starting with the new CLA, hitting roads in the next few months. We announced Isaac GR00T N1, the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development. We also launched new open NVIDIA Cosmos World Foundation models. Leading companies include 1X, Agility Robotics, Figure AI, Uber and Waabi. We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPENG Robotics are harnessing Isaac's simulation to advance their humanoid efforts. GE Healthcare is using the new NVIDIA Isaac platform for health care simulation built on NVIDIA Omniverse and using NVIDIA Cosmos for platform speed, development of robotic imaging and surgery systems. The era of robotics is here, billions of robots, hundreds of millions of autonomous vehicles and hundreds of thousands of robotic factories and warehouses will be developed. All right. Moving to the rest of the P&L. GAAP gross margins and non-GAAP gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-GAAP gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter. Sequentially, GAAP operating expenses were up 7% and non-GAAP operating expenses were up 6%, reflecting higher compensation and employee growth. Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year. In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy. Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms. In Data Center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter. GAAP and non-GAAP gross margins are expected to be 71.8% and 72%, respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins. We are continuing to work towards achieving gross margins in the mid-70s range late this year. GAAP and non-GAAP operating expenses are expected to be approximately $5.7 billion and $4 billion, respectively, and we continue to expect full year fiscal year '26 operating expense growth to be in the mid-30% range. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $450 million, excluding gains and losses from nonmarketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new financial information AI agent. Let me highlight upcoming events for the financial community. We will be at the BofA Global Technology Conference in San Francisco on June 4. The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at VivaTech on June 11 in Paris. We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27. Well, now let me turn it over to Jensen to make some remarks."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Thanks, Colette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked. On export control, China is one of the world's largest AI markets and a springboard to global success. With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally. Today, however, the $50 billion China market is effectively closed to U.S. industry. The H20 export ban ended our Hopper Data Center business in China. We cannot reduce Hopper further to comply. As a result, we are taking a multibillion-dollar write-off on inventory that cannot be sold or repurposed. We are exploring limited ways to compete, but Hopper is no longer an option. China's AI moves on with or without U.S. chips. It has the compute to train and deploy advanced models. The question is not whether China will have AI, it already does. The question is whether one of the world's largest AI markets will run on American platforms. Shielding Chinese chip makers from U.S. competition only strengthens them abroad and weakens America's position. Export restrictions have spurred China's innovation and scale. The AI race is not just about chips. It's about which stack the world runs on. As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake. The U.S. has based its policy on the assumption that China cannot make AI chips. That assumption was always questionable, and now it's clearly wrong. China has enormous manufacturing capability. In the end, the platform that wins the AI developers win AI wins AI. Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals. On DeepSeek, DeepSeek and Qwen from China are among the most -- among the best open source AI models. Released freely, they've gained traction across the U.S., Europe and beyond. DeepSeek-R1, like ChatGPT, introduce reasoning AI that produces better answers the longer it thinks. Reasoning AI enables step-by-step problem-solving, planning and tool use, turning models into intelligent agents. Reasoning is compute-intensive, requires hundreds to thousands more -- thousands of times more tokens per task than previous one-shot inference. Reasoning models are driving a step-function surge in inference demand. AI scaling laws remain firmly intact, not only for training, but now inference too requires massive scale compute. DeepSeek also underscores the strategic value of open source AI. When popular models are trained and optimized on U.S. platforms, it drives usage, feedback and continuous improvement, reinforcing American leadership across the stack. U.S. platforms must remain the preferred platform for open source AI. That means supporting collaboration with top developers globally, including in China. America wins when models like DeepSeek and Qwen runs best on American infrastructure. Regarding onshore manufacturing, President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs and strengthen national security. Future plants will be highly computerized in robotics. We share this vision. TSMC is building 6 fabs and 2 advanced packaging plants in Arizona to make chips for NVIDIA. Process qualification is underway with volume production expected by year-end. SPIL and Amkor are also investing in Arizona, constructing packaging, assembly and test facilities. In Houston, we're partnering with Foxconn to construct a 1 million square foot factory to build AI supercomputers. Wistron is building a similar plant in Fort Worth, Texas. To encourage and support these investments, we've made substantial long-term purchase commitments, a deep investment in America's AI manufacturing future. Our goal from chip to supercomputer built in America within a year. Each GB200 NVLink 72 racks contains 1.2 million components and weighs nearly 2 tons. No one has produced supercomputers on this scale. Our partners are doing an extraordinary job. On AI Diffusion Rule, President Trump rescinded the AI Diffusion Rule, calling it counterproductive, and proposed a new policy to promote U.S. AI tech with trusted partners. On his Middle East tour, he announced historic investments. I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia and a 5-gigawatt AI campus in the U.A.E. President Trump wants U.S. tech to lead. The deals he announced are wins for America, creating jobs, advancing infrastructure, generating tax revenue and reducing the U.S. trade deficit. The U.S. will always be NVIDIA's largest market and home to the largest installed base of our infrastructure. Every nation now sees AI as core to the next industrial revolution, a new industry that produces intelligence and essential infrastructure for every economy. Countries are racing to build national AI platforms to elevate their digital capabilities. At COMPUTEX, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government. Last week, I was in Sweden to launch its first national AI infrastructure. Japan, Korea, India, Canada, France, the U.K., Germany, Italy, Spain and more are now building national AI factories to empower start-ups, industries and societies. Sovereign AI is a new growth engine for NVIDIA. Toshiya, back to you. Thank you."
    },
    {
      "speaker": "Toshiya  Hari",
      "role": "Vice President of Investor Relations & Strategic FinanceOperator, we will",
      "text": "Vice President of Investor Relations & Strategic FinanceOperator, we will now open the call for questions. Would you please poll for questions? Question and Answer"
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "[Operator Instructions] Your first question comes from the line of Joe Moore with Morgan Stanley."
    },
    {
      "speaker": "Joseph Lawrence Moore",
      "role": "Morgan Stanley, Research Division",
      "text": "Morgan Stanley, Research Division You guys have talked about this scaling up of inference around reasoning models for at least a year now. And we've really seen that come to fruition as you talked about. We've heard it from your customers. Can you give us a sense for how much of that demand you're able to serve? And give us a sense for maybe how big the inference business is for you guys. And do we need full on NVL72 rack scale solutions for reasoning inference going forward?"
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Well, we would like to serve all of it. And I think we're on track to serve most of it. Grace Blackwell NVLink 72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI. There's a couple of reasons for that. The first reason is that the token generation amount, the number of tokens reasoning goes through, is 100, 1,000x more than a one- shot chatbot. It's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer. It could be using tools, reading PDFs, reading web pages, watching videos and then producing a result, an answer. The longer it thinks, the better the answer, the smarter the answer is. And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step-up in inference performance is so that you could do all this and still get a response as quickly as possible. Compared to Hopper, Grace Blackwell is some 40x higher speed and throughput, compared. And so this is going to be a huge, huge benefit in driving down the cost while improving the quality of response with excellent quality of service at the same time. So that's the fundamental reason. That was the core driving reason for Grace Blackwell and NVLink 72. Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built. But now we're in full production. It's going to be exciting. It's going to be incredibly exciting."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "The next question comes from Vivek Arya with Bank of America Securities."
    },
    {
      "speaker": "Vivek  Arya",
      "role": "BofA Securities, Research Division",
      "text": "BofA Securities, Research Division Just a clarification for Colette first. So on the China impact, I think previously, it was mentioned at about $15 billion. So you had the $8 billion in Q2. So is there still some left as a headwind for the remaining quarters, just for how to model that. And then a question, Jensen, for you. Back at GTC, you had outlined a path towards almost $1 trillion of AI spending over the next few years. Where are we in that build-out? And do you think it's going to be uniform that you will see every spender, whether it's CSPs, sovereigns, enterprises, all build out? Should we expect some periods of digestion in between? Just what are your customer discussions telling you about how to model growth for next year?"
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized $4.6 billion H20 in Q1. We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion. When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue, and we had highlighted in terms of the amount of orders that we had planned for H20 in Q2, and that was $8 billion. Now going forward, we did have other orders going forward that we will not be able to fulfill. That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion. That write-down was about inventory and purchase commitments and our purchase commitments were about what we expected regarding the orders that we had received. Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve. We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for China."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Vivek, probably the best way to think through it is that AI is several things. Of course, we know that AI is this incredible technology that's going to transform every industry from, of course, the way we do software to health care and financial services, to retail, to, I guess, every industry, transportation, manufacturing. And we're at the beginning of that. But maybe another way to think about that is where do we need intelligence? Where do we need digital intelligence? And it's in every country, it's in every industry. And we know, because of that, we recognize that AI is also an infrastructure. It's a way of developing -- delivering a technology that requires factories. And these factories produce tokens. And they, as I mentioned, are important to every single industry and every single country. And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early, early stages. Now we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling. And of course, it created a whole new -- we've entered an era where inference is going to be a significant part of the compute workload. But anyhow, it's going to be a new infrastructure, and we're building it out in the cloud. The United States is really the early starter and available in U.S. clouds. And this is our largest market, our largest installed base, and we continue to see that happening. But beyond that, we're going to have to -- we're going to see AI go into enterprise, which is on-prem. Because so much of the data is still on-prem, access control is really important, it's really hard to move all of -- every company's data into the cloud. And so we're going to move AI into the enterprise. And you saw that we announced a couple of really exciting new products: our RTX Pro enterprise AI server that runs everything enterprise and AI; our DGX Spark and DGX Station, which is designed for developers who want to work on-prem. And so enterprise AI is just taking off. Telcos, today, a lot of the telco infrastructure will be, in the future, software-defined and built on AI. And so 6G is going to be built on AI. And that infrastructure needs to be built out and, as I said, it's very, very early stages. And then, of course, every factory today that makes things will have an AI factory that sits with it. And the AI factory is going to be creating AI and operating AI for the factory itself, but also to power the products and the things that are made by the factory. So it's very clear that every car company will have AI factories. And very soon, there'll be robotics companies, robot companies, and those companies will be also building AIs to drive the robots. And so we're at the beginning of all of this build-out."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "The next question comes from C.J. Muse with Cantor Fitzgerald."
    },
    {
      "speaker": "Christopher James Muse",
      "role": "Cantor Fitzgerald & Co., Research Division",
      "text": "Cantor Fitzgerald & Co., Research Division There have been many large GPU cluster investment announcements in the last month, and you alluded to a few of them with Saudi Arabia, the U.A.E., and then also we heard from Oracle and xAI, just to name a few. So my question, are there others that have yet to be announced of the same kind of scale and magnitude? And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, almost halfway through 2025?"
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Well, we have more orders today than we did at the last time I spoke about orders at GTC. However, we're also increasing our supply chain and building out our supply chain. They're doing a fantastic job. We're building it here onshore in the United States, but we're going to keep our supply chain quite busy for several many more years coming. And with respect to further announcements, I'm going to be on the road next week through Europe. And just about every country needs to build out AI infrastructure, and their umpteenth AI factories being planned. I think in the remarks, Colette mentioned there's some 100 AI factories being built. There's a whole bunch that haven't been announced. And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure; of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society, intelligence is surely one of those things. I don't know any company, industry, country who thinks that intelligence is optional. It's essential infrastructure. And so we've now digitalized intelligence. And so I think we're clearly in the beginning of the build-out of this infrastructure. And every country will have it. I'm certain of that. Every industry will use it, that I'm certain of. And what's unique about this infrastructure is that it needs factories. It's a little bit like the energy infrastructure, electricity. It needs factories. We need factories to produce this intelligence. And the intelligence is getting more sophisticated. We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI and now there are agents that reason and there's super agents that use a whole bunch of tools and then there's clusters of super agents where agents are working with agents, solving problems. And so you could just imagine, compared to one-shot chatbots and the agents that are now using AI built on these large language models, how much more compute-intensive they really need to be and are. And so I think we're in the beginning of the build-out. And there should be many, many more announcements in the future."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from Ben Reitzes with Melius."
    },
    {
      "speaker": "Benjamin Alexander Reitzes",
      "role": "Melius Research LLC",
      "text": "Melius Research LLC I wanted to ask first to Colette, just a little clarification around the guidance and maybe putting it in a different way. The $8 billion for H20 just seems like it's roughly $3 billion more than most people thought with regard to what you'd be foregoing in the second quarter. So that would mean that with regard to your guidance, the rest of the business, in order to hit $45 billion, is doing $2 billion to $3 billion or so better. So I was wondering if that math made sense to you. And then in terms of the guidance, that would imply the non-China business is doing a bit better than the Street expected. So wondering what the primary driver was there in your view? And then the second part of my question, Jensen, I know you guide one quarter at a time. But with regard to the AI Diffusion Rule being lifted and this momentum with sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction in sequential growth throughout the year, et cetera. And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction in sequential growth as we go throughout the year? So first one for Colette and then next one for Jensen."
    },
    {
      "speaker": "Colette M. Kress",
      "role": "Executive VP & CFO",
      "text": "Executive VP & CFO Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that, had the export controls not occurred, we would have had orders of about $8 billion for H20. That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2. So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers. So putting those together, that's where we came through with the guidance that we provided. I'm going to turn the rest over to Jensen to see how he wants to..."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Yes. Thanks, Ben. I would say compared to the beginning of the year, compared to GTC time frame, there are 4 positive surprises. The first positive surprise is the step-function demand increase of reasoning AI. I think it is fairly clear now that AI is going through an exponential growth and reasoning AI really busted through. Concerns about hallucination or its ability to really solve problems, I think a lot of people are crossing that barrier and realizing how incredible, incredibly effective agentic AI is and reasoning AI is. So number one is inference reasoning and the exponential growth there, demand growth. The second one, you mentioned AI diffusion. It's really terrific to see that the AI Diffusion Rule was rescinded. President Trump wants America to win. And he also realizes that we're not the only country in the race. And he wants the United States to win and recognizes that we have to get the American stack out to the world and have the world build on top of American stacks instead of alternatives. And so AI diffusion happened, the rescinding of it happened at almost precisely the time that countries around the world are awakening to the importance of AI as an infrastructure, not just as a technology of great curiosity and great importance, but infrastructure for their industries and start-ups and society. Just as they had to build out infrastructure for electricity and Internet, you got to build out infrastructure for AI. I think that, that's an awakening, and that creates a lot of opportunity. The third is enterprise AI. Agents work. And agents are doing -- these agents are really quite successful. Much more than generative AI, agentic AI is game changing. Agents can understand ambiguous and rather implicit instructions and are able to problem solve and use tools and have memory and so on. And so I think this is -- enterprise AI is ready to take off. And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks -- run enterprise IT stacks, but add AI to it. And this is the RTX Pro enterprise server that we announced at COMPUTEX just last week. And just about every major IT company has joined us and super excited about that. And so computing is one stack, one part of it. But remember, enterprise IT is really 3 pillars. It's compute, storage and networking. And we've now put all 3 of them together for finally, and we're going to market with that. And then lastly, industrial AI. Remember, one of the implications of the world reordering, if you will, is regions onshoring manufacturing and building plants everywhere. In addition to AI factories, of course, there are new electronics manufacturing, chip manufacturing being built around the world. And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging. And so this fourth pillar is quite important. Every factory will have an AI factory associated with it. And in order to create these physical AI systems, you really have to train a vast amount of data. So back to more data, more training, more AIs to be created, more computers. And so these 4 drivers are really kicking into turbocharge."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your next question comes from Timothy Arcuri with UBS."
    },
    {
      "speaker": "Timothy Michael Arcuri",
      "role": "UBS Investment Bank, Research Division",
      "text": "UBS Investment Bank, Research Division Jensen, I wanted to ask about China. It sounds like the July guidance assumes there's no SKU replacement for the H20. But if the President wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China. So I guess I had 2 points on that. First of all, have you been approved to ship a new modified version into China and you're currently building it, but you just can't ship it in fiscal Q2? And then you were sort of run-rating $7 billion to $8 billion a quarter into China. Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China? I think we're all trying to figure out how much to add back to our models and when. So whatever you can say there would be great."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director The President has a plan. He has a vision, and I trust him. With respect to our export controls, it's a set of limits. And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use. And so the new limits, it's kind of the end of the road for hopper. We have limited options. And so we just -- the key is to understand the limits. The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market. We don't have anything at the moment, but we're considering it. We're thinking about it. Obviously, the limits are quite stringent at the moment. And we have nothing to announce today. And when the time comes, we'll engage the administration and discuss that."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "Your final question comes from the line of Aaron Rakers with Wells Fargo."
    },
    {
      "speaker": "Jacob Michael Wilhelm",
      "role": "Wells Fargo Securities, LLC, Research Division",
      "text": "Wells Fargo Securities, LLC, Research Division This is Jake on for Aaron. Congrats on the great quarter. I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as any change you're seeing in network attach rates."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Yes. Thank you for that. We now have 3 networking platforms, maybe 4. The first one is the scale-up platform to turn a computer into a much larger computer. Scaling up is incredibly hard to do. Scaling out is easier to do, but scaling up is hard to do. And that platform is called NVLink. And NVLink comes with it chips and switches and NVLink spines. It's really complicated. But anyways, that's our new platform, scale-up platform. In addition to InfiniBand, we also have Spectrum-X. We've been fairly consistent that Ethernet was designed for a lot of traffic that are independent. But in the case of AI, you have a lot of computers working together. And the traffic of AI is insanely bursty. Latency matters a lot because the AI is thinking and it wants to get work done as quickly as possible, and you've got a whole bunch of nodes working together. And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing, the type of technologies that were available only in InfiniBand to Ethernet. And as a result, we improved the utilization of Ethernet in these clusters, these clusters are gigantic, from as low as 50% to as high as 85%, 90%. And so the difference is, if you had a cluster that's $10 billion, and you improved its effectiveness by 40%, that's worth $4 billion. It's incredible. And so Spectrum-X has been really, quite frankly, a home run. And this last quarter, as we said in the prepared remarks, we added 2 very significant CSPs to the Spectrum-X adoption. And then the last one is BlueField, which is our control plane. And so in those 4 -- the control plane of the network, which is used for storage, it's used for security. And for many of these clusters that want to achieve isolation among its users, multi-tenant clusters and still be able to use and have extremely high-performance bare-metal performance, BlueField is ideal for that and is used in a lot of these cases. And so we have these 4 networking platforms. They're all growing, and we're doing really well. I'm very proud of the team."
    },
    {
      "speaker": "Operator",
      "role": "Operator",
      "text": "That is all the time we have for questions. Jensen, I will turn the call back to you."
    },
    {
      "speaker": "Jen-Hsun  Huang",
      "role": "Co-Founder, CEO, President & Director",
      "text": "Co-Founder, CEO, President & Director Thank you. This is the start of a powerful new wave of growth. Grace Blackwell is in full production. We're off to the races. We now have multiple significant growth engines. Inference, one's delighted workload is surging with revenue-generating AI services. AI is growing faster and will be larger than any platform shifts before, including the Internet, mobile and cloud. Blackwell is built to power the full AI life cycle from training frontier models to running complex inference and reasoning agents at scale. Training demand continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation, but inference is exploding. Reasoning AI agents require orders of magnitude more compute. The foundations of our next growth platforms are in place and ready to scale. Sovereign AI, nations are investing in AI infrastructure like they once did for electricity and Internet. Enterprise AI, AI must be deployable on-prem and integrated with existing IT. Our RTX Pro, DGX Spark and DGX Station enterprise AI systems are ready to modernize the $500 billion IT infrastructure on-prem or in the cloud. Every major IT provider is partnering with us. Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac GR00T are powering next- generation factories and humanoid robotic systems worldwide. The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI and industrial AI. NVIDIA is ready. Join us at GTC Paris. I'll keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots and celebrate our partnerships building AI factories across the region. The NVIDIA Band will tour France, the U.K., Germany and Belgium. Thank you for joining us at the earnings call today. See you in Paris. OperatorThis concludes today's conference call. You may now disconnect. Copyright  2025 by S&P Global Market Intelligence, a division of S&P Global Inc. All rights reserved. These materials have been prepared solely for information purposes based upon information generally available to the public and from sources believed to be reliable. No content (including index data, ratings, credit- related analyses and data, research, model, software or other application or output therefrom) or any part thereof (Content) may be modified, reverse engineered, reproduced or distributed in any form by any means, or stored in a database or retrieval system, without the prior written permission of S&P Global Market Intelligence or its affiliates (collectively, S&P Global). The Content shall not be used for any unlawful or unauthorized purposes. S&P Global and any third-party providers, (collectively S&P Global Parties) do not guarantee the accuracy, completeness, timeliness or availability of the Content. S&P Global Parties are not responsible for any errors or omissions, regardless of the cause, for the results obtained from the use of the Content. THE CONTENT IS PROVIDED ON \"AS IS\" BASIS. S&P GLOBAL PARTIES DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR DEFECTS, THAT THE CONTENT'S FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE CONTENT WILL OPERATE WITH ANY SOFTWARE OR HARDWARE CONFIGURATION. In no event shall S&P Global Parties be liable to any party for any direct, indirect, incidental, exemplary, compensatory, punitive, special or consequential damages, costs, expenses, legal fees, or losses (including, without limitation, lost income or lost profits and opportunity costs or losses caused by negligence) in connection with any use of the Content even if advised of the possibility of such damages. S&P Global Market Intelligence's opinions, quotes and credit-related and other analyses are statements of opinion as of the date they are expressed and not statements of fact or recommendations to purchase, hold, or sell any securities or to make any investment decisions, and do not address the suitability of any security. S&P Global Market Intelligence may provide index data. Direct investment in an index is not possible. Exposure to an asset class represented by an index is available through investable instruments based on that index. S&P Global Market Intelligence assumes no obligation to update the Content following publication in any form or format. The Content should not be relied on and is not a substitute for the skill, judgment and experience of the user, its management, employees, advisors and/or clients when making investment and other business decisions. S&P Global Market Intelligence does not act as a fiduciary or an investment advisor except where registered as such. S&P Global keeps certain activities of its divisions separate from each other in order to preserve the independence and objectivity of their respective activities. As a result, certain divisions of S&P Global may have information that is not available to other S&P Global divisions. S&P Global has established policies and procedures to maintain the confidentiality of certain nonpublic information received in connection with each analytical process. S&P Global may receive compensation for its ratings and certain analyses, normally from issuers or underwriters of securities or from obligors. S&P Global reserves the right to disseminate its opinions and analyses. S&P Global's public ratings and analyses are made available on its Web sites, www.standardandpoors.com  (free of charge), and www.ratingsdirect.com  and www.globalcreditportal.com (subscription), and may be distributed through other means, including via S&P Global publications and third-party redistributors. Additional information about our ratings fees is available at www.standardandpoors.com/usratingsfees.  2025 S&P Global Market Intelligence."
    }
  ],
  "source_file": "NVIDIA Corporation, Q1 2026 Earnings Call, May 28, 2025.rtf"
}